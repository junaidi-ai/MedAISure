{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# 01 â€” Basic Model Evaluation\n\nThis quickstart notebook shows how to:\n\n- List available tasks in `bench/tasks/`\n- Run a simple local demo model via `EvaluationHarness`\n- Save and inspect a `BenchmarkReport`\n\nIt defaults to a tiny local model so it runs without external downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench.evaluation.harness import EvaluationHarness\n",
    "from bench.models.benchmark_report import BenchmarkReport\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize harness pointing to repo-relative paths\n",
    "harness = EvaluationHarness(\n",
    "    tasks_dir=str(Path(\"bench/tasks\")),\n",
    "    results_dir=str(Path(\"results\")),\n",
    "    cache_dir=str(Path(\"cache\")),\n",
    "    log_level=\"INFO\",\n",
    ")\n",
    "\n",
    "# List a few available tasks\n",
    "available = harness.list_available_tasks()\n",
    "len(available), available[:3]  # show first 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "We'll evaluate a couple of lightweight example tasks that ship with the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick real task IDs from bench/tasks/\n",
    "task_ids = [\n",
    "    \"simple_qa\",\n",
    "    \"medical_qa_symptoms\",\n",
    "]\n",
    "\n",
    "# Use the minimal local model example implemented at bench/examples/mypkg/mylocal.py\n",
    "# ModelRunner will import module_path and call load_model()\n",
    "report = harness.evaluate(\n",
    "    model_id=\"demo-local\",\n",
    "    task_ids=task_ids,\n",
    "    model_type=\"local\",\n",
    "    batch_size=8,\n",
    "    strict_validation=False,\n",
    "    module_path=\"bench.examples.mypkg.mylocal\",\n",
    "    model_path=None,  # not used by this toy loader\n",
    ")\n",
    "report.to_dict()[\"overall_scores\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "source": [
    "The `BenchmarkReport` is returned and also saved to `results/`. You can reload it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a recently saved report JSON and load it back\n",
    "paths = sorted(Path(\"results\").glob(\"*.json\"))\n",
    "latest = paths[-1] if paths else None\n",
    "print(\"Latest report:\", latest)\n",
    "if latest:\n",
    "    rep2 = BenchmarkReport.from_file(latest)\n",
    "    print(rep2.overall_scores)\n",
    "    # Optional: visualize (requires matplotlib)\n",
    "    try:\n",
    "        rep2.plot_overall_scores()\n",
    "    except Exception as e:\n",
    "        print(\"Plotting skipped:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "Next: see `02_custom_task_creation.ipynb` to learn how to create your own task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

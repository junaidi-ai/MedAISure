{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# 04b â€” Custom Metrics (Registration & Usage)\n\nThis notebook demonstrates how to:\n\n- Register a custom metric with `MetricCalculator.register_metric()`\n- Run an evaluation with `EvaluationHarness` and then compute the custom metric on the run's predictions and references\n- Optionally, combine custom and built-in metrics\n\nNote: `EvaluationHarness` uses its internal metric set during evaluation. To compute custom metrics, we re-compute metrics post-hoc using `MetricCalculator` with the run's predictions and references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench.evaluation.harness import EvaluationHarness\n",
    "from bench.evaluation.metric_calculator import MetricCalculator\n",
    "\n",
    "# Run a quick evaluation using the local demo model\n",
    "h = EvaluationHarness(tasks_dir=\"bench/tasks\", results_dir=\"results\", cache_dir=\"cache\")\n",
    "rep = h.evaluate(\n",
    "    model_id=\"demo-local\",\n",
    "    task_ids=[\"simple_qa\"],\n",
    "    model_type=\"local\",\n",
    "    module_path=\"bench.examples.mypkg.mylocal\",\n",
    "    model_path=None,\n",
    ")\n",
    "len(rep.detailed_results), list(rep.overall_scores.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "Extract predictions and references for a task from the `EvaluationResult` objects in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions and references from the first task result\n",
    "er = rep.detailed_results[0]\n",
    "task_id = er.task_id\n",
    "predictions = er.model_outputs  # standardized prediction dicts\n",
    "references = er.expected_outputs\n",
    "task_id, predictions[:2], references[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "source": [
    "Register a custom metric function and compute it with `MetricCalculator`.\n\nBelow we define a simple exact-match score over a string field (defaulting to `label`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = MetricCalculator()\n",
    "\n",
    "\n",
    "def exact_match(y_true, y_pred, *, field=\"label\", **kwargs):\n",
    "    t = [(r.get(field) if isinstance(r, dict) else r) for r in y_true]\n",
    "    p = [(r.get(field) if isinstance(r, dict) else r) for r in y_pred]\n",
    "    num = sum(int(tt == pp) for tt, pp in zip(t, p))\n",
    "    den = max(1, len(t))\n",
    "    return float(num / den)\n",
    "\n",
    "\n",
    "mc.register_metric(\"exact_match\", exact_match, field=\"label\")\n",
    "custom = mc.calculate_metrics(\n",
    "    task_id, predictions, references, metric_names=[\"exact_match\"]\n",
    ")\n",
    "{k: v.value for k, v in custom.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "You can also compute both built-in and custom metrics together by passing a combined list to `metric_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "both = mc.calculate_metrics(\n",
    "    task_id, predictions, references, metric_names=[\"accuracy\", \"exact_match\"]\n",
    ")\n",
    "{k: v.value for k, v in both.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "source": [
    "For a pure code example, see also `bench/examples/register_custom_metric.py`.\n\nNext: see `02b_python_task_interface.ipynb` for defining tasks programmatically and registering them with the registry."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

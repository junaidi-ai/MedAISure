{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# 03 â€” Result Analysis & Visualization\n\nThis notebook demonstrates how to:\n\n- Load or create evaluation runs\n- Use `ResultAggregator` to retrieve a report by `run_id`\n- Export reports to JSON/CSV/Markdown/HTML\n- Compare two runs and visualize metric distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench.evaluation.harness import EvaluationHarness\n",
    "from bench.evaluation.result_aggregator import ResultAggregator\n",
    "\n",
    "# First, ensure at least one run exists (use the local demo model).\n",
    "h = EvaluationHarness(tasks_dir=\"bench/tasks\", results_dir=\"results\", cache_dir=\"cache\")\n",
    "run1 = h.evaluate(\n",
    "    model_id=\"demo-local\",\n",
    "    task_ids=[\"simple_qa\", \"medical_qa_symptoms\"],\n",
    "    model_type=\"local\",\n",
    "    module_path=\"bench.examples.mypkg.mylocal\",\n",
    "    model_path=None,\n",
    ")\n",
    "run1_id = run1.metadata.get(\"run_id\")\n",
    "run1_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "Create a second run (simulating a configuration change).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = EvaluationHarness(\n",
    "    tasks_dir=\"bench/tasks\", results_dir=\"results\", cache_dir=\"cache\"\n",
    ")\n",
    "run2 = h2.evaluate(\n",
    "    model_id=\"demo-local-v2\",\n",
    "    task_ids=[\"simple_qa\", \"medical_qa_symptoms\"],\n",
    "    model_type=\"local\",\n",
    "    module_path=\"bench.examples.mypkg.mylocal\",\n",
    "    model_path=None,\n",
    ")\n",
    "run2_id = run2.metadata.get(\"run_id\")\n",
    "run1_id, run2_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "source": [
    "Use `ResultAggregator` to access run reports and export formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = ResultAggregator(output_dir=\"results\")\n",
    "# Re-add results from both runs to the aggregator for this session\n",
    "ra.add_evaluation_result(run1.detailed_results[0], run_id=run1_id)\n",
    "for er in run1.detailed_results[1:]:\n",
    "    ra.add_evaluation_result(er, run_id=run1_id)\n",
    "for er in run2.detailed_results:\n",
    "    ra.add_evaluation_result(er, run_id=run2_id)\n",
    "\n",
    "# Export to various formats\n",
    "p_json = ra.export_report_json(run1_id)\n",
    "p_csv = ra.export_report_csv(run1_id, \"results/\" + run1_id + \".csv\")\n",
    "p_md = ra.export_report_markdown(run1_id, \"results/\" + run1_id + \".md\")\n",
    "p_html = ra.export_report_html(\n",
    "    run1_id, \"results/\" + run1_id + \".html\", include_examples=True\n",
    ")\n",
    "p_json, p_csv, p_md, p_html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "Compare the two runs using `compare_runs()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = ra.compare_runs(run1_id, run2_id)\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "source": [
    "Visualize a metric distribution across tasks with `plot_metric_distribution()`. If matplotlib isn't installed, data is returned without plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = ra.plot_metric_distribution(\n",
    "    run1_id,\n",
    "    metric=\"clinical_accuracy\",\n",
    "    output_path=\"results/\" + run1_id + \"_clinical_accuracy.png\",\n",
    ")\n",
    "plot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "source": [
    "You can also call `BenchmarkReport.plot_overall_scores()` and `plot_task_scores(metric)` on a loaded report.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

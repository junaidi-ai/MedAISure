{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Core Evaluation Framework Structure",
        "description": "Create the foundational structure for the MEDDSAI benchmark evaluation framework including the Task Loader, Model Runner, Metric Calculator, and Result Aggregator components.",
        "details": "Develop the core evaluation framework with the following components:\n\n1. Task Loader:\n- Create a class that loads task definitions and data from dataset sources\n- Implement methods to parse task configurations\n- Add validation for task schema compliance\n\n2. Model Runner:\n- Develop interface for executing predictions on models\n- Create adapters for different model types (local, API-based, HuggingFace)\n- Implement batching and execution optimization\n\n3. Metric Calculator:\n- Build base metric calculation infrastructure\n- Implement metric registration system\n- Create metric computation pipeline\n\n4. Result Aggregator:\n- Develop system to combine results across tasks and metrics\n- Implement the BenchmarkReport schema as defined in the PRD\n- Create exporters for different output formats\n\nExample implementation structure:\n```python\nclass TaskLoader:\n    def load_task(self, task_id: str) -> MedicalTask:\n        # Implementation\n        pass\n\nclass ModelRunner:\n    def run_model(self, model, inputs: List[Dict]) -> List[Dict]:\n        # Implementation\n        pass\n\nclass MetricCalculator:\n    def calculate_metrics(self, task: MedicalTask, outputs: List[Dict]) -> Dict[str, float]:\n        # Implementation\n        pass\n\nclass ResultAggregator:\n    def aggregate_results(self, results: List[EvaluationResult]) -> BenchmarkReport:\n        # Implementation\n        pass\n\nclass EvaluationHarness:\n    def __init__(self):\n        self.task_loader = TaskLoader()\n        self.model_runner = ModelRunner()\n        self.metric_calculator = MetricCalculator()\n        self.result_aggregator = ResultAggregator()\n    \n    def evaluate(self, model_id: str, task_ids: List[str]) -> BenchmarkReport:\n        # Implementation\n        pass\n```",
        "testStrategy": "1. Unit tests for each component:\n   - Test TaskLoader with sample task definitions\n   - Test ModelRunner with mock models\n   - Test MetricCalculator with predefined inputs/outputs\n   - Test ResultAggregator with sample results\n\n2. Integration tests:\n   - End-to-end test with simple model and task\n   - Verify correct data flow between components\n\n3. Schema validation tests:\n   - Ensure all data structures conform to defined schemas\n   - Test error handling for malformed inputs",
        "priority": "high",
        "dependencies": [],
        "status": "in-progress",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement Data Models and Schemas",
        "description": "Create the data models and schemas required for the benchmark, including MedicalTask, EvaluationResult, and BenchmarkReport classes as defined in the PRD.",
        "details": "Implement the following data models with proper typing and validation:\n\n1. MedicalTask Schema:\n```python\nfrom enum import Enum\nfrom typing import List, Dict\nfrom pydantic import BaseModel\n\nclass TaskType(Enum):\n    DiagnosticReasoning = \"diagnostic_reasoning\"\n    QA = \"qa\"\n    Summarization = \"summarization\"\n    Communication = \"communication\"\n\nclass MedicalTask(BaseModel):\n    task_id: str\n    task_type: TaskType\n    description: str\n    inputs: List[Dict]  # Structured input data\n    expected_outputs: List[Dict]  # Reference outputs for evaluation\n    metrics: List[str]  # Metrics used for this task\n```\n\n2. EvaluationResult Schema:\n```python\nclass EvaluationResult(BaseModel):\n    model_id: str\n    task_id: str\n    inputs: List[Dict]\n    model_outputs: List[Dict]\n    metrics_results: Dict[str, float]  # Metric name to score\n    metadata: Dict  # Additional information (runtime, model version, etc.)\n```\n\n3. BenchmarkReport Schema:\n```python\nfrom datetime import datetime\n\nclass BenchmarkReport(BaseModel):\n    model_id: str\n    timestamp: datetime\n    overall_scores: Dict[str, float]\n    task_scores: Dict[str, Dict[str, float]]\n    detailed_results: List[EvaluationResult]\n```\n\nImplement serialization/deserialization methods for each model to support JSON and other formats. Add validation logic to ensure data integrity and consistency.",
        "testStrategy": "1. Unit tests for each data model:\n   - Test initialization with valid data\n   - Test validation errors with invalid data\n   - Test serialization/deserialization\n\n2. Property-based tests:\n   - Generate random valid instances\n   - Verify round-trip serialization\n\n3. Edge case tests:\n   - Test with empty collections\n   - Test with minimal valid data\n   - Test with maximum expected data sizes",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "in-progress",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Develop Basic Task Implementations",
        "description": "Implement the initial set of medical domain tasks including medical QA, diagnostic reasoning, and clinical summarization tasks as specified in Phase 1 of the roadmap.",
        "details": "Create implementations for the following task types:\n\n1. Medical QA Tasks:\n```python\nclass MedicalQATask(MedicalTask):\n    def __init__(self, task_id: str, description: str):\n        super().__init__(\n            task_id=task_id,\n            task_type=TaskType.QA,\n            description=description,\n            inputs=[],\n            expected_outputs=[],\n            metrics=[\"accuracy\", \"clinical_correctness\"]\n        )\n    \n    def load_data(self, data_path: str):\n        # Load QA pairs from data source\n        # Format as inputs and expected_outputs\n        pass\n```\n\n2. Diagnostic Reasoning Tasks:\n```python\nclass DiagnosticReasoningTask(MedicalTask):\n    def __init__(self, task_id: str, description: str):\n        super().__init__(\n            task_id=task_id,\n            task_type=TaskType.DiagnosticReasoning,\n            description=description,\n            inputs=[],\n            expected_outputs=[],\n            metrics=[\"diagnostic_accuracy\", \"reasoning_quality\"]\n        )\n    \n    def load_data(self, data_path: str):\n        # Load clinical cases and diagnoses\n        # Format as inputs and expected_outputs\n        pass\n```\n\n3. Clinical Summarization Tasks:\n```python\nclass ClinicalSummarizationTask(MedicalTask):\n    def __init__(self, task_id: str, description: str):\n        super().__init__(\n            task_id=task_id,\n            task_type=TaskType.Summarization,\n            description=description,\n            inputs=[],\n            expected_outputs=[],\n            metrics=[\"rouge\", \"clinical_relevance\", \"factual_consistency\"]\n        )\n    \n    def load_data(self, data_path: str):\n        # Load medical documents and reference summaries\n        # Format as inputs and expected_outputs\n        pass\n```\n\nFor each task type, implement at least 2-3 specific task instances with sample data. Include proper documentation and ensure tasks follow the defined schemas.",
        "testStrategy": "1. Unit tests for each task type:\n   - Test initialization and configuration\n   - Test data loading functionality\n   - Verify task schema compliance\n\n2. Data validation tests:\n   - Ensure sample data is properly formatted\n   - Test with various input sizes and formats\n\n3. Integration tests:\n   - Test tasks with the evaluation harness\n   - Verify metrics calculation for each task type",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "in-progress",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Implement Basic Metrics",
        "description": "Develop the fundamental metrics for evaluating model performance on medical tasks, including clinical accuracy, reasoning quality, and domain-specific metrics.",
        "details": "Create a metrics implementation framework with the following components:\n\n1. Base Metric Interface:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict\n\nclass Metric(ABC):\n    @abstractmethod\n    def calculate(self, expected_outputs: List[Dict], model_outputs: List[Dict]) -> float:\n        pass\n    \n    @property\n    @abstractmethod\n    def name(self) -> str:\n        pass\n```\n\n2. Clinical Accuracy Metrics:\n```python\nclass ClinicalAccuracyMetric(Metric):\n    @property\n    def name(self) -> str:\n        return \"clinical_accuracy\"\n    \n    def calculate(self, expected_outputs: List[Dict], model_outputs: List[Dict]) -> float:\n        # Implementation of clinical accuracy calculation\n        # Compare medical entities, diagnoses, treatments, etc.\n        pass\n```\n\n3. Reasoning Quality Metrics:\n```python\nclass ReasoningQualityMetric(Metric):\n    @property\n    def name(self) -> str:\n        return \"reasoning_quality\"\n    \n    def calculate(self, expected_outputs: List[Dict], model_outputs: List[Dict]) -> float:\n        # Implementation of reasoning quality assessment\n        # Evaluate logical structure, evidence citation, etc.\n        pass\n```\n\n4. Domain-Specific Metrics:\n```python\nclass DiagnosticAccuracyMetric(Metric):\n    @property\n    def name(self) -> str:\n        return \"diagnostic_accuracy\"\n    \n    def calculate(self, expected_outputs: List[Dict], model_outputs: List[Dict]) -> float:\n        # Implementation specific to diagnostic tasks\n        pass\n\nclass ClinicalRelevanceMetric(Metric):\n    @property\n    def name(self) -> str:\n        return \"clinical_relevance\"\n    \n    def calculate(self, expected_outputs: List[Dict], model_outputs: List[Dict]) -> float:\n        # Implementation for assessing relevance in clinical context\n        pass\n```\n\n5. Metric Registry:\n```python\nclass MetricRegistry:\n    def __init__(self):\n        self._metrics = {}\n    \n    def register_metric(self, metric: Metric):\n        self._metrics[metric.name] = metric\n    \n    def get_metric(self, name: str) -> Metric:\n        return self._metrics.get(name)\n    \n    def calculate_metrics(self, metric_names: List[str], expected_outputs: List[Dict], model_outputs: List[Dict]) -> Dict[str, float]:\n        results = {}\n        for name in metric_names:\n            metric = self.get_metric(name)\n            if metric:\n                results[name] = metric.calculate(expected_outputs, model_outputs)\n        return results\n```",
        "testStrategy": "1. Unit tests for each metric:\n   - Test with known inputs and expected scores\n   - Test edge cases (empty inputs, perfect match, no match)\n   - Verify score ranges and normalization\n\n2. Comparison tests:\n   - Compare metric scores against human judgments on sample outputs\n   - Verify correlation with expected performance patterns\n\n3. Integration tests:\n   - Test metrics within the evaluation harness\n   - Verify correct aggregation of metric scores",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Create Docker Environment",
        "description": "Set up a Docker environment for consistent evaluation across platforms, including base container with Python dependencies, CUDA support, and test execution environment.",
        "details": "Develop Docker configurations for the MEDDSAI benchmark with the following components:\n\n1. Base Dockerfile:\n```dockerfile\nFROM python:3.10-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    git \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy benchmark code\nCOPY . .\n\n# Set up entrypoint\nENTRYPOINT [\"python\", \"-m\", \"meddsai.cli\"]\n```\n\n2. GPU-enabled Dockerfile:\n```dockerfile\nFROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    git \\\n    python3 \\\n    python3-pip \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip3 install --no-cache-dir -r requirements.txt\n\n# Copy benchmark code\nCOPY . .\n\n# Set up entrypoint\nENTRYPOINT [\"python3\", \"-m\", \"meddsai.cli\"]\n```\n\n3. Docker Compose Configuration:\n```yaml\nversion: '3'\n\nservices:\n  meddsai-cpu:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    volumes:\n      - ./data:/app/data\n      - ./results:/app/results\n    command: [\"evaluate\", \"--config\", \"/app/data/config.json\"]\n\n  meddsai-gpu:\n    build:\n      context: .\n      dockerfile: Dockerfile.gpu\n    volumes:\n      - ./data:/app/data\n      - ./results:/app/results\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n    command: [\"evaluate\", \"--config\", \"/app/data/config.json\"]\n```\n\n4. Requirements File:\n```\npydantic>=2.0.0\nnumpy>=1.20.0\npandas>=1.3.0\ntorch>=2.0.0\ntransformers>=4.30.0\nfastapi>=0.95.0\nuvicorn>=0.22.0\npython-dotenv>=1.0.0\ntyper>=0.9.0\nrich>=13.0.0\n```\n\n5. Docker Ignore File:\n```\n.git\n__pycache__/\n*.py[cod]\n*$py.class\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n.coverage\nhtmlcov/\n.pytest_cache/\n```\n\nEnsure the Docker environment is properly documented with usage instructions and environment variable configurations.",
        "testStrategy": "1. Build tests:\n   - Verify Docker images build successfully\n   - Test both CPU and GPU configurations\n\n2. Runtime tests:\n   - Test container execution with sample tasks\n   - Verify proper volume mounting and data access\n   - Test GPU detection and utilization\n\n3. Environment tests:\n   - Verify all dependencies are correctly installed\n   - Test Python version and library compatibility\n   - Ensure reproducibility across different host systems",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Develop Command Line Interface",
        "description": "Create a comprehensive CLI for the benchmark that allows users to select tasks, register models, execute evaluations, and generate reports.",
        "details": "Implement a command-line interface using Typer with the following features:\n\n1. Main CLI Structure:\n```python\nimport typer\nfrom typing import List, Optional\nfrom pathlib import Path\n\napp = typer.Typer()\n\n@app.callback()\ndef callback():\n    \"\"\"MEDDSAI Benchmark: Evaluation framework for medical domain AI models\"\"\"\n    pass\n\n@app.command()\ndef evaluate(\n    model_id: str = typer.Argument(..., help=\"ID of the model to evaluate\"),\n    tasks: List[str] = typer.Option(None, help=\"Specific tasks to run (runs all if not specified)\"),\n    config_file: Optional[Path] = typer.Option(None, help=\"Path to configuration file\"),\n    output_dir: Path = typer.Option(\"./results\", help=\"Directory to store results\"),\n    format: str = typer.Option(\"json\", help=\"Output format (json, md, csv)\"),\n):\n    \"\"\"Run evaluation on specified model and tasks\"\"\"\n    # Implementation\n    pass\n\n@app.command()\ndef list_tasks():\n    \"\"\"List all available tasks in the benchmark\"\"\"\n    # Implementation\n    pass\n\n@app.command()\ndef register_model(\n    model_path: Path = typer.Argument(..., help=\"Path to model or model configuration\"),\n    model_id: Optional[str] = typer.Option(None, help=\"Custom ID for the model\"),\n    model_type: str = typer.Option(\"local\", help=\"Model type (local, huggingface, api)\"),\n):\n    \"\"\"Register a model for evaluation\"\"\"\n    # Implementation\n    pass\n\n@app.command()\ndef generate_report(\n    results_file: Path = typer.Argument(..., help=\"Path to results file\"),\n    output_file: Optional[Path] = typer.Option(None, help=\"Output file path\"),\n    format: str = typer.Option(\"md\", help=\"Report format (md, html, pdf)\"),\n):\n    \"\"\"Generate a human-readable report from results\"\"\"\n    # Implementation\n    pass\n\nif __name__ == \"__main__\":\n    app()\n```\n\n2. Configuration Handling:\n```python\nfrom pydantic import BaseModel\nfrom typing import Dict, List, Optional\n\nclass BenchmarkConfig(BaseModel):\n    model_id: str\n    tasks: Optional[List[str]] = None\n    metrics: Optional[Dict[str, List[str]]] = None\n    output_dir: str = \"./results\"\n    output_format: str = \"json\"\n    \n    @classmethod\n    def from_file(cls, file_path: Path) -> \"BenchmarkConfig\":\n        # Load from JSON/YAML file\n        pass\n```\n\n3. Rich Terminal Output:\n```python\nfrom rich.console import Console\nfrom rich.progress import Progress, TaskID\nfrom rich.table import Table\n\nconsole = Console()\n\ndef display_task_list(tasks: List[MedicalTask]):\n    table = Table(title=\"Available Tasks\")\n    table.add_column(\"ID\")\n    table.add_column(\"Type\")\n    table.add_column(\"Description\")\n    table.add_column(\"Metrics\")\n    \n    for task in tasks:\n        table.add_row(\n            task.task_id,\n            task.task_type.value,\n            task.description,\n            \", \".join(task.metrics)\n        )\n    \n    console.print(table)\n\ndef display_evaluation_progress(total_tasks: int):\n    with Progress() as progress:\n        task_id = progress.add_task(\"Evaluating...\", total=total_tasks)\n        # Update progress during evaluation\n```\n\nEnsure the CLI provides helpful error messages, proper documentation, and follows best practices for command-line interfaces.",
        "testStrategy": "1. Command tests:\n   - Test each command with various arguments\n   - Verify help text and documentation\n   - Test error handling for invalid inputs\n\n2. Integration tests:\n   - Test end-to-end workflows (register → evaluate → report)\n   - Verify correct file handling and output generation\n\n3. Usability tests:\n   - Review command structure with potential users\n   - Test with various terminal sizes and environments\n   - Verify accessibility features (color contrast, etc.)",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          3,
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement Model Interface",
        "description": "Create a standardized model interface that supports various model types (local, API-based, HuggingFace) and provides a consistent prediction method.",
        "details": "Develop a model interface system with the following components:\n\n1. Base Model Interface:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any\n\nclass ModelInterface(ABC):\n    @abstractmethod\n    def predict(self, inputs: List[Dict]) -> List[Dict]:\n        \"\"\"Run prediction on the given inputs\"\"\"\n        pass\n    \n    @property\n    @abstractmethod\n    def model_id(self) -> str:\n        \"\"\"Unique identifier for the model\"\"\"\n        pass\n    \n    @property\n    def metadata(self) -> Dict[str, Any]:\n        \"\"\"Model metadata (version, architecture, etc.)\"\"\"\n        return {}\n```\n\n2. Local Model Implementation:\n```python\nclass LocalModel(ModelInterface):\n    def __init__(self, model_path: str, model_id: str = None):\n        self._model = self._load_model(model_path)\n        self._model_id = model_id or os.path.basename(model_path)\n    \n    def _load_model(self, model_path: str):\n        # Implementation to load local model\n        pass\n    \n    def predict(self, inputs: List[Dict]) -> List[Dict]:\n        # Implementation to run prediction\n        pass\n    \n    @property\n    def model_id(self) -> str:\n        return self._model_id\n```\n\n3. HuggingFace Model Implementation:\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass HuggingFaceModel(ModelInterface):\n    def __init__(self, model_name: str, model_id: str = None):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self._model_id = model_id or model_name\n    \n    def predict(self, inputs: List[Dict]) -> List[Dict]:\n        results = []\n        for input_item in inputs:\n            # Process input and generate prediction\n            prompt = input_item.get(\"prompt\", \"\")\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n            outputs = self.model.generate(**inputs)\n            text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            results.append({\"text\": text})\n        return results\n    \n    @property\n    def model_id(self) -> str:\n        return self._model_id\n    \n    @property\n    def metadata(self) -> Dict[str, Any]:\n        return {\n            \"model_name\": self._model_id,\n            \"framework\": \"huggingface\",\n            \"parameters\": self.model.num_parameters()\n        }\n```\n\n4. API Model Implementation:\n```python\nimport requests\n\nclass APIModel(ModelInterface):\n    def __init__(self, api_url: str, api_key: str = None, model_id: str = None):\n        self.api_url = api_url\n        self.api_key = api_key\n        self._model_id = model_id or f\"api-{api_url.split('/')[-1]}\"\n    \n    def predict(self, inputs: List[Dict]) -> List[Dict]:\n        results = []\n        headers = {}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n        \n        for input_item in inputs:\n            response = requests.post(\n                self.api_url,\n                json=input_item,\n                headers=headers\n            )\n            response.raise_for_status()\n            results.append(response.json())\n        \n        return results\n    \n    @property\n    def model_id(self) -> str:\n        return self._model_id\n```\n\n5. Model Registry:\n```python\nclass ModelRegistry:\n    def __init__(self):\n        self._models = {}\n    \n    def register_model(self, model: ModelInterface):\n        self._models[model.model_id] = model\n    \n    def get_model(self, model_id: str) -> ModelInterface:\n        return self._models.get(model_id)\n    \n    def list_models(self) -> List[str]:\n        return list(self._models.keys())\n```",
        "testStrategy": "1. Unit tests for each model interface:\n   - Test initialization with various parameters\n   - Test prediction with sample inputs\n   - Verify output format consistency\n\n2. Mock API tests:\n   - Test API model with mocked responses\n   - Verify error handling for API failures\n\n3. Integration tests:\n   - Test model interfaces with the evaluation harness\n   - Verify compatibility with different task types\n   - Test performance monitoring and resource usage",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Create Dataset Connectors",
        "description": "Implement interfaces for medical datasets (MIMIC, PubMed, etc.), data preprocessing pipelines, and secure data handling for sensitive medical information.",
        "details": "Develop dataset connectors with the following components:\n\n1. Base Dataset Interface:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Iterator\nfrom pathlib import Path\n\nclass DatasetConnector(ABC):\n    @abstractmethod\n    def load_data(self) -> Iterator[Dict]:\n        \"\"\"Load and yield data items\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_metadata(self) -> Dict:\n        \"\"\"Return dataset metadata\"\"\"\n        pass\n```\n\n2. Local File Dataset:\n```python\nimport json\nimport csv\nfrom pathlib import Path\n\nclass JSONDataset(DatasetConnector):\n    def __init__(self, file_path: Path, encryption_key: str = None):\n        self.file_path = file_path\n        self.encryption_key = encryption_key\n    \n    def load_data(self) -> Iterator[Dict]:\n        data = self._read_file()\n        for item in data:\n            yield item\n    \n    def _read_file(self) -> List[Dict]:\n        # Implementation with optional decryption\n        with open(self.file_path, 'r') as f:\n            data = json.load(f)\n        return data\n    \n    def get_metadata(self) -> Dict:\n        return {\n            \"source\": str(self.file_path),\n            \"format\": \"json\",\n            \"size\": self.file_path.stat().st_size\n        }\n\nclass CSVDataset(DatasetConnector):\n    def __init__(self, file_path: Path, encryption_key: str = None):\n        self.file_path = file_path\n        self.encryption_key = encryption_key\n    \n    def load_data(self) -> Iterator[Dict]:\n        with open(self.file_path, 'r') as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                yield dict(row)\n    \n    def get_metadata(self) -> Dict:\n        return {\n            \"source\": str(self.file_path),\n            \"format\": \"csv\",\n            \"size\": self.file_path.stat().st_size\n        }\n```\n\n3. Medical Database Connectors:\n```python\nclass MIMICConnector(DatasetConnector):\n    def __init__(self, connection_string: str, query: str):\n        self.connection_string = connection_string\n        self.query = query\n    \n    def load_data(self) -> Iterator[Dict]:\n        # Implementation with database connection\n        # Ensure secure handling of PHI\n        pass\n    \n    def get_metadata(self) -> Dict:\n        return {\n            \"source\": \"MIMIC\",\n            \"query\": self.query,\n            \"connection\": self.connection_string.split('@')[-1]  # Hide credentials\n        }\n\nclass PubMedConnector(DatasetConnector):\n    def __init__(self, search_terms: List[str], max_results: int = 100):\n        self.search_terms = search_terms\n        self.max_results = max_results\n    \n    def load_data(self) -> Iterator[Dict]:\n        # Implementation with PubMed API\n        pass\n    \n    def get_metadata(self) -> Dict:\n        return {\n            \"source\": \"PubMed\",\n            \"search_terms\": self.search_terms,\n            \"max_results\": self.max_results\n        }\n```\n\n4. Data Preprocessing Pipeline:\n```python\nfrom typing import Callable, List\n\nclass DataPreprocessor:\n    def __init__(self):\n        self.steps: List[Callable[[Dict], Dict]] = []\n    \n    def add_step(self, step: Callable[[Dict], Dict]):\n        self.steps.append(step)\n    \n    def process(self, data_item: Dict) -> Dict:\n        result = data_item.copy()\n        for step in self.steps:\n            result = step(result)\n        return result\n    \n    def process_batch(self, data_items: List[Dict]) -> List[Dict]:\n        return [self.process(item) for item in data_items]\n```\n\n5. Secure Data Handling:\n```python\nfrom cryptography.fernet import Fernet\nimport hashlib\n\nclass SecureDataHandler:\n    def __init__(self, encryption_key: str = None):\n        self.encryption_key = encryption_key\n        if encryption_key:\n            key = hashlib.sha256(encryption_key.encode()).digest()\n            self.cipher = Fernet(base64.urlsafe_b64encode(key))\n    \n    def encrypt_data(self, data: Dict) -> Dict:\n        if not self.encryption_key:\n            return data\n        \n        result = {}\n        for key, value in data.items():\n            if isinstance(value, str):\n                result[key] = self.cipher.encrypt(value.encode()).decode()\n            else:\n                result[key] = value\n        return result\n    \n    def decrypt_data(self, data: Dict) -> Dict:\n        if not self.encryption_key:\n            return data\n        \n        result = {}\n        for key, value in data.items():\n            if isinstance(value, str):\n                try:\n                    result[key] = self.cipher.decrypt(value.encode()).decode()\n                except Exception:\n                    result[key] = value  # Not encrypted or invalid\n            else:\n                result[key] = value\n        return result\n```",
        "testStrategy": "1. Unit tests for each connector:\n   - Test data loading functionality\n   - Verify metadata retrieval\n   - Test with sample datasets\n\n2. Security tests:\n   - Test encryption/decryption functionality\n   - Verify secure handling of sensitive data\n   - Test access controls and permissions\n\n3. Integration tests:\n   - Test connectors with task implementations\n   - Verify preprocessing pipeline functionality\n   - Test performance with large datasets",
        "priority": "medium",
        "dependencies": [
          1,
          2
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Develop Reporting Interface",
        "description": "Create a reporting system that generates detailed evaluation reports in various formats (JSON, Markdown, etc.) and integrates with visualization tools.",
        "details": "Implement a reporting interface with the following components:\n\n1. Report Generator Base Class:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any\nfrom pathlib import Path\n\nclass ReportGenerator(ABC):\n    @abstractmethod\n    def generate(self, benchmark_report: BenchmarkReport) -> Any:\n        \"\"\"Generate report from benchmark results\"\"\"\n        pass\n    \n    @abstractmethod\n    def save(self, report: Any, output_path: Path) -> None:\n        \"\"\"Save report to file\"\"\"\n        pass\n```\n\n2. JSON Report Generator:\n```python\nimport json\n\nclass JSONReportGenerator(ReportGenerator):\n    def generate(self, benchmark_report: BenchmarkReport) -> Dict:\n        return benchmark_report.dict()\n    \n    def save(self, report: Dict, output_path: Path) -> None:\n        with open(output_path, 'w') as f:\n            json.dump(report, f, indent=2)\n```\n\n3. Markdown Report Generator:\n```python\nclass MarkdownReportGenerator(ReportGenerator):\n    def generate(self, benchmark_report: BenchmarkReport) -> str:\n        lines = []\n        \n        # Header\n        lines.append(f\"# Benchmark Report: {benchmark_report.model_id}\")\n        lines.append(f\"Generated at: {benchmark_report.timestamp}\\n\")\n        \n        # Overall scores\n        lines.append(\"## Overall Scores\")\n        for metric, score in benchmark_report.overall_scores.items():\n            lines.append(f\"- **{metric}**: {score:.4f}\")\n        lines.append(\"\")\n        \n        # Task scores\n        lines.append(\"## Task Scores\")\n        for task_id, scores in benchmark_report.task_scores.items():\n            lines.append(f\"### {task_id}\")\n            for metric, score in scores.items():\n                lines.append(f\"- **{metric}**: {score:.4f}\")\n            lines.append(\"\")\n        \n        # Detailed results\n        lines.append(\"## Detailed Results\")\n        for result in benchmark_report.detailed_results:\n            lines.append(f\"### Task: {result.task_id}\")\n            lines.append(\"#### Metrics\")\n            for metric, score in result.metrics_results.items():\n                lines.append(f\"- **{metric}**: {score:.4f}\")\n            lines.append(\"\")\n        \n        return \"\\n\".join(lines)\n    \n    def save(self, report: str, output_path: Path) -> None:\n        with open(output_path, 'w') as f:\n            f.write(report)\n```\n\n4. HTML Report Generator:\n```python\nclass HTMLReportGenerator(ReportGenerator):\n    def generate(self, benchmark_report: BenchmarkReport) -> str:\n        # Implementation using a template engine or direct HTML generation\n        pass\n    \n    def save(self, report: str, output_path: Path) -> None:\n        with open(output_path, 'w') as f:\n            f.write(report)\n```\n\n5. Visualization Integration:\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nclass VisualizationGenerator:\n    def __init__(self, benchmark_report: BenchmarkReport):\n        self.report = benchmark_report\n    \n    def generate_metric_comparison(self, output_path: Path) -> None:\n        # Create DataFrame from results\n        data = []\n        for task_id, scores in self.report.task_scores.items():\n            for metric, score in scores.items():\n                data.append({\"Task\": task_id, \"Metric\": metric, \"Score\": score})\n        \n        df = pd.DataFrame(data)\n        \n        # Create visualization\n        plt.figure(figsize=(12, 8))\n        pivot = df.pivot(index=\"Task\", columns=\"Metric\", values=\"Score\")\n        pivot.plot(kind=\"bar\")\n        plt.title(f\"Metric Comparison: {self.report.model_id}\")\n        plt.ylabel(\"Score\")\n        plt.tight_layout()\n        \n        # Save figure\n        plt.savefig(output_path)\n```\n\n6. Report Factory:\n```python\nclass ReportFactory:\n    @staticmethod\n    def create_generator(format: str) -> ReportGenerator:\n        if format.lower() == \"json\":\n            return JSONReportGenerator()\n        elif format.lower() == \"md\":\n            return MarkdownReportGenerator()\n        elif format.lower() == \"html\":\n            return HTMLReportGenerator()\n        else:\n            raise ValueError(f\"Unsupported format: {format}\")\n```",
        "testStrategy": "1. Unit tests for each report generator:\n   - Test report generation with sample data\n   - Verify output format correctness\n   - Test with various report sizes and content\n\n2. Format validation tests:\n   - Validate JSON output against schema\n   - Verify Markdown syntax correctness\n   - Test HTML validity\n\n3. Visualization tests:\n   - Test chart generation with sample data\n   - Verify image output quality and correctness\n   - Test with various data distributions",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Create Documentation and Examples",
        "description": "Develop comprehensive documentation including developer guides, usage examples, and contribution guidelines to facilitate adoption and extension of the benchmark.",
        "details": "Create documentation and examples with the following components:\n\n1. Project Documentation Structure:\n```\ndocs/\n├── getting_started.md\n├── architecture.md\n├── tasks/\n│   ├── overview.md\n│   ├── medical_qa.md\n│   ├── diagnostic_reasoning.md\n│   └── clinical_summarization.md\n├── metrics/\n│   ├── overview.md\n│   ├── clinical_accuracy.md\n│   └── reasoning_quality.md\n├── models/\n│   ├── model_interface.md\n│   ├── local_models.md\n│   └── api_models.md\n├── api/\n│   ├── core_api.md\n│   └── cli.md\n└── contributing.md\n```\n\n2. Getting Started Guide:\n```markdown\n# Getting Started with MEDDSAI\n\n## Installation\n\n```bash\n# Install from PyPI\npip install meddsai\n\n# Or install from source\ngit clone https://github.com/meddsai/benchmark.git\ncd benchmark\npip install -e .\n```\n\n## Quick Start\n\n```python\nfrom meddsai import EvaluationHarness\nfrom meddsai.models import HuggingFaceModel\n\n# Initialize a model\nmodel = HuggingFaceModel(\"gpt2\")\n\n# Create evaluation harness\nharness = EvaluationHarness()\n\n# Run evaluation\nresults = harness.evaluate(model, tasks=[\"medical_qa_basic\"])\n\n# Print results\nprint(results.overall_scores)\n```\n\n## Using the CLI\n\n```bash\n# List available tasks\nmeddsai list-tasks\n\n# Register a model\nmeddsai register-model ./my_model --model-id my-model\n\n# Run evaluation\nmeddsai evaluate my-model --tasks medical_qa_basic\n\n# Generate report\nmeddsai generate-report ./results/my-model_results.json --format md\n```\n```\n\n3. Example Notebooks:\n- Create Jupyter notebooks demonstrating key workflows:\n  - Basic model evaluation\n  - Custom task creation\n  - Result analysis and visualization\n  - Advanced configuration options\n\n4. API Documentation:\n- Generate comprehensive API documentation using Sphinx or a similar tool\n- Include docstrings for all public classes and methods\n- Provide usage examples for each component\n\n5. Contribution Guidelines:\n```markdown\n# Contributing to MEDDSAI\n\n## Code of Conduct\n\nThis project adheres to the Contributor Covenant code of conduct. By participating, you are expected to uphold this code.\n\n## How to Contribute\n\n### Reporting Bugs\n\n- Use the issue tracker to report bugs\n- Describe the bug and include specific details to help reproduce it\n- Include sample code if possible\n\n### Suggesting Enhancements\n\n- Use the issue tracker to suggest enhancements\n- Clearly describe the enhancement and its expected behavior\n- Explain why this enhancement would be useful\n\n### Pull Requests\n\n1. Fork the repository\n2. Create a new branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Run tests (`pytest`)\n5. Commit your changes (`git commit -m 'Add some amazing feature'`)\n6. Push to the branch (`git push origin feature/amazing-feature`)\n7. Open a Pull Request\n\n## Development Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/meddsai.git\ncd meddsai\n\n# Create a virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install development dependencies\npip install -e \".[dev]\"\n\n# Run tests\npytest\n```\n\n## Coding Standards\n\n- Follow PEP 8 style guide\n- Write docstrings for all functions, classes, and methods\n- Maintain test coverage for all code\n```\n\n6. Task Development Guide:\n```markdown\n# Creating Custom Tasks\n\n## Task Interface\n\nAll tasks must implement the `MedicalTask` interface:\n\n```python\nfrom meddsai.tasks import MedicalTask, TaskType\n\nclass MyCustomTask(MedicalTask):\n    def __init__(self, task_id: str, description: str):\n        super().__init__(\n            task_id=task_id,\n            task_type=TaskType.QA,  # Or another appropriate type\n            description=description,\n            inputs=[],\n            expected_outputs=[],\n            metrics=[\"accuracy\", \"my_custom_metric\"]\n        )\n    \n    def load_data(self, data_path: str):\n        # Implementation to load task data\n        pass\n```\n\n## Registering Your Task\n\n```python\nfrom meddsai.registry import task_registry\n\n# Register your task\ntask_registry.register_task(MyCustomTask(\"my_custom_task\", \"Description of my task\"))\n```\n\n## Example Task Implementation\n\n[Detailed example with complete implementation]\n```",
        "testStrategy": "1. Documentation tests:\n   - Verify all links work correctly\n   - Test code examples for correctness\n   - Ensure documentation is up-to-date with code\n\n2. Example tests:\n   - Run all example notebooks to verify functionality\n   - Test with different environments and configurations\n\n3. User testing:\n   - Have new users follow documentation to complete tasks\n   - Collect feedback on clarity and completeness\n   - Identify areas for improvement",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          3,
          4,
          6,
          7,
          8,
          9
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-07-28T05:24:41.681Z",
      "updated": "2025-07-28T05:24:41.681Z",
      "description": "Tasks for master context"
    }
  }
}

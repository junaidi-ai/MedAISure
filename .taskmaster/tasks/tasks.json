{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Core Evaluation Framework Structure",
        "description": "Create the foundational structure for the MedAISure benchmark evaluation framework including the Task Loader, Model Runner, Metric Calculator, and Result Aggregator components.",
        "details": "Develop the core evaluation framework with the following components:\n\n1. Task Loader:\n- Create a class that loads task definitions and data from dataset sources\n- Implement methods to parse task configurations\n- Add validation for task schema compliance\n\n2. Model Runner:\n- Develop interface for executing predictions on models\n- Create adapters for different model types (local, API-based, HuggingFace)\n- Implement batching and execution optimization\n\n3. Metric Calculator:\n- Build base metric calculation infrastructure\n- Implement metric registration system\n- Create metric computation pipeline\n\n4. Result Aggregator:\n- Develop system to combine results across tasks and metrics\n- Implement the BenchmarkReport schema as defined in the PRD\n- Create exporters for different output formats\n\nExample implementation structure:\n```python\nclass TaskLoader:\n    def load_task(self, task_id: str) -> MedicalTask:\n        # Implementation\n        pass\n\nclass ModelRunner:\n    def run_model(self, model, inputs: List[Dict]) -> List[Dict]:\n        # Implementation\n        pass\n\nclass MetricCalculator:\n    def calculate_metrics(self, task: MedicalTask, outputs: List[Dict]) -> Dict[str, float]:\n        # Implementation\n        pass\n\nclass ResultAggregator:\n    def aggregate_results(self, results: List[EvaluationResult]) -> BenchmarkReport:\n        # Implementation\n        pass\n\nclass EvaluationHarness:\n    def __init__(self):\n        self.task_loader = TaskLoader()\n        self.model_runner = ModelRunner()\n        self.metric_calculator = MetricCalculator()\n        self.result_aggregator = ResultAggregator()\n    \n    def evaluate(self, model_id: str, task_ids: List[str]) -> BenchmarkReport:\n        # Implementation\n        pass\n```",
        "testStrategy": "1. Unit tests for each component:\n   - Test TaskLoader with sample task definitions\n   - Test ModelRunner with mock models\n   - Test MetricCalculator with predefined inputs/outputs\n   - Test ResultAggregator with sample results\n\n2. Integration tests:\n   - End-to-end test with simple model and task\n   - Verify correct data flow between components\n\n3. Schema validation tests:\n   - Ensure all data structures conform to defined schemas\n   - Test error handling for malformed inputs",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and implement TaskLoader component",
            "description": "Create the TaskLoader class that loads task definitions and data from dataset sources with proper validation and error handling.",
            "dependencies": [],
            "details": "Implement the TaskLoader with the following features:\n- Create methods to load task definitions from JSON/YAML files\n- Implement task configuration parsing logic\n- Add schema validation for task definitions\n- Implement error handling for missing or malformed task files\n- Create a registry for available tasks\n- Add support for different data sources (local files, URLs, databases)\n- Document the public API with docstrings and type hints",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Design and implement ModelRunner component",
            "description": "Develop the ModelRunner class that provides a unified interface for executing predictions on different types of models.",
            "dependencies": [],
            "details": "Implement the ModelRunner with the following features:\n- Create a base interface for model execution\n- Implement adapters for different model types (local, API-based, HuggingFace)\n- Add batching and execution optimization\n- Implement timeout and retry logic for API calls\n- Add error handling for model execution failures\n- Create logging for model execution metrics (latency, token usage)\n- Support for asynchronous execution where applicable",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Design and implement MetricCalculator component",
            "description": "Build the MetricCalculator class that handles metric registration, calculation, and aggregation for evaluation results.",
            "dependencies": [],
            "details": "Implement the MetricCalculator with the following features:\n- Create a base metric interface with calculate method\n- Implement a metric registration system\n- Build a metric computation pipeline\n- Add support for task-specific metric configurations\n- Implement caching for expensive metric calculations\n- Create validation for metric inputs and outputs\n- Support for custom metric implementations\n- Add logging for metric calculation results",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Design and implement ResultAggregator component",
            "description": "Develop the ResultAggregator class that combines results across tasks and metrics into a comprehensive benchmark report.",
            "dependencies": [],
            "details": "Implement the ResultAggregator with the following features:\n- Create methods to combine results across tasks and metrics\n- Implement the BenchmarkReport schema as defined in the PRD\n- Add exporters for different output formats (JSON, CSV, HTML, Markdown)\n- Implement statistical aggregation methods (mean, median, percentiles)\n- Create visualization helpers for report generation\n- Add support for filtering and sorting results\n- Implement comparison functionality between different model runs",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement EvaluationHarness integration class",
            "description": "Create the main EvaluationHarness class that integrates all components and provides the primary evaluation interface.",
            "dependencies": [],
            "details": "Implement the EvaluationHarness with the following features:\n- Initialize and connect all component classes\n- Create the main evaluate() method that orchestrates the evaluation process\n- Implement progress tracking and reporting\n- Add configuration options for evaluation runs\n- Create methods for partial evaluations and resuming interrupted runs\n- Implement proper resource management and cleanup\n- Add event hooks for monitoring the evaluation process",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Develop unit tests for core components",
            "description": "Create comprehensive unit tests for each core component to ensure correct functionality and error handling.",
            "dependencies": [],
            "details": "Implement unit tests covering:\n- TaskLoader tests with sample task definitions and edge cases\n- ModelRunner tests with mock models and various input types\n- MetricCalculator tests with predefined inputs/outputs\n- ResultAggregator tests with sample results\n- EvaluationHarness tests with mocked components\n- Error handling tests for each component\n- Performance tests for critical operations",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Develop integration tests for the framework",
            "description": "Create integration tests that verify the correct interaction between components and end-to-end functionality.",
            "dependencies": [],
            "details": "Implement integration tests covering:\n- End-to-end tests with simple models and tasks\n- Component interaction tests\n- Configuration validation tests\n- Error propagation tests\n- Resource management tests\n- Performance benchmarks for the full evaluation pipeline\n- Compatibility tests with different Python versions and environments",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Create documentation and usage examples",
            "description": "Develop comprehensive documentation for the evaluation framework including API references, usage examples, and extension guides.",
            "dependencies": [],
            "details": "Create documentation including:\n- API reference for all public classes and methods\n- Usage examples for common evaluation scenarios\n- Extension guides for adding new tasks, metrics, and model types\n- Configuration reference for the framework\n- Troubleshooting guide for common issues\n- Performance optimization tips\n- Contribution guidelines for the project",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Data Models and Schemas",
        "description": "Create the data models and schemas required for the benchmark, including MedicalTask, EvaluationResult, and BenchmarkReport classes as defined in the PRD.",
        "details": "Implement the following data models with proper typing and validation:\n\n1. MedicalTask Schema:\n```python\nfrom enum import Enum\nfrom typing import List, Dict\nfrom pydantic import BaseModel\n\nclass TaskType(Enum):\n    DiagnosticReasoning = \"diagnostic_reasoning\"\n    QA = \"qa\"\n    Summarization = \"summarization\"\n    Communication = \"communication\"\n\nclass MedicalTask(BaseModel):\n    task_id: str\n    task_type: TaskType\n    description: str\n    inputs: List[Dict]  # Structured input data\n    expected_outputs: List[Dict]  # Reference outputs for evaluation\n    metrics: List[str]  # Metrics used for this task\n```\n\n2. EvaluationResult Schema:\n```python\nclass EvaluationResult(BaseModel):\n    model_id: str\n    task_id: str\n    inputs: List[Dict]\n    model_outputs: List[Dict]\n    metrics_results: Dict[str, float]  # Metric name to score\n    metadata: Dict  # Additional information (runtime, model version, etc.)\n```\n\n3. BenchmarkReport Schema:\n```python\nfrom datetime import datetime\n\nclass BenchmarkReport(BaseModel):\n    model_id: str\n    timestamp: datetime\n    overall_scores: Dict[str, float]\n    task_scores: Dict[str, Dict[str, float]]\n    detailed_results: List[EvaluationResult]\n```\n\nImplement serialization/deserialization methods for each model to support JSON and other formats. Add validation logic to ensure data integrity and consistency.",
        "testStrategy": "1. Unit tests for each data model:\n   - Test initialization with valid data\n   - Test validation errors with invalid data\n   - Test serialization/deserialization\n\n2. Property-based tests:\n   - Generate random valid instances\n   - Verify round-trip serialization\n\n3. Edge case tests:\n   - Test with empty collections\n   - Test with minimal valid data\n   - Test with maximum expected data sizes",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "in-progress",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement MedicalTask Schema",
            "description": "Create the MedicalTask data model with proper typing, validation, and serialization/deserialization methods.",
            "dependencies": [],
            "details": "Implement the MedicalTask class using Pydantic with:\n- TaskType enum implementation\n- All required fields (task_id, task_type, description, inputs, expected_outputs, metrics)\n- Field validation (e.g., non-empty strings, valid enum values)\n- Custom validators for complex fields\n- JSON serialization/deserialization methods\n- Documentation for each field and method",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement EvaluationResult Schema",
            "description": "Create the EvaluationResult data model with proper typing, validation, and serialization/deserialization methods.",
            "dependencies": [],
            "details": "Implement the EvaluationResult class using Pydantic with:\n- All required fields (model_id, task_id, inputs, model_outputs, metrics_results, metadata)\n- Field validation for required fields\n- Type checking for metrics_results dictionary\n- JSON serialization/deserialization methods\n- Methods to calculate summary statistics from results\n- Documentation for each field and method",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement BenchmarkReport Schema",
            "description": "Create the BenchmarkReport data model with proper typing, validation, and serialization/deserialization methods.",
            "dependencies": [
              "2.2"
            ],
            "details": "Implement the BenchmarkReport class using Pydantic with:\n- All required fields (model_id, timestamp, overall_scores, task_scores, detailed_results)\n- Field validation for required fields\n- Proper datetime handling\n- Nested EvaluationResult handling\n- JSON serialization/deserialization methods\n- Methods to generate summary statistics and visualizations\n- Documentation for each field and method",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Schema Validation Logic",
            "description": "Add comprehensive validation logic to ensure data integrity and consistency across all data models.",
            "dependencies": [
              "2.1",
              "2.2",
              "2.3"
            ],
            "details": "Implement validation logic including:\n- Cross-field validation rules\n- Custom validators for complex business rules\n- Type coercion where appropriate\n- Error handling with descriptive messages\n- Validation for nested structures\n- Implement methods to check relationships between models (e.g., task_id consistency)\n- Add validation for edge cases (empty lists, special characters, etc.)",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Advanced Serialization Features",
            "description": "Extend the basic serialization to support multiple formats and special use cases.",
            "dependencies": [
              "2.1",
              "2.2",
              "2.3"
            ],
            "details": "Implement advanced serialization features including:\n- Support for multiple formats (JSON, YAML, CSV where applicable)\n- Custom serializers for complex types\n- Versioning support for schema evolution\n- Partial serialization/deserialization\n- Performance optimizations for large datasets\n- Methods to convert between different formats\n- Documentation for serialization APIs",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create Unit and Property-Based Tests",
            "description": "Develop comprehensive test suite for all data models to ensure correctness and robustness.",
            "dependencies": [
              "2.1",
              "2.2",
              "2.3",
              "2.4",
              "2.5"
            ],
            "details": "Create tests including:\n- Unit tests for initialization with valid data\n- Validation tests with invalid data\n- Serialization/deserialization round-trip tests\n- Property-based tests with randomly generated valid instances\n- Edge case tests (empty values, boundary values, etc.)\n- Performance tests for large datasets\n- Integration tests between related models\n- Documentation for test cases and coverage",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 3,
        "title": "Develop Basic Task Implementations",
        "description": "Implement the initial set of medical domain tasks including medical QA, diagnostic reasoning, and clinical summarization tasks as specified in Phase 1 of the roadmap.",
        "details": "Create implementations for the following task types:\n\n1. Medical QA Tasks:\n```python\nclass MedicalQATask(MedicalTask):\n    def __init__(self, task_id: str, description: str):\n        super().__init__(\n            task_id=task_id,\n            task_type=TaskType.QA,\n            description=description,\n            inputs=[],\n            expected_outputs=[],\n            metrics=[\"accuracy\", \"clinical_correctness\"]\n        )\n    \n    def load_data(self, data_path: str):\n        # Load QA pairs from data source\n        # Format as inputs and expected_outputs\n        pass\n```\n\n2. Diagnostic Reasoning Tasks:\n```python\nclass DiagnosticReasoningTask(MedicalTask):\n    def __init__(self, task_id: str, description: str):\n        super().__init__(\n            task_id=task_id,\n            task_type=TaskType.DiagnosticReasoning,\n            description=description,\n            inputs=[],\n            expected_outputs=[],\n            metrics=[\"diagnostic_accuracy\", \"reasoning_quality\"]\n        )\n    \n    def load_data(self, data_path: str):\n        # Load clinical cases and diagnoses\n        # Format as inputs and expected_outputs\n        pass\n```\n\n3. Clinical Summarization Tasks:\n```python\nclass ClinicalSummarizationTask(MedicalTask):\n    def __init__(self, task_id: str, description: str):\n        super().__init__(\n            task_id=task_id,\n            task_type=TaskType.Summarization,\n            description=description,\n            inputs=[],\n            expected_outputs=[],\n            metrics=[\"rouge\", \"clinical_relevance\", \"factual_consistency\"]\n        )\n    \n    def load_data(self, data_path: str):\n        # Load medical documents and reference summaries\n        # Format as inputs and expected_outputs\n        pass\n```\n\nFor each task type, implement at least 2-3 specific task instances with sample data. Include proper documentation and ensure tasks follow the defined schemas.",
        "testStrategy": "1. Unit tests for each task type:\n   - Test initialization and configuration\n   - Test data loading functionality\n   - Verify task schema compliance\n\n2. Data validation tests:\n   - Ensure sample data is properly formatted\n   - Test with various input sizes and formats\n\n3. Integration tests:\n   - Test tasks with the evaluation harness\n   - Verify metrics calculation for each task type",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Medical QA Task Class",
            "description": "Complete the implementation of the MedicalQATask class including data loading functionality and create 2-3 specific task instances with sample data.",
            "dependencies": [],
            "details": "1. Implement the load_data method to parse QA pairs from various formats (JSON, CSV)\n2. Create sample data files with medical questions and answers\n3. Develop 2-3 specific task instances (e.g., general medical QA, specialty-specific QA)\n4. Ensure proper validation of inputs and outputs\n5. Implement methods for evaluating model responses against expected outputs",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Diagnostic Reasoning Task Class",
            "description": "Complete the implementation of the DiagnosticReasoningTask class including data loading functionality and create 2-3 specific task instances with sample clinical cases.",
            "dependencies": [],
            "details": "1. Implement the load_data method to parse clinical cases and diagnoses\n2. Create sample data files with patient cases and correct diagnoses\n3. Develop 2-3 specific task instances (e.g., common conditions, rare diseases)\n4. Implement structured input/output formats for diagnostic reasoning\n5. Add methods to evaluate diagnostic accuracy and reasoning quality",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Clinical Summarization Task Class",
            "description": "Complete the implementation of the ClinicalSummarizationTask class including data loading functionality and create 2-3 specific task instances with medical documents and reference summaries.",
            "dependencies": [],
            "details": "1. Implement the load_data method to parse medical documents and reference summaries\n2. Create sample data files with clinical notes, reports, and reference summaries\n3. Develop 2-3 specific task instances (e.g., discharge summary, radiology report summary)\n4. Implement methods to evaluate generated summaries using specified metrics\n5. Add functionality to handle different document types and lengths",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Develop Task Schema Validation",
            "description": "Implement validation mechanisms to ensure all task implementations comply with the defined schemas and properly integrate with the evaluation framework.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3"
            ],
            "details": "1. Create schema validation functions for each task type\n2. Implement input/output format validators\n3. Add error handling for malformed data\n4. Create tests to verify schema compliance\n5. Develop documentation on schema requirements for each task type",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Integrate Tasks with Evaluation Harness",
            "description": "Ensure all implemented tasks properly integrate with the evaluation harness, including metric calculation and result reporting.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3",
              "3.4"
            ],
            "details": "1. Connect task implementations to the evaluation pipeline\n2. Implement task-specific evaluation logic\n3. Ensure proper metric selection and calculation for each task type\n4. Add result formatting for the reporting system\n5. Create integration tests for the full evaluation workflow",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create Task Documentation",
            "description": "Develop comprehensive documentation for all implemented tasks, including usage examples, data format specifications, and metric explanations.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3"
            ],
            "details": "1. Write detailed docstrings for all classes and methods\n2. Create usage examples for each task type\n3. Document data format requirements and specifications\n4. Explain task-specific metrics and evaluation criteria\n5. Add documentation on how to create custom task instances",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement Task Registry System",
            "description": "Develop a registry system that allows dynamic registration and discovery of available tasks within the benchmark framework.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3",
              "3.4"
            ],
            "details": "1. Create a TaskRegistry class to manage available tasks\n2. Implement methods for registering new tasks\n3. Add functionality to discover and list available tasks\n4. Develop filtering capabilities (by task type, difficulty, etc.)\n5. Ensure proper integration with the CLI and evaluation system",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement Basic Metrics",
        "description": "Develop the fundamental metrics for evaluating model performance on medical tasks, including clinical accuracy, reasoning quality, and domain-specific metrics.",
        "details": "Create a metrics implementation framework with the following components:\n\n1. Base Metric Interface:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict\n\nclass Metric(ABC):\n    @abstractmethod\n    def calculate(self, expected_outputs: List[Dict], model_outputs: List[Dict]) -> float:\n        pass\n    \n    @property\n    @abstractmethod\n    def name(self) -> str:\n        pass\n```\n\n2. Clinical Accuracy Metrics:\n```python\nclass ClinicalAccuracyMetric(Metric):\n    @property\n    def name(self) -> str:\n        return \"clinical_accuracy\"\n    \n    def calculate(self, expected_outputs: List[Dict], model_outputs: List[Dict]) -> float:\n        # Implementation of clinical accuracy calculation\n        # Compare medical entities, diagnoses, treatments, etc.\n        pass\n```\n\n3. Reasoning Quality Metrics:\n```python\nclass ReasoningQualityMetric(Metric):\n    @property\n    def name(self) -> str:\n        return \"reasoning_quality\"\n    \n    def calculate(self, expected_outputs: List[Dict], model_outputs: List[Dict]) -> float:\n        # Implementation of reasoning quality assessment\n        # Evaluate logical structure, evidence citation, etc.\n        pass\n```\n\n4. Domain-Specific Metrics:\n```python\nclass DiagnosticAccuracyMetric(Metric):\n    @property\n    def name(self) -> str:\n        return \"diagnostic_accuracy\"\n    \n    def calculate(self, expected_outputs: List[Dict], model_outputs: List[Dict]) -> float:\n        # Implementation specific to diagnostic tasks\n        pass\n\nclass ClinicalRelevanceMetric(Metric):\n    @property\n    def name(self) -> str:\n        return \"clinical_relevance\"\n    \n    def calculate(self, expected_outputs: List[Dict], model_outputs: List[Dict]) -> float:\n        # Implementation for assessing relevance in clinical context\n        pass\n```\n\n5. Metric Registry:\n```python\nclass MetricRegistry:\n    def __init__(self):\n        self._metrics = {}\n    \n    def register_metric(self, metric: Metric):\n        self._metrics[metric.name] = metric\n    \n    def get_metric(self, name: str) -> Metric:\n        return self._metrics.get(name)\n    \n    def calculate_metrics(self, metric_names: List[str], expected_outputs: List[Dict], model_outputs: List[Dict]) -> Dict[str, float]:\n        results = {}\n        for name in metric_names:\n            metric = self.get_metric(name)\n            if metric:\n                results[name] = metric.calculate(expected_outputs, model_outputs)\n        return results\n```",
        "testStrategy": "1. Unit tests for each metric:\n   - Test with known inputs and expected scores\n   - Test edge cases (empty inputs, perfect match, no match)\n   - Verify score ranges and normalization\n\n2. Comparison tests:\n   - Compare metric scores against human judgments on sample outputs\n   - Verify correlation with expected performance patterns\n\n3. Integration tests:\n   - Test metrics within the evaluation harness\n   - Verify correct aggregation of metric scores",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Implement Base Metric Interface",
            "description": "Finalize the design and implementation of the base Metric abstract class that all metrics will inherit from.",
            "dependencies": [],
            "details": "1. Review and refine the existing Metric ABC implementation\n2. Add documentation for each method\n3. Consider adding additional utility methods like validation\n4. Implement error handling for invalid inputs\n5. Create unit tests for the base interface\n6. Ensure proper type hinting and validation",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Clinical Accuracy Metric",
            "description": "Complete the implementation of the ClinicalAccuracyMetric class to evaluate the accuracy of clinical information in model outputs.",
            "dependencies": [
              "4.1"
            ],
            "details": "1. Define the algorithm for comparing medical entities between expected and model outputs\n2. Implement entity extraction and normalization\n3. Add weighting for different types of clinical information (diagnoses, treatments, etc.)\n4. Normalize scores between 0-1\n5. Add detailed scoring breakdown in results\n6. Create comprehensive unit tests with medical examples",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Reasoning Quality Metric",
            "description": "Complete the implementation of the ReasoningQualityMetric class to evaluate the logical structure and evidence citation in model outputs.",
            "dependencies": [
              "4.1"
            ],
            "details": "1. Define criteria for evaluating reasoning quality (logical flow, evidence citation, etc.)\n2. Implement algorithms to detect reasoning patterns\n3. Create scoring mechanism for different reasoning aspects\n4. Add support for detecting logical fallacies\n5. Implement evidence validation against known medical facts\n6. Create unit tests with examples of good and poor reasoning",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Domain-Specific Metrics",
            "description": "Complete the implementation of domain-specific metrics including DiagnosticAccuracyMetric and ClinicalRelevanceMetric.",
            "dependencies": [
              "4.1"
            ],
            "details": "1. Finalize the DiagnosticAccuracyMetric implementation\n2. Complete the ClinicalRelevanceMetric implementation\n3. Add medical knowledge validation components\n4. Implement specialty-specific scoring adjustments\n5. Create configurable thresholds for different medical domains\n6. Develop comprehensive test cases for each specialty domain",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Build Metric Registry System",
            "description": "Complete the implementation of the MetricRegistry class for managing and executing multiple metrics.",
            "dependencies": [
              "4.1",
              "4.2",
              "4.3",
              "4.4"
            ],
            "details": "1. Finalize the MetricRegistry implementation\n2. Add validation for registered metrics\n3. Implement parallel execution of metrics for performance\n4. Add caching mechanism for repeated calculations\n5. Create methods for metric result aggregation\n6. Implement serialization/deserialization of metric results\n7. Develop unit tests for the registry",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Develop Unit Tests for Individual Metrics",
            "description": "Create comprehensive unit tests for each metric implementation to ensure correctness and robustness.",
            "dependencies": [
              "4.2",
              "4.3",
              "4.4"
            ],
            "details": "1. Create test cases with known inputs and expected outputs\n2. Test edge cases (empty inputs, perfect matches, no matches)\n3. Verify score ranges and normalization\n4. Test with malformed or unexpected inputs\n5. Create performance tests for large input sets\n6. Implement property-based testing for metrics\n7. Document test coverage and results",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Develop Integration Tests for Metrics System",
            "description": "Create integration tests that verify the entire metrics system works correctly with the rest of the evaluation framework.",
            "dependencies": [
              "4.5",
              "4.6"
            ],
            "details": "1. Create end-to-end tests with sample tasks and model outputs\n2. Test integration with the Model Runner component\n3. Verify correct aggregation of multiple metric results\n4. Test performance with large datasets\n5. Create comparison tests against human judgments\n6. Implement regression tests for known edge cases\n7. Document integration test coverage and results",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Create Docker Environment",
        "description": "Set up a Docker environment for consistent evaluation across platforms, including base container with Python dependencies, CUDA support, and test execution environment.",
        "details": "Develop Docker configurations for the MedAISure benchmark with the following components:\n\n1. Base Dockerfile:\n```dockerfile\nFROM python:3.10-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    git \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy benchmark code\nCOPY . .\n\n# Set up entrypoint\nENTRYPOINT [\"python\", \"-m\", \"medaisure.cli\"]\n```\n\n2. GPU-enabled Dockerfile:\n```dockerfile\nFROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    git \\\n    python3 \\\n    python3-pip \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip3 install --no-cache-dir -r requirements.txt\n\n# Copy benchmark code\nCOPY . .\n\n# Set up entrypoint\nENTRYPOINT [\"python3\", \"-m\", \"medaisure.cli\"]\n```\n\n3. Docker Compose Configuration:\n```yaml\nversion: '3'\n\nservices:\n  medaisure-cpu:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    volumes:\n      - ./data:/app/data\n      - ./results:/app/results\n    command: [\"evaluate\", \"--config\", \"/app/data/config.json\"]\n\n  medaisure-gpu:\n    build:\n      context: .\n      dockerfile: Dockerfile.gpu\n    volumes:\n      - ./data:/app/data\n      - ./results:/app/results\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n    command: [\"evaluate\", \"--config\", \"/app/data/config.json\"]\n```\n\n4. Requirements File:\n```\npydantic>=2.0.0\nnumpy>=1.20.0\npandas>=1.3.0\ntorch>=2.0.0\ntransformers>=4.30.0\nfastapi>=0.95.0\nuvicorn>=0.22.0\npython-dotenv>=1.0.0\ntyper>=0.9.0\nrich>=13.0.0\n```\n\n5. Docker Ignore File:\n```\n.git\n__pycache__/\n*.py[cod]\n*$py.class\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n.coverage\nhtmlcov/\n.pytest_cache/\n```\n\nEnsure the Docker environment is properly documented with usage instructions and environment variable configurations.",
        "testStrategy": "1. Build tests:\n   - Verify Docker images build successfully\n   - Test both CPU and GPU configurations\n\n2. Runtime tests:\n   - Test container execution with sample tasks\n   - Verify proper volume mounting and data access\n   - Test GPU detection and utilization\n\n3. Environment tests:\n   - Verify all dependencies are correctly installed\n   - Test Python version and library compatibility\n   - Ensure reproducibility across different host systems",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Base Dockerfile",
            "description": "Develop and test the base Dockerfile for CPU environments, ensuring all dependencies are properly installed and the container functions correctly.",
            "dependencies": [],
            "details": "1. Review and finalize the provided base Dockerfile\n2. Test building the image with the Python 3.10 slim base\n3. Verify all system dependencies (build-essential, git) are installed correctly\n4. Ensure Python dependencies from requirements.txt install properly\n5. Test the ENTRYPOINT configuration with the medaisure.cli module\n6. Document any issues or optimizations needed",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create GPU-enabled Dockerfile",
            "description": "Develop and test the GPU-enabled Dockerfile with CUDA support, ensuring compatibility with NVIDIA GPUs and proper driver integration.",
            "dependencies": [
              "5.1"
            ],
            "details": "1. Review and finalize the provided GPU Dockerfile based on nvidia/cuda image\n2. Test building the GPU-enabled image\n3. Verify CUDA libraries and dependencies are correctly configured\n4. Test GPU detection within the container\n5. Ensure Python dependencies are compatible with the CUDA environment\n6. Benchmark basic GPU operations to confirm functionality\n7. Document any CUDA-specific configuration requirements",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Configure Docker Compose",
            "description": "Set up and test the Docker Compose configuration for orchestrating both CPU and GPU containers, including volume mounting and command configurations.",
            "dependencies": [
              "5.1",
              "5.2"
            ],
            "details": "1. Review and finalize the provided docker-compose.yml file\n2. Test the compose configuration for both CPU and GPU services\n3. Verify volume mounting for data and results directories\n4. Test the command configuration for evaluation runs\n5. Ensure GPU resource allocation is properly configured\n6. Document how to use docker-compose for different evaluation scenarios\n7. Create sample commands for common use cases",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Create Documentation and Environment Files",
            "description": "Develop comprehensive documentation for the Docker environment, including usage instructions, environment variable configurations, and finalize supporting files like requirements.txt and .dockerignore.",
            "dependencies": [
              "5.1",
              "5.2",
              "5.3"
            ],
            "details": "1. Finalize the requirements.txt file with all necessary dependencies\n2. Create the .dockerignore file to exclude unnecessary files\n3. Write detailed usage documentation including:\n   - Installation instructions\n   - Basic usage examples\n   - Environment variable configurations\n   - Troubleshooting common issues\n4. Document differences between CPU and GPU environments\n5. Create a quick-start guide for new users\n6. Document volume mounting and data handling best practices",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 6,
        "title": "Develop Command Line Interface",
        "description": "Create a comprehensive CLI for the benchmark that allows users to select tasks, register models, execute evaluations, and generate reports.",
        "details": "Implement a command-line interface using Typer with the following features:\n\n1. Main CLI Structure:\n```python\nimport typer\nfrom typing import List, Optional\nfrom pathlib import Path\n\napp = typer.Typer()\n\n@app.callback()\ndef callback():\n    \"\"\"MedAISure Benchmark: Evaluation framework for medical domain AI models\"\"\"\n    pass\n\n@app.command()\ndef evaluate(\n    model_id: str = typer.Argument(..., help=\"ID of the model to evaluate\"),\n    tasks: List[str] = typer.Option(None, help=\"Specific tasks to run (runs all if not specified)\"),\n    config_file: Optional[Path] = typer.Option(None, help=\"Path to configuration file\"),\n    output_dir: Path = typer.Option(\"./results\", help=\"Directory to store results\"),\n    format: str = typer.Option(\"json\", help=\"Output format (json, md, csv)\"),\n):\n    \"\"\"Run evaluation on specified model and tasks\"\"\"\n    # Implementation\n    pass\n\n@app.command()\ndef list_tasks():\n    \"\"\"List all available tasks in the benchmark\"\"\"\n    # Implementation\n    pass\n\n@app.command()\ndef register_model(\n    model_path: Path = typer.Argument(..., help=\"Path to model or model configuration\"),\n    model_id: Optional[str] = typer.Option(None, help=\"Custom ID for the model\"),\n    model_type: str = typer.Option(\"local\", help=\"Model type (local, huggingface, api)\"),\n):\n    \"\"\"Register a model for evaluation\"\"\"\n    # Implementation\n    pass\n\n@app.command()\ndef generate_report(\n    results_file: Path = typer.Argument(..., help=\"Path to results file\"),\n    output_file: Optional[Path] = typer.Option(None, help=\"Output file path\"),\n    format: str = typer.Option(\"md\", help=\"Report format (md, html, pdf)\"),\n):\n    \"\"\"Generate a human-readable report from results\"\"\"\n    # Implementation\n    pass\n\nif __name__ == \"__main__\":\n    app()\n```\n\n2. Configuration Handling:\n```python\nfrom pydantic import BaseModel\nfrom typing import Dict, List, Optional\n\nclass BenchmarkConfig(BaseModel):\n    model_id: str\n    tasks: Optional[List[str]] = None\n    metrics: Optional[Dict[str, List[str]]] = None\n    output_dir: str = \"./results\"\n    output_format: str = \"json\"\n    \n    @classmethod\n    def from_file(cls, file_path: Path) -> \"BenchmarkConfig\":\n        # Load from JSON/YAML file\n        pass\n```\n\n3. Rich Terminal Output:\n```python\nfrom rich.console import Console\nfrom rich.progress import Progress, TaskID\nfrom rich.table import Table\n\nconsole = Console()\n\ndef display_task_list(tasks: List[MedicalTask]):\n    table = Table(title=\"Available Tasks\")\n    table.add_column(\"ID\")\n    table.add_column(\"Type\")\n    table.add_column(\"Description\")\n    table.add_column(\"Metrics\")\n    \n    for task in tasks:\n        table.add_row(\n            task.task_id,\n            task.task_type.value,\n            task.description,\n            \", \".join(task.metrics)\n        )\n    \n    console.print(table)\n\ndef display_evaluation_progress(total_tasks: int):\n    with Progress() as progress:\n        task_id = progress.add_task(\"Evaluating...\", total=total_tasks)\n        # Update progress during evaluation\n```\n\nEnsure the CLI provides helpful error messages, proper documentation, and follows best practices for command-line interfaces.",
        "testStrategy": "1. Command tests:\n   - Test each command with various arguments\n   - Verify help text and documentation\n   - Test error handling for invalid inputs\n\n2. Integration tests:\n   - Test end-to-end workflows (register → evaluate → report)\n   - Verify correct file handling and output generation\n\n3. Usability tests:\n   - Review command structure with potential users\n   - Test with various terminal sizes and environments\n   - Verify accessibility features (color contrast, etc.)",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          3,
          4
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up CLI structure and base commands",
            "description": "Implement the basic CLI structure using Typer, including the main app setup, callback function, and command skeletons.",
            "dependencies": [],
            "details": "1. Create the CLI module structure\n2. Set up the Typer app instance\n3. Implement the callback function with appropriate documentation\n4. Define command function signatures for evaluate, list_tasks, register_model, and generate_report\n5. Add proper type hints and argument definitions\n6. Ensure help text is comprehensive and clear",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement evaluate command functionality",
            "description": "Complete the implementation of the evaluate command to run benchmarks on specified models and tasks.",
            "dependencies": [
              "6.1"
            ],
            "details": "1. Implement model loading based on model_id\n2. Add task filtering based on user input\n3. Create configuration handling from config_file\n4. Set up the evaluation pipeline execution\n5. Implement result storage in the specified output_dir\n6. Add support for different output formats (json, md, csv)",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement list_tasks and register_model commands",
            "description": "Complete the implementation of the list_tasks and register_model commands for task discovery and model registration.",
            "dependencies": [
              "6.1"
            ],
            "details": "1. Implement task discovery and listing functionality\n2. Create formatted output for task listing\n3. Implement model registration logic\n4. Add support for different model types (local, huggingface, api)\n5. Implement model configuration validation\n6. Create model ID generation for unspecified IDs",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement generate_report command and configuration handling",
            "description": "Complete the implementation of the generate_report command and the configuration management system.",
            "dependencies": [
              "6.1"
            ],
            "details": "1. Implement results file parsing\n2. Add support for different report formats (md, html, pdf)\n3. Create report generation logic\n4. Implement the BenchmarkConfig class\n5. Add configuration loading from files (JSON/YAML)\n6. Implement configuration validation",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement rich terminal output and progress tracking",
            "description": "Enhance the CLI with rich terminal output, progress bars, and formatted tables for improved user experience.",
            "dependencies": [
              "6.1",
              "6.2",
              "6.3",
              "6.4"
            ],
            "details": "1. Implement the display_task_list function with rich tables\n2. Create the display_evaluation_progress function with progress bars\n3. Add colorized output for important information\n4. Implement result summaries with formatted tables\n5. Add spinners for long-running operations\n6. Ensure consistent styling across all command outputs",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement error handling and CLI testing",
            "description": "Add comprehensive error handling to the CLI and create tests to ensure functionality and usability.",
            "dependencies": [
              "6.1",
              "6.2",
              "6.3",
              "6.4",
              "6.5"
            ],
            "details": "1. Implement exception handling for all commands\n2. Add user-friendly error messages\n3. Create input validation for all command arguments\n4. Implement command tests for each CLI function\n5. Create integration tests for end-to-end workflows\n6. Perform usability testing with sample scenarios",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement Model Interface",
        "description": "Create a standardized model interface that supports various model types (local, API-based, HuggingFace) and provides a consistent prediction method.",
        "details": "Develop a model interface system with the following components:\n\n1. Base Model Interface:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any\n\nclass ModelInterface(ABC):\n    @abstractmethod\n    def predict(self, inputs: List[Dict]) -> List[Dict]:\n        \"\"\"Run prediction on the given inputs\"\"\"\n        pass\n    \n    @property\n    @abstractmethod\n    def model_id(self) -> str:\n        \"\"\"Unique identifier for the model\"\"\"\n        pass\n    \n    @property\n    def metadata(self) -> Dict[str, Any]:\n        \"\"\"Model metadata (version, architecture, etc.)\"\"\"\n        return {}\n```\n\n2. Local Model Implementation:\n```python\nclass LocalModel(ModelInterface):\n    def __init__(self, model_path: str, model_id: str = None):\n        self._model = self._load_model(model_path)\n        self._model_id = model_id or os.path.basename(model_path)\n    \n    def _load_model(self, model_path: str):\n        # Implementation to load local model\n        pass\n    \n    def predict(self, inputs: List[Dict]) -> List[Dict]:\n        # Implementation to run prediction\n        pass\n    \n    @property\n    def model_id(self) -> str:\n        return self._model_id\n```\n\n3. HuggingFace Model Implementation:\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass HuggingFaceModel(ModelInterface):\n    def __init__(self, model_name: str, model_id: str = None):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self._model_id = model_id or model_name\n    \n    def predict(self, inputs: List[Dict]) -> List[Dict]:\n        results = []\n        for input_item in inputs:\n            # Process input and generate prediction\n            prompt = input_item.get(\"prompt\", \"\")\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n            outputs = self.model.generate(**inputs)\n            text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            results.append({\"text\": text})\n        return results\n    \n    @property\n    def model_id(self) -> str:\n        return self._model_id\n    \n    @property\n    def metadata(self) -> Dict[str, Any]:\n        return {\n            \"model_name\": self._model_id,\n            \"framework\": \"huggingface\",\n            \"parameters\": self.model.num_parameters()\n        }\n```\n\n4. API Model Implementation:\n```python\nimport requests\n\nclass APIModel(ModelInterface):\n    def __init__(self, api_url: str, api_key: str = None, model_id: str = None):\n        self.api_url = api_url\n        self.api_key = api_key\n        self._model_id = model_id or f\"api-{api_url.split('/')[-1]}\"\n    \n    def predict(self, inputs: List[Dict]) -> List[Dict]:\n        results = []\n        headers = {}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n        \n        for input_item in inputs:\n            response = requests.post(\n                self.api_url,\n                json=input_item,\n                headers=headers\n            )\n            response.raise_for_status()\n            results.append(response.json())\n        \n        return results\n    \n    @property\n    def model_id(self) -> str:\n        return self._model_id\n```\n\n5. Model Registry:\n```python\nclass ModelRegistry:\n    def __init__(self):\n        self._models = {}\n    \n    def register_model(self, model: ModelInterface):\n        self._models[model.model_id] = model\n    \n    def get_model(self, model_id: str) -> ModelInterface:\n        return self._models.get(model_id)\n    \n    def list_models(self) -> List[str]:\n        return list(self._models.keys())\n```",
        "testStrategy": "1. Unit tests for each model interface:\n   - Test initialization with various parameters\n   - Test prediction with sample inputs\n   - Verify output format consistency\n\n2. Mock API tests:\n   - Test API model with mocked responses\n   - Verify error handling for API failures\n\n3. Integration tests:\n   - Test model interfaces with the evaluation harness\n   - Verify compatibility with different task types\n   - Test performance monitoring and resource usage",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Finalize Base Model Interface Design",
            "description": "Complete the design and implementation of the abstract ModelInterface class with all required methods and properties.",
            "dependencies": [],
            "details": "Implement the ModelInterface ABC with proper type hints and docstrings. Ensure the interface includes:\n- Abstract predict method with proper input/output typing\n- Abstract model_id property\n- Optional metadata property\n- Error handling guidelines\n- Documentation for extension requirements",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Local Model Class",
            "description": "Create the LocalModel implementation that loads and runs locally stored models.",
            "dependencies": [
              "7.1"
            ],
            "details": "Complete the LocalModel class implementation with:\n- Model loading logic for common formats (pickle, joblib, etc.)\n- Prediction method implementation with proper input preprocessing\n- Error handling for file not found and model compatibility issues\n- Metadata extraction from local model files\n- Unit tests for various model types",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement HuggingFace Model Class",
            "description": "Develop the HuggingFaceModel implementation that works with transformer models from the HuggingFace library.",
            "dependencies": [
              "7.1"
            ],
            "details": "Complete the HuggingFaceModel implementation with:\n- Support for different model architectures (causal LM, seq2seq, etc.)\n- Efficient tokenization and generation parameters\n- Batching support for multiple inputs\n- Proper resource management (GPU allocation, memory efficiency)\n- Comprehensive metadata extraction from model config",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement API Model Class",
            "description": "Create the APIModel implementation that interfaces with external API-based models.",
            "dependencies": [
              "7.1"
            ],
            "details": "Complete the APIModel implementation with:\n- Robust HTTP request handling with timeouts and retries\n- Authentication support for different API providers\n- Rate limiting and throttling mechanisms\n- Error handling for API-specific responses\n- Response parsing and standardization\n- Support for async requests for performance optimization",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Develop Model Registry",
            "description": "Implement the ModelRegistry class for managing and accessing different model implementations.",
            "dependencies": [
              "7.1",
              "7.2",
              "7.3",
              "7.4"
            ],
            "details": "Complete the ModelRegistry implementation with:\n- Model registration and retrieval methods\n- Model validation on registration\n- Support for model versioning\n- Model metadata querying\n- Default model configuration\n- Persistence of registry state",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create Unit Tests for Model Implementations",
            "description": "Develop comprehensive unit tests for all model interface implementations.",
            "dependencies": [
              "7.2",
              "7.3",
              "7.4",
              "7.5"
            ],
            "details": "Create unit tests covering:\n- Initialization with various parameters\n- Prediction functionality with sample inputs\n- Error handling scenarios\n- Edge cases (empty inputs, large inputs)\n- Metadata property validation\n- Mock testing for external dependencies\n- Performance benchmarking tests",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Develop Integration Tests for Model Interface System",
            "description": "Create integration tests that verify the entire model interface system works together correctly.",
            "dependencies": [
              "7.5",
              "7.6"
            ],
            "details": "Implement integration tests that:\n- Test model registration and retrieval through the registry\n- Verify consistent output format across different model types\n- Test with real-world medical examples\n- Validate error propagation and handling\n- Test model switching and fallback mechanisms\n- Verify compatibility with the broader evaluation framework",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 8,
        "title": "Create Dataset Connectors",
        "description": "Implement interfaces for medical datasets (MIMIC, PubMed, etc.), data preprocessing pipelines, and secure data handling for sensitive medical information.",
        "details": "Develop dataset connectors with the following components:\n\n1. Base Dataset Interface:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Iterator\nfrom pathlib import Path\n\nclass DatasetConnector(ABC):\n    @abstractmethod\n    def load_data(self) -> Iterator[Dict]:\n        \"\"\"Load and yield data items\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_metadata(self) -> Dict:\n        \"\"\"Return dataset metadata\"\"\"\n        pass\n```\n\n2. Local File Dataset:\n```python\nimport json\nimport csv\nfrom pathlib import Path\n\nclass JSONDataset(DatasetConnector):\n    def __init__(self, file_path: Path, encryption_key: str = None):\n        self.file_path = file_path\n        self.encryption_key = encryption_key\n    \n    def load_data(self) -> Iterator[Dict]:\n        data = self._read_file()\n        for item in data:\n            yield item\n    \n    def _read_file(self) -> List[Dict]:\n        # Implementation with optional decryption\n        with open(self.file_path, 'r') as f:\n            data = json.load(f)\n        return data\n    \n    def get_metadata(self) -> Dict:\n        return {\n            \"source\": str(self.file_path),\n            \"format\": \"json\",\n            \"size\": self.file_path.stat().st_size\n        }\n\nclass CSVDataset(DatasetConnector):\n    def __init__(self, file_path: Path, encryption_key: str = None):\n        self.file_path = file_path\n        self.encryption_key = encryption_key\n    \n    def load_data(self) -> Iterator[Dict]:\n        with open(self.file_path, 'r') as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                yield dict(row)\n    \n    def get_metadata(self) -> Dict:\n        return {\n            \"source\": str(self.file_path),\n            \"format\": \"csv\",\n            \"size\": self.file_path.stat().st_size\n        }\n```\n\n3. Medical Database Connectors:\n```python\nclass MIMICConnector(DatasetConnector):\n    def __init__(self, connection_string: str, query: str):\n        self.connection_string = connection_string\n        self.query = query\n    \n    def load_data(self) -> Iterator[Dict]:\n        # Implementation with database connection\n        # Ensure secure handling of PHI\n        pass\n    \n    def get_metadata(self) -> Dict:\n        return {\n            \"source\": \"MIMIC\",\n            \"query\": self.query,\n            \"connection\": self.connection_string.split('@')[-1]  # Hide credentials\n        }\n\nclass PubMedConnector(DatasetConnector):\n    def __init__(self, search_terms: List[str], max_results: int = 100):\n        self.search_terms = search_terms\n        self.max_results = max_results\n    \n    def load_data(self) -> Iterator[Dict]:\n        # Implementation with PubMed API\n        pass\n    \n    def get_metadata(self) -> Dict:\n        return {\n            \"source\": \"PubMed\",\n            \"search_terms\": self.search_terms,\n            \"max_results\": self.max_results\n        }\n```\n\n4. Data Preprocessing Pipeline:\n```python\nfrom typing import Callable, List\n\nclass DataPreprocessor:\n    def __init__(self):\n        self.steps: List[Callable[[Dict], Dict]] = []\n    \n    def add_step(self, step: Callable[[Dict], Dict]):\n        self.steps.append(step)\n    \n    def process(self, data_item: Dict) -> Dict:\n        result = data_item.copy()\n        for step in self.steps:\n            result = step(result)\n        return result\n    \n    def process_batch(self, data_items: List[Dict]) -> List[Dict]:\n        return [self.process(item) for item in data_items]\n```\n\n5. Secure Data Handling:\n```python\nfrom cryptography.fernet import Fernet\nimport hashlib\n\nclass SecureDataHandler:\n    def __init__(self, encryption_key: str = None):\n        self.encryption_key = encryption_key\n        if encryption_key:\n            key = hashlib.sha256(encryption_key.encode()).digest()\n            self.cipher = Fernet(base64.urlsafe_b64encode(key))\n    \n    def encrypt_data(self, data: Dict) -> Dict:\n        if not self.encryption_key:\n            return data\n        \n        result = {}\n        for key, value in data.items():\n            if isinstance(value, str):\n                result[key] = self.cipher.encrypt(value.encode()).decode()\n            else:\n                result[key] = value\n        return result\n    \n    def decrypt_data(self, data: Dict) -> Dict:\n        if not self.encryption_key:\n            return data\n        \n        result = {}\n        for key, value in data.items():\n            if isinstance(value, str):\n                try:\n                    result[key] = self.cipher.decrypt(value.encode()).decode()\n                except Exception:\n                    result[key] = value  # Not encrypted or invalid\n            else:\n                result[key] = value\n        return result\n```",
        "testStrategy": "1. Unit tests for each connector:\n   - Test data loading functionality\n   - Verify metadata retrieval\n   - Test with sample datasets\n\n2. Security tests:\n   - Test encryption/decryption functionality\n   - Verify secure handling of sensitive data\n   - Test access controls and permissions\n\n3. Integration tests:\n   - Test connectors with task implementations\n   - Verify preprocessing pipeline functionality\n   - Test performance with large datasets",
        "priority": "medium",
        "dependencies": [
          1,
          2
        ],
        "status": "in-progress",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Base Dataset Interface",
            "description": "Finalize and implement the base DatasetConnector abstract class with all required methods and documentation.",
            "dependencies": [],
            "details": "Enhance the existing DatasetConnector ABC with:\n- Additional utility methods for data validation\n- Error handling mechanisms\n- Documentation for extending the interface\n- Type hints and docstrings\n- Unit tests for implementations",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Develop Local File Connectors",
            "description": "Complete the implementation of JSON and CSV dataset connectors with proper error handling and security features.",
            "dependencies": [
              "8.1"
            ],
            "details": "Extend the JSONDataset and CSVDataset classes with:\n- Robust error handling for file operations\n- Support for compressed files (gzip, zip)\n- Batch loading capabilities\n- Data validation methods\n- Integration with the SecureDataHandler for encrypted files\n- Comprehensive unit tests with sample datasets",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Medical Database Connectors",
            "description": "Complete the MIMIC and PubMed connectors with secure database connections and API integration.",
            "dependencies": [
              "8.1"
            ],
            "details": "Finalize the MIMICConnector and PubMedConnector implementations with:\n- Secure database connection handling\n- API rate limiting and error handling\n- Caching mechanisms for performance\n- PHI (Protected Health Information) safeguards\n- Query builders and filters\n- Authentication handling\n- Unit and integration tests with mock databases/APIs",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Enhance Data Preprocessing Pipeline",
            "description": "Extend the DataPreprocessor class with medical-specific preprocessing functions and pipeline management.",
            "dependencies": [
              "8.1",
              "8.2",
              "8.3"
            ],
            "details": "Improve the DataPreprocessor with:\n- Medical text normalization functions\n- Entity recognition preprocessing\n- Missing data handling strategies\n- Data augmentation capabilities\n- Parallel processing support\n- Pipeline serialization/deserialization\n- Preprocessing metrics and logging\n- Unit tests for each preprocessing step",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Secure Data Handling",
            "description": "Complete the SecureDataHandler implementation with robust encryption, anonymization, and compliance features.",
            "dependencies": [
              "8.1"
            ],
            "details": "Enhance the SecureDataHandler with:\n- Multiple encryption algorithms support\n- PHI detection and automatic anonymization\n- Compliance logging for audit trails\n- Key management and rotation capabilities\n- Field-level encryption options\n- Performance optimizations for large datasets\n- Security testing suite\n- Documentation on security best practices",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create Comprehensive Testing Suite",
            "description": "Develop a complete testing framework for all dataset connectors, including unit, integration, and security tests.",
            "dependencies": [
              "8.1",
              "8.2",
              "8.3",
              "8.4",
              "8.5"
            ],
            "details": "Build a testing suite with:\n- Unit tests for each connector class\n- Integration tests with sample datasets\n- Security tests for data handling\n- Performance benchmarks\n- Mock services for external APIs and databases\n- Test fixtures and factories\n- CI/CD integration\n- Documentation for testing procedures\n- Coverage reporting",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 9,
        "title": "Develop Reporting Interface",
        "description": "Create a reporting system that generates detailed evaluation reports in various formats (JSON, Markdown, etc.) and integrates with visualization tools.",
        "details": "Implement a reporting interface with the following components:\n\n1. Report Generator Base Class:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any\nfrom pathlib import Path\n\nclass ReportGenerator(ABC):\n    @abstractmethod\n    def generate(self, benchmark_report: BenchmarkReport) -> Any:\n        \"\"\"Generate report from benchmark results\"\"\"\n        pass\n    \n    @abstractmethod\n    def save(self, report: Any, output_path: Path) -> None:\n        \"\"\"Save report to file\"\"\"\n        pass\n```\n\n2. JSON Report Generator:\n```python\nimport json\n\nclass JSONReportGenerator(ReportGenerator):\n    def generate(self, benchmark_report: BenchmarkReport) -> Dict:\n        return benchmark_report.dict()\n    \n    def save(self, report: Dict, output_path: Path) -> None:\n        with open(output_path, 'w') as f:\n            json.dump(report, f, indent=2)\n```\n\n3. Markdown Report Generator:\n```python\nclass MarkdownReportGenerator(ReportGenerator):\n    def generate(self, benchmark_report: BenchmarkReport) -> str:\n        lines = []\n        \n        # Header\n        lines.append(f\"# Benchmark Report: {benchmark_report.model_id}\")\n        lines.append(f\"Generated at: {benchmark_report.timestamp}\\n\")\n        \n        # Overall scores\n        lines.append(\"## Overall Scores\")\n        for metric, score in benchmark_report.overall_scores.items():\n            lines.append(f\"- **{metric}**: {score:.4f}\")\n        lines.append(\"\")\n        \n        # Task scores\n        lines.append(\"## Task Scores\")\n        for task_id, scores in benchmark_report.task_scores.items():\n            lines.append(f\"### {task_id}\")\n            for metric, score in scores.items():\n                lines.append(f\"- **{metric}**: {score:.4f}\")\n            lines.append(\"\")\n        \n        # Detailed results\n        lines.append(\"## Detailed Results\")\n        for result in benchmark_report.detailed_results:\n            lines.append(f\"### Task: {result.task_id}\")\n            lines.append(\"#### Metrics\")\n            for metric, score in result.metrics_results.items():\n                lines.append(f\"- **{metric}**: {score:.4f}\")\n            lines.append(\"\")\n        \n        return \"\\n\".join(lines)\n    \n    def save(self, report: str, output_path: Path) -> None:\n        with open(output_path, 'w') as f:\n            f.write(report)\n```\n\n4. HTML Report Generator:\n```python\nclass HTMLReportGenerator(ReportGenerator):\n    def generate(self, benchmark_report: BenchmarkReport) -> str:\n        # Implementation using a template engine or direct HTML generation\n        pass\n    \n    def save(self, report: str, output_path: Path) -> None:\n        with open(output_path, 'w') as f:\n            f.write(report)\n```\n\n5. Visualization Integration:\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nclass VisualizationGenerator:\n    def __init__(self, benchmark_report: BenchmarkReport):\n        self.report = benchmark_report\n    \n    def generate_metric_comparison(self, output_path: Path) -> None:\n        # Create DataFrame from results\n        data = []\n        for task_id, scores in self.report.task_scores.items():\n            for metric, score in scores.items():\n                data.append({\"Task\": task_id, \"Metric\": metric, \"Score\": score})\n        \n        df = pd.DataFrame(data)\n        \n        # Create visualization\n        plt.figure(figsize=(12, 8))\n        pivot = df.pivot(index=\"Task\", columns=\"Metric\", values=\"Score\")\n        pivot.plot(kind=\"bar\")\n        plt.title(f\"Metric Comparison: {self.report.model_id}\")\n        plt.ylabel(\"Score\")\n        plt.tight_layout()\n        \n        # Save figure\n        plt.savefig(output_path)\n```\n\n6. Report Factory:\n```python\nclass ReportFactory:\n    @staticmethod\n    def create_generator(format: str) -> ReportGenerator:\n        if format.lower() == \"json\":\n            return JSONReportGenerator()\n        elif format.lower() == \"md\":\n            return MarkdownReportGenerator()\n        elif format.lower() == \"html\":\n            return HTMLReportGenerator()\n        else:\n            raise ValueError(f\"Unsupported format: {format}\")\n```",
        "testStrategy": "1. Unit tests for each report generator:\n   - Test report generation with sample data\n   - Verify output format correctness\n   - Test with various report sizes and content\n\n2. Format validation tests:\n   - Validate JSON output against schema\n   - Verify Markdown syntax correctness\n   - Test HTML validity\n\n3. Visualization tests:\n   - Test chart generation with sample data\n   - Verify image output quality and correctness\n   - Test with various data distributions",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Complete HTML Report Generator Implementation",
            "description": "Implement the HTML Report Generator class that was only partially defined in the initial code. This should include proper HTML formatting with CSS styling for readability.",
            "dependencies": [],
            "details": "Complete the `HTMLReportGenerator` class implementation:\n1. Implement the `generate` method to create HTML reports with proper structure\n2. Include CSS styling for better readability\n3. Add tables for metrics and results\n4. Implement collapsible sections for detailed results\n5. Ensure the implementation handles all data from the BenchmarkReport object",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Enhance Visualization Integration",
            "description": "Expand the visualization capabilities by adding more chart types and visualization options to the VisualizationGenerator class.",
            "dependencies": [],
            "details": "Enhance the `VisualizationGenerator` class with:\n1. Add radar charts for comparing multiple metrics across tasks\n2. Implement heatmap visualization for task performance\n3. Create time-series plots for model performance over multiple runs\n4. Add export options for different image formats (PNG, SVG, PDF)\n5. Implement interactive visualization options using libraries like Plotly",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement CSV Report Generator",
            "description": "Create a new CSV Report Generator class that extends the ReportGenerator base class to output benchmark results in CSV format.",
            "dependencies": [],
            "details": "Implement a new `CSVReportGenerator` class:\n1. Create class structure extending ReportGenerator\n2. Implement generate method to convert BenchmarkReport to CSV format\n3. Implement save method to write CSV data to file\n4. Handle proper formatting of nested data structures\n5. Update the ReportFactory to support the new CSV format",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Create Report Format Validation System",
            "description": "Implement a validation system to ensure generated reports conform to expected formats and contain all required information.",
            "dependencies": [
              "9.1",
              "9.3"
            ],
            "details": "Develop a report validation system:\n1. Create schema definitions for each report format\n2. Implement validators for JSON, Markdown, HTML, and CSV formats\n3. Add validation methods to each ReportGenerator class\n4. Create unit tests to verify validation functionality\n5. Implement error reporting for validation failures",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Develop Report Generation Integration Tests",
            "description": "Create comprehensive integration tests for the reporting system to ensure all components work together correctly.",
            "dependencies": [
              "9.1",
              "9.2",
              "9.3",
              "9.4"
            ],
            "details": "Implement integration tests for the reporting system:\n1. Create test fixtures with sample benchmark data\n2. Test end-to-end report generation in all formats\n3. Verify visualization integration works correctly\n4. Test report factory with various format options\n5. Implement performance tests for large benchmark reports",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 10,
        "title": "Create Documentation and Examples",
        "description": "Develop comprehensive documentation including developer guides, usage examples, and contribution guidelines to facilitate adoption and extension of the benchmark.",
        "details": "Create documentation and examples with the following components:\n\n1. Project Documentation Structure:\n```\ndocs/\n├── getting_started.md\n├── architecture.md\n├── tasks/\n│   ├── overview.md\n│   ├── medical_qa.md\n│   ├── diagnostic_reasoning.md\n│   └── clinical_summarization.md\n├── metrics/\n│   ├── overview.md\n│   ├── clinical_accuracy.md\n│   └── reasoning_quality.md\n├── models/\n│   ├── model_interface.md\n│   ├── local_models.md\n│   └── api_models.md\n├── api/\n│   ├── core_api.md\n│   └── cli.md\n└── contributing.md\n```\n\n2. Getting Started Guide:\n```markdown\n# Getting Started with MedAISure\n\n## Installation\n\n```bash\n# Install from PyPI\npip install medaisure\n\n# Or install from source\ngit clone https://github.com/junaidi-ai/MedAISure.git\ncd benchmark\npip install -e .\n```\n\n## Quick Start\n\n```python\nfrom medaisure import EvaluationHarness\nfrom medaisure.models import HuggingFaceModel\n\n# Initialize a model\nmodel = HuggingFaceModel(\"gpt2\")\n\n# Create evaluation harness\nharness = EvaluationHarness()\n\n# Run evaluation\nresults = harness.evaluate(model, tasks=[\"medical_qa_basic\"])\n\n# Print results\nprint(results.overall_scores)\n```\n\n## Using the CLI\n\n```bash\n# List available tasks\nmedaisure list-tasks\n\n# Register a model\nmedaisure register-model ./my_model --model-id my-model\n\n# Run evaluation\nmedaisure evaluate my-model --tasks medical_qa_basic\n\n# Generate report\nmedaisure generate-report ./results/my-model_results.json --format md\n```\n```\n\n3. Example Notebooks:\n- Create Jupyter notebooks demonstrating key workflows:\n  - Basic model evaluation\n  - Custom task creation\n  - Result analysis and visualization\n  - Advanced configuration options\n\n4. API Documentation:\n- Generate comprehensive API documentation using Sphinx or a similar tool\n- Include docstrings for all public classes and methods\n- Provide usage examples for each component\n\n5. Contribution Guidelines:\n```markdown\n# Contributing to MedAISure\n\n## Code of Conduct\n\nThis project adheres to the Contributor Covenant code of conduct. By participating, you are expected to uphold this code.\n\n## How to Contribute\n\n### Reporting Bugs\n\n- Use the issue tracker to report bugs\n- Describe the bug and include specific details to help reproduce it\n- Include sample code if possible\n\n### Suggesting Enhancements\n\n- Use the issue tracker to suggest enhancements\n- Clearly describe the enhancement and its expected behavior\n- Explain why this enhancement would be useful\n\n### Pull Requests\n\n1. Fork the repository\n2. Create a new branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Run tests (`pytest`)\n5. Commit your changes (`git commit -m 'Add some amazing feature'`)\n6. Push to the branch (`git push origin feature/amazing-feature`)\n7. Open a Pull Request\n\n## Development Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/MedAISure.git\ncd MedAISure\n\n# Create a virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install development dependencies\npip install -e \".[dev]\"\n\n# Run tests\npytest\n```\n\n## Coding Standards\n\n- Follow PEP 8 style guide\n- Write docstrings for all functions, classes, and methods\n- Maintain test coverage for all code\n```\n\n6. Task Development Guide:\n```markdown\n# Creating Custom Tasks\n\n## Task Interface\n\nAll tasks must implement the `MedicalTask` interface:\n\n```python\nfrom medaisure.tasks import MedicalTask, TaskType\n\nclass MyCustomTask(MedicalTask):\n    def __init__(self, task_id: str, description: str):\n        super().__init__(\n            task_id=task_id,\n            task_type=TaskType.QA,  # Or another appropriate type\n            description=description,\n            inputs=[],\n            expected_outputs=[],\n            metrics=[\"accuracy\", \"my_custom_metric\"]\n        )\n    \n    def load_data(self, data_path: str):\n        # Implementation to load task data\n        pass\n```\n\n## Registering Your Task\n\n```python\nfrom medaisure.registry import task_registry\n\n# Register your task\ntask_registry.register_task(MyCustomTask(\"my_custom_task\", \"Description of my task\"))\n```\n\n## Example Task Implementation\n\n[Detailed example with complete implementation]\n```",
        "testStrategy": "1. Documentation tests:\n   - Verify all links work correctly\n   - Test code examples for correctness\n   - Ensure documentation is up-to-date with code\n\n2. Example tests:\n   - Run all example notebooks to verify functionality\n   - Test with different environments and configurations\n\n3. User testing:\n   - Have new users follow documentation to complete tasks\n   - Collect feedback on clarity and completeness\n   - Identify areas for improvement",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          3,
          4,
          6,
          7,
          8,
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop Project Documentation Structure",
            "description": "Create the documentation directory structure and implement core documentation files according to the specified hierarchy.",
            "dependencies": [],
            "details": "1. Create the documentation directory structure as specified in the requirements\n2. Implement core documentation files including:\n   - getting_started.md with installation instructions\n   - architecture.md with system overview\n   - contributing.md with contribution guidelines\n3. Set up documentation build system using Sphinx or MkDocs\n4. Configure automatic API documentation generation\n5. Implement documentation testing to verify links and examples",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create Task and API Documentation",
            "description": "Develop detailed documentation for all tasks, metrics, models, and API components of the benchmark.",
            "dependencies": [
              "10.1"
            ],
            "details": "1. Create task documentation files:\n   - tasks/overview.md with general task information\n   - Individual task documentation (medical_qa.md, diagnostic_reasoning.md, clinical_summarization.md)\n2. Develop metrics documentation:\n   - metrics/overview.md explaining evaluation approach\n   - Individual metric documentation (clinical_accuracy.md, reasoning_quality.md)\n3. Write model interface documentation:\n   - model_interface.md with integration guidelines\n   - local_models.md and api_models.md with specific implementation details\n4. Document API components:\n   - core_api.md with programmatic interface documentation\n   - cli.md with command-line usage instructions",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Develop Example Notebooks and Tutorials",
            "description": "Create Jupyter notebooks and tutorials demonstrating key workflows and use cases for the benchmark.",
            "dependencies": [
              "10.1",
              "10.2"
            ],
            "details": "1. Create basic model evaluation notebook showing:\n   - Model initialization\n   - Task selection\n   - Evaluation execution\n   - Results interpretation\n2. Develop custom task creation tutorial with:\n   - Task interface implementation\n   - Data loading\n   - Metric selection\n   - Registration process\n3. Implement result analysis and visualization notebook:\n   - Loading evaluation results\n   - Generating visualizations\n   - Comparative analysis\n4. Create advanced configuration notebook showing:\n   - Custom metrics\n   - Evaluation parameters\n   - Batch processing\n   - Performance optimization",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Developer Guides and Contribution Documentation",
            "description": "Create comprehensive developer guides and contribution documentation to facilitate community involvement.",
            "dependencies": [
              "10.1",
              "10.2"
            ],
            "details": "1. Develop detailed contribution guidelines including:\n   - Code of conduct\n   - Bug reporting process\n   - Feature request workflow\n   - Pull request procedures\n2. Create development setup guide with:\n   - Environment setup instructions\n   - Testing procedures\n   - Code style guidelines\n3. Implement task development guide with:\n   - Task interface documentation\n   - Registration process\n   - Example implementations\n4. Create model integration guide explaining:\n   - Model interface requirements\n   - Performance considerations\n   - Testing requirements\n5. Document release process and versioning strategy",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 11,
        "title": "Implement Weighted Combined Score Aggregation",
        "description": "Add a Combined Score metric aggregated with PRD-defined weights: 40% diagnostics, 30% safety, 20% communication, 10% summarization.",
        "details": "Implement Combined Score support in the reporting/aggregation layer:\n\n1. Configuration:\n- Add a configuration structure for weighted combined scoring with defaults: diagnostics=0.40, safety=0.30, communication=0.20, summarization=0.10\n- Allow override via config file or CLI flag\n\n2. Aggregation Logic (ResultAggregator):\n- Compute task-type group scores\n- Apply weights and produce a single Combined Score in overall_scores\n- Validate all required component scores exist; handle missing components gracefully\n\n3. Metrics and Naming:\n- Standardize metric keys to align with README/PRD categories\n- Add unit tests for weight application and edge cases\n\n4. Reporting:\n- Include Combined Score in JSON/Markdown/HTML reports\n- Document the formula in report summaries",
        "testStrategy": "1. Unit tests:\n- Validate weight defaults and overrides\n- Test aggregation with synthetic inputs covering each category\n- Verify rounding/formatting in reports\n\n2. Integration tests:\n- End-to-end evaluation producing component metrics and Combined Score\n\n3. Regression tests:\n- Ensure changes do not break existing metric aggregation",
        "priority": "high",
        "dependencies": [
          1,
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add weighted scoring config and parser",
            "description": "Introduce a config section and CLI flags to control Combined Score weights with sane defaults.",
            "dependencies": [],
            "details": "- Config schema updates\n- CLI flags: --combined-weights diagnostics=0.4,safety=0.3,communication=0.2,summarization=0.1\n- Validation of provided weights (sum to 1.0)",
            "status": "pending",
            "testStrategy": "Config parsing and validation tests"
          },
          {
            "id": 2,
            "title": "Implement aggregation in ResultAggregator",
            "description": "Compute Combined Score from component categories with proper error handling.",
            "dependencies": [],
            "details": "- Map metrics to categories\n- Aggregate per-category scores\n- Apply weights and compute final Combined Score",
            "status": "pending",
            "testStrategy": "Deterministic synthetic inputs to validate outputs"
          },
          {
            "id": 3,
            "title": "Expose Combined Score in reports",
            "description": "Render Combined Score in JSON/MD/HTML reports and docs.",
            "dependencies": [],
            "details": "- Update report generators\n- Add documentation blurb on formula",
            "status": "pending",
            "testStrategy": "Snapshot tests for rendered reports"
          }
        ]
      },
      {
        "id": 12,
        "title": "Define Dataset Specifications and Registry",
        "description": "Create dataset registry entries and specs for MedAISure-Core (200 tasks) and placeholders for Hard/Specialty/Multimodal (planned).",
        "details": "Implement dataset specifications aligned with PRD:\n\n1. Dataset Registry:\n- Define registry entries: medaisure-core, medaisure-hard (planned), medaisure-specialty (planned), medaisure-multimodal (planned)\n- Include metadata: size, task categories, source links\n\n2. MedAISure-Core:\n- Specify 200-task composition: Diagnostics 100, Summarization 50, Communication 50\n- Provide loader stubs and validation hooks\n\n3. Docs and Examples:\n- Document how to load datasets via CLI and Python API\n- Reference README dataset links",
        "testStrategy": "- Unit tests for registry lookups and metadata\n- Loader stub tests for medaisure-core\n- Validation of dataset composition metadata",
        "priority": "medium",
        "dependencies": [
          1,
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create dataset registry and metadata schema",
            "description": "Introduce a DatasetRegistry with schema-validated entries.",
            "dependencies": [],
            "details": "- Registry class and JSON schema\n- CRUD for dataset entries\n- Basic CLI/Python API to list datasets",
            "status": "pending",
            "testStrategy": "Schema validation and list command tests"
          },
          {
            "id": 2,
            "title": "Define MedAISure-Core specification",
            "description": "Add composition and loader stubs for the 200-task core set.",
            "dependencies": [],
            "details": "- Composition metadata (counts per category)\n- Loader interfaces and validation hooks\n- Example listing command output",
            "status": "pending",
            "testStrategy": "Loader stub tests and metadata checks"
          }
        ]
      },
      {
        "id": 13,
        "title": "Implement Leaderboard Submission Exporter",
        "description": "Create an exporter for leaderboard submissions (predictions + reasoning traces) and validation against expected schema.",
        "details": "- Define submission schema (JSON) including predictions and optional reasoning traces\n- Implement exporter in EvaluationHarness/ResultAggregator to save submission-ready artifacts\n- Add CLI command: generate-submission --run-id <id> --out <file.json>\n- Validate file against schema before write\n- Document submission process and links",
        "testStrategy": "- Schema validation tests\n- End-to-end test generating a submission file\n- Negative tests for invalid data",
        "priority": "medium",
        "dependencies": [
          1,
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Design Federated Evaluation Scaffold (Planned)",
        "description": "Create a design and code scaffold for a future federated evaluation mode consistent with the PRD.",
        "details": "- Draft architecture docs (secure comms, local runners, aggregation)\n- Define interfaces and TODO stubs behind a feature flag\n- Ensure no side effects until feature is enabled\n- Link to security considerations in PRD",
        "testStrategy": "- Lint/build checks only (no runtime until enabled)\n- Docs presence and link checks",
        "priority": "low",
        "dependencies": [
          1
        ],
        "status": "planned",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-07-28T05:24:41.681Z",
      "updated": "2025-08-23T00:29:52.023Z",
      "description": "Tasks for master context"
    }
  }
}

{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api_reference/","title":"API Reference (concise)","text":"<p>References for primary public classes. Read the code for full details.</p>"},{"location":"api_reference/#evaluationharness-benchevaluationharnesspy","title":"EvaluationHarness (<code>bench/evaluation/harness.py</code>)","text":"<ul> <li><code>__init__(tasks_dir, results_dir, cache_dir=None, log_level=\"INFO\", callbacks...)</code></li> <li><code>evaluate(model_id, task_ids, model_type=\"huggingface\", batch_size=8, use_cache=True, save_results=True, report_formats: Optional[List[str]] = None, report_dir: Optional[str] = None, **model_kwargs) -&gt; BenchmarkReport</code></li> <li><code>list_available_tasks() -&gt; List[dict]</code></li> <li><code>get_task_info(task_id) -&gt; dict</code></li> </ul> <p>Notes: - <code>evaluate()</code> loads the model via <code>ModelRunner.load_model()</code> then iterates tasks. - Metrics are computed via <code>MetricCalculator.calculate_metrics()</code>. - Results aggregated via <code>ResultAggregator.add_evaluation_result()</code> and returned as <code>BenchmarkReport</code>. - When <code>report_formats</code> is provided (e.g. <code>[\"html\", \"md\"]</code>), extra reports are generated and saved to <code>report_dir</code> (defaults to <code>results_dir</code>).</p> <p>Example \u2013 basic run: <pre><code>from bench.evaluation import EvaluationHarness\nh = EvaluationHarness(tasks_dir=\"bench/tasks\", results_dir=\"bench/results\")\ntasks = h.list_available_tasks()\nreport = h.evaluate(\n    model_id=\"textattack/bert-base-uncased-MNLI\",\n    task_ids=[tasks[0][\"task_id\"]],\n    model_type=\"huggingface\",\n    batch_size=4,\n    report_formats=[\"html\", \"md\"],\n    report_dir=\"bench/reports\",\n)\nprint(report.overall_scores)\n</code></pre></p> <p>Example \u2013 get task info: <pre><code>info = h.get_task_info(tasks[0][\"task_id\"])\nprint(info[\"name\"], info.get(\"metrics\"))\n</code></pre></p>"},{"location":"api_reference/#taskloader-benchevaluationtask_loaderpy","title":"TaskLoader (<code>bench/evaluation/task_loader.py</code>)","text":"<ul> <li><code>load_task(task_id: str) -&gt; MedicalTask</code> (task_id can be id, local file path, or HTTP(S) URL)</li> <li><code>load_tasks(task_ids: List[str]) -&gt; Dict[str, MedicalTask&gt;</code></li> <li><code>discover_tasks() -&gt; Dict[str, str]</code></li> <li><code>list_available_tasks() -&gt; List[dict]</code></li> </ul> <p>Examples: <pre><code>from bench.evaluation.task_loader import TaskLoader\ntl = TaskLoader(tasks_dir=\"bench/tasks\")\ntask = tl.load_task(\"clinical_summarization_discharge\")\nall_tasks = tl.list_available_tasks()\n</code></pre></p>"},{"location":"api_reference/#modelrunner-benchevaluationmodel_runnerpy","title":"ModelRunner (<code>bench/evaluation/model_runner.py</code>)","text":"<ul> <li><code>load_model(model_name, model_type=\"local\", model_path=None, **kwargs)</code></li> <li><code>model_type in {\"huggingface\", \"local\", \"api\"}</code></li> <li>HuggingFace options:<ul> <li><code>hf_task</code>: <code>text-classification | summarization | text-generation</code></li> <li><code>model_kwargs</code>, <code>tokenizer_kwargs</code>, <code>pipeline_kwargs</code>, <code>device</code>, <code>num_labels</code></li> <li>Generation: <code>generation_kwargs</code> (used in <code>run_model()</code> for summarization/text-generation)</li> <li>Advanced loading: <code>device_map</code>, <code>torch_dtype</code>, <code>low_cpu_mem_usage</code>, <code>revision</code>, <code>trust_remote_code</code></li> </ul> </li> <li>Local: requires <code>module_path</code>, optional <code>load_func</code> (default <code>load_model</code>)</li> <li>API: requires <code>endpoint</code>, <code>api_key</code>, optional <code>timeout</code>, <code>max_retries</code>, <code>backoff_factor</code>, <code>headers</code></li> <li><code>run_model(model_id, inputs, batch_size=8, **kwargs) -&gt; List[dict]</code></li> <li><code>unload_model(model_name)</code></li> </ul> <p>Examples: <pre><code>from bench.evaluation.model_runner import ModelRunner\nmr = ModelRunner()\n# HF summarization with generation params\nmr.load_model(\n    \"sshleifer/distilbart-cnn-12-6\",\n    model_type=\"huggingface\",\n    hf_task=\"summarization\",\n    generation_kwargs={\"max_new_tokens\": 128, \"temperature\": 0.7, \"do_sample\": True},\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n)\npreds = mr.run_model(\"sshleifer/distilbart-cnn-12-6\", inputs=[{\"text\": \"note\"}], batch_size=1)\n\n# Local\nmr.load_model(\"my_local\", model_type=\"local\", module_path=\"bench.examples.mypkg.mylocal\")\npreds = mr.run_model(\"my_local\", inputs=[{\"text\": \"x\"}], batch_size=2)\n</code></pre></p> <p>Returned prediction dicts typically include <code>{\"label\", \"score\"}</code> for classification or <code>{\"summary\"|\"text\"|\"prediction\"}</code> for generative tasks.</p> <p>See runnable examples: - <code>bench/examples/run_hf_summarization_gen.py</code> - <code>bench/examples/run_hf_text_generation_gen.py</code></p>"},{"location":"api_reference/#localmodel-benchevaluationmodel_interfacepy","title":"LocalModel (<code>bench/evaluation/model_interface.py</code>)","text":"<ul> <li><code>LocalModel(predict_fn: Optional[Callable]=None, model_path: Optional[str]=None, loader: Optional[Callable]=None, model_id: Optional[str]=None)</code></li> <li>Load local models directly from files or wrap a Python callable.</li> <li>Auto-loaders by extension:<ul> <li><code>.pkl</code>/<code>.pickle</code> \u2192 <code>pickle.load</code></li> <li><code>.joblib</code> \u2192 <code>joblib.load</code> (optional dependency)</li> <li><code>.pt</code>/<code>.pth</code> \u2192 <code>torch.load</code> (optional dependency)</li> </ul> </li> <li>Prediction: calls the model object (if callable) or its <code>.predict()</code> method; otherwise uses <code>predict_fn</code>.</li> <li><code>metadata</code> includes <code>file_path</code>, <code>ext</code>, <code>file_size</code>, <code>file_mtime</code>, <code>object_class</code>, <code>object_module</code>, and inferred <code>framework</code>.</li> </ul> <p>Examples (runnable scripts):</p> <pre><code>python bench/examples/run_localmodel_pickle.py\npython bench/examples/run_localmodel_joblib.py   # requires joblib\npython bench/examples/run_localmodel_torch.py    # requires torch\n</code></pre> <p>Inline usage \u2013 Pickle: <pre><code>from bench.evaluation.model_interface import LocalModel\nlm = LocalModel(model_path=\".local_models/echo.pkl\")\npreds = lm.predict([{\"text\": \"hello\"}, {\"text\": \"world\"}])\nprint(preds, lm.metadata)\n</code></pre></p> <p>Inline usage \u2013 Joblib: <pre><code>from bench.evaluation.model_interface import LocalModel\nlm = LocalModel(model_path=\".local_models/echo.joblib\")\npreds = lm.predict([{\"a\": 1}, {\"bbb\": 2}])\nprint(preds)\n</code></pre></p> <p>Inline usage \u2013 Torch: <pre><code>from bench.evaluation.model_interface import LocalModel\nlm = LocalModel(model_path=\".local_models/echo.pt\")\npreds = lm.predict([{\"x\": 0.1}, {\"x\": 0.9}])\nprint(preds)\n</code></pre></p>"},{"location":"api_reference/#metriccalculator-benchevaluationmetric_calculatorpy","title":"MetricCalculator (<code>bench/evaluation/metric_calculator.py</code>)","text":"<ul> <li>Built-ins: <code>accuracy</code>, <code>precision</code>, <code>recall</code>, <code>f1</code>, <code>roc_auc</code>, <code>average_precision</code>, <code>mse</code>, <code>mae</code>, <code>r2</code></li> <li>Medical: <code>diagnostic_accuracy</code>, <code>clinical_correctness</code>, <code>reasoning_quality</code>, <code>rouge_l</code>, <code>clinical_relevance</code>, <code>factual_consistency</code></li> <li><code>register_metric(name, fn, **default_kwargs)</code></li> <li><code>calculate_metrics(task_id, predictions, references, metric_names=None, **metric_kwargs) -&gt; Dict[str, MetricResult]</code></li> <li><code>aggregate_metrics(metric_results, aggregation=\"mean\") -&gt; Dict[str, MetricResult]</code></li> </ul> <p>Examples: <pre><code>from bench.evaluation.metric_calculator import MetricCalculator\nmc = MetricCalculator()\n\npreds = [{\"label\": \"yes\"}, {\"label\": \"no\"}]\nrefs = [{\"label\": \"yes\"}, {\"label\": \"no\"}]\nres = mc.calculate_metrics(\"demo\", preds, refs, metric_names=[\"accuracy\"])\n\ndef exact_match(y_true, y_pred, **kw):\n    return float(sum(int(t == p) for t, p in zip(y_true, y_pred)) / len(y_true))\nmc.register_metric(\"exact_match\", exact_match)\nres2 = mc.calculate_metrics(\"demo\", preds, refs, metric_names=[\"exact_match\"])\n</code></pre></p>"},{"location":"api_reference/#resultaggregator-benchevaluationresult_aggregatorpy","title":"ResultAggregator (<code>bench/evaluation/result_aggregator.py</code>)","text":"<ul> <li><code>add_evaluation_result(EvaluationResult, run_id=None)</code></li> <li><code>get_report(run_id) -&gt; BenchmarkReport</code></li> <li><code>export_report_json|csv|markdown|html(run_id, output)</code></li> <li><code>aggregate_statistics(run_id, metrics=None, percentiles=None, tasks=None)</code></li> <li><code>filter_and_sort_tasks(run_id, tasks=None, metrics=None, sort_by=None, descending=True)</code></li> <li><code>compare_runs(run_a, run_b, metrics=None, relative=False)</code></li> </ul> <p>Examples: <pre><code>from bench.evaluation.result_aggregator import ResultAggregator\nfrom bench.models import EvaluationResult\n\nra = ResultAggregator(output_dir=\"bench/results\")\ner = EvaluationResult(\n    model_id=\"demo\",\n    task_id=\"task1\",\n    inputs=[{\"text\": \"a\"}],\n    model_outputs=[{\"label\": \"y\"}],\n    metrics_results={},\n)\nra.add_evaluation_result(er, run_id=\"run-1\")\nreport = ra.get_report(\"run-1\")\nra.export_report_json(\"run-1\", output_path=\"bench/results/run-1.json\")\n</code></pre></p>"},{"location":"api_reference/#data-models-benchmodels","title":"Data Models (<code>bench/models/</code>)","text":"<ul> <li><code>MedicalTask</code>: fields include <code>task_id</code>, <code>task_type</code>, <code>name</code>, <code>description</code>, <code>inputs</code>, <code>expected_outputs</code>, <code>metrics</code>, <code>input_schema</code>, <code>output_schema</code>, <code>dataset</code></li> <li><code>EvaluationResult</code>: <code>model_id</code>, <code>task_id</code>, <code>inputs</code>, <code>model_outputs</code>, <code>metrics_results</code>, <code>metadata</code>, <code>timestamp</code></li> <li><code>BenchmarkReport</code>: overall/task scores + <code>detailed_results</code>, JSON save/load helpers</li> </ul>"},{"location":"api_reference/#advanced-serialization-models","title":"Advanced Serialization (Models)","text":"<ul> <li> <p><code>schema_version</code>: int, default <code>1</code> on all models for forward compatibility</p> </li> <li> <p>Partial serialization (all models):</p> </li> <li><code>to_dict(include=None, exclude=None)</code></li> <li> <p><code>to_json(indent=None, include=None, exclude=None)</code></p> </li> <li> <p>YAML support (all models):</p> </li> <li><code>to_yaml()</code> \u2192 YAML string</li> <li> <p><code>from_yaml(text: str)</code> \u2192 instance</p> </li> <li> <p>File I/O and conversion (all models):</p> </li> <li><code>save(path, format=None)</code> \u2192 format inferred from extension (<code>.json</code>, <code>.yaml</code>, <code>.yml</code>) or by <code>format</code></li> <li><code>from_file(path)</code> \u2192 loads JSON/YAML by extension</li> <li> <p><code>convert(to)</code> \u2192 returns string in <code>json</code> or <code>yaml</code></p> </li> <li> <p>CSV helpers:</p> </li> <li><code>EvaluationResult</code>: <code>inputs_to_csv()</code>, <code>outputs_to_csv()</code>, <code>from_inputs_csv(model_id, task_id, csv_text)</code>, <code>from_outputs_csv(model_id, task_id, csv_text)</code></li> <li><code>MedicalTask</code>: <code>dataset_to_csv()</code></li> <li><code>BenchmarkReport</code>: <code>overall_scores_to_csv()</code>, <code>task_scores_to_csv()</code></li> </ul> <p>Examples: <pre><code>from bench.models import MedicalTask, EvaluationResult, BenchmarkReport\n\n# YAML round-trip\ntask_yaml = task.to_yaml()\ntask2 = MedicalTask.from_yaml(task_yaml)\n\n# Partial JSON (only overall_scores)\nreport_json = report.to_json(indent=2, include={\"overall_scores\": True})\n\n# Save/Load\nresult.save(\"res.yaml\")\nres2 = EvaluationResult.from_file(\"res.yaml\")\n\n# CSV exports\ncsv_inputs = res2.inputs_to_csv()\ncsv_overall = report.overall_scores_to_csv()\n\n## CLI (Typer) \u2013 Evaluate\n\nFile: `bench/cli_typer.py`\n\n- `--tasks &lt;id&gt;`: select tasks (repeatable)\n- `--output-dir &lt;path&gt;`: base report directory\n- `--format &lt;fmt&gt;`: base report format (`json|yaml|md|csv`)\n- `--extra-report &lt;fmt&gt;`: extra report export (repeatable). Example: `--extra-report html --extra-report md`\n- `--report-dir &lt;path&gt;`: directory for extra reports (defaults to `--output-dir`)\n- `--config-file &lt;path&gt;`: JSON/YAML config (see `docs/configuration.md`)\n - `--html-open-metadata [true|false]`: open Metadata sections by default in generated HTML reports\n - `--html-preview-limit &lt;int&gt;`: max items to show in inputs/outputs previews in HTML before truncation\n\nExample:\n\n```bash\n# Base JSON report + extra HTML/MD exports into custom directory\nMEDAISURE_NO_RICH=1 python -m bench.cli_typer evaluate textattack/bert-base-uncased-MNLI \\\n  --tasks clinical_icd10_classification \\\n  --output-dir results \\\n  --format json \\\n  --extra-report html --extra-report md \\\n  --report-dir reports \\\n  --html-open-metadata true \\\n  --html-preview-limit 10\n</code></pre></p>"},{"location":"api_reference/#supported-formats-extra-reports","title":"Supported formats (extra reports)","text":"<ul> <li>json</li> <li>md (markdown)</li> <li>html</li> </ul> <p>Reference implementation: <code>ReportFactory</code> in <code>bench/reports/factory.py</code>.</p> <p>Environment variables affecting HTML rendering:</p> <ul> <li><code>MEDAISURE_HTML_OPEN_METADATA</code> = <code>\"1\"</code> to open Metadata sections by default</li> <li><code>MEDAISURE_HTML_PREVIEW_LIMIT</code> = integer truncation limit for list previews (default 5)</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>This document provides a high-level overview of MedAISure's architecture.</p> <ul> <li>Core components: Task Loader, Model Runner, Metric Calculator, Result Aggregator, Evaluation Harness</li> <li>Data flow: tasks -&gt; model execution -&gt; metrics -&gt; aggregation -&gt; reports</li> </ul> <p>Refer to API Reference for detailed class and method documentation.</p>"},{"location":"configuration/","title":"Configuration","text":"<ul> <li>MEDAISURE_NO_RICH</li> <li>Purpose: Disable Rich live console rendering during tests/CI, and force plain stdout output for both text and JSON from the Typer CLI in <code>bench/cli_typer.py</code>.</li> <li>Values: <code>\"1\"</code> (enabled) or unset/other (disabled).</li> <li>Behavior when set to <code>1</code>:<ul> <li>Rich status/progress bars are not created (<code>_status()</code> becomes no-op).</li> <li><code>_print()</code> writes plain text to <code>sys.stdout</code> (captured by pytest/CliRunner).</li> <li><code>_print_json()</code> always writes raw JSON to <code>sys.stdout</code> (no ANSI styling), ensuring robust parsing in tests.</li> <li>Python logging is disabled via <code>logging.disable(logging.CRITICAL)</code> to avoid noisy writes to captured streams.</li> </ul> </li> <li>Recommended usage:<ul> <li>Local runs of the CLI can omit it for nicer output.</li> <li>Enable for pytest and CI runs to avoid flaky captures: <code>export MEDAISURE_NO_RICH=1</code>.</li> </ul> </li> <li>CI: The default GitHub Actions workflow sets this for the test job (see <code>.github/workflows/tests.yml</code>).</li> </ul>"},{"location":"configuration/#configuration-reference","title":"Configuration Reference","text":"<p>Key parameters and where they apply.</p>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>MEDAISURE_NO_RICH</code>: Disable Rich live console rendering during tests/CI, and force plain stdout output for both text and JSON from the Typer CLI in <code>bench/cli_typer.py</code>.</li> <li><code>MEDAISURE_HTML_OPEN_METADATA</code>: When set to <code>\"1\"</code>, HTML reports open the Metadata section by default; otherwise collapsed.</li> <li><code>MEDAISURE_HTML_PREVIEW_LIMIT</code>: Integer. Max number of list items to show in HTML previews for <code>inputs</code>/<code>model_outputs</code> before truncation. Default <code>5</code>.</li> </ul>"},{"location":"configuration/#evaluationharness","title":"EvaluationHarness","text":"<ul> <li><code>tasks_dir</code>: where task YAML/JSON files are discovered</li> <li><code>results_dir</code>: where <code>&lt;run_id&gt;.json</code> reports and exports are written</li> <li><code>cache_dir</code>: if set, caches per-task predictions for reuse</li> <li><code>log_level</code>: python logging level string</li> <li>callbacks: <code>on_task_start</code>, <code>on_task_end</code>, <code>on_progress</code>, <code>on_error</code>, <code>on_metrics</code></li> </ul>"},{"location":"configuration/#modelrunner-hugging-face","title":"ModelRunner (Hugging Face)","text":"<ul> <li><code>hf_task</code>: <code>text-classification</code> (default), <code>summarization</code> (alias of <code>text2text-generation</code>), <code>text-generation</code></li> <li><code>model_kwargs</code>: passed to <code>AutoModel.from_pretrained</code></li> <li><code>tokenizer_kwargs</code>: passed to <code>AutoTokenizer.from_pretrained</code></li> <li><code>pipeline_kwargs</code>: passed to <code>transformers.pipeline</code></li> <li><code>device</code>: -1 (CPU) or CUDA index</li> <li><code>num_labels</code>: classification heads</li> </ul>"},{"location":"configuration/#modelrunner-local","title":"ModelRunner (Local)","text":"<ul> <li><code>model_path</code>: arbitrary path to your weights/artifacts</li> <li><code>module_path</code>: import path to your loader module</li> <li><code>load_func</code>: optional function name (default <code>load_model</code>)</li> </ul>"},{"location":"configuration/#modelrunner-api","title":"ModelRunner (API)","text":"<ul> <li><code>endpoint</code>: POST URL</li> <li><code>api_key</code>: bearer token used by <code>_call_api_model</code></li> <li>optional: <code>headers</code>, <code>timeout</code>, <code>max_retries</code>, <code>backoff_factor</code></li> </ul>"},{"location":"configuration/#metriccalculator","title":"MetricCalculator","text":"<ul> <li><code>register_metric(name, fn, **defaults)</code></li> <li><code>calculate_metrics(task_id, predictions, references, metric_names=None, **metric_kwargs)</code></li> <li><code>metric_kwargs</code> can be a mapping keyed by metric name for per-metric overrides</li> </ul>"},{"location":"configuration/#resultaggregator","title":"ResultAggregator","text":"<ul> <li><code>export_report_json|csv|markdown|html(run_id, output_path)</code></li> <li><code>aggregate_statistics(run_id, metrics=None, percentiles=None, tasks=None)</code></li> <li><code>filter_and_sort_tasks(run_id, tasks=None, metrics=None, sort_by=None, descending=True)</code></li> <li><code>compare_runs(run_a, run_b, metrics=None, relative=False)</code></li> </ul>"},{"location":"configuration/#benchmarkconfig-cli-config-file","title":"BenchmarkConfig (CLI config file)","text":"<p>You can provide defaults for CLI runs via a JSON/YAML config file and pass it with <code>--config-file</code> to the Typer CLI <code>evaluate</code> command.</p> <p>Fields (subset):</p> <ul> <li><code>model_id</code>: string</li> <li><code>tasks</code>: list of task IDs</li> <li><code>output_dir</code>: path for results</li> <li><code>output_format</code>: <code>json|yaml|md|csv</code></li> <li><code>model_type</code>: <code>huggingface|local|api</code></li> <li><code>batch_size</code>: int</li> <li><code>use_cache</code>: bool</li> <li><code>save_results</code>: bool</li> <li><code>extra_reports</code>: optional list of extra export formats, e.g. <code>[\"html\", \"md\"]</code></li> <li><code>report_dir</code>: optional path where extra reports are written (defaults to <code>output_dir</code>)</li> <li><code>html_open_metadata</code>: optional bool; if true, open Metadata sections by default in HTML reports</li> <li><code>html_preview_limit</code>: optional int; controls truncation length for inputs/outputs previews in HTML</li> </ul> <p>Example YAML:</p> <pre><code>model_id: textattack/bert-base-uncased-MNLI\ntasks: [clinical_icd10_classification]\noutput_dir: results\noutput_format: json\nmodel_type: huggingface\nbatch_size: 4\nuse_cache: true\nsave_results: true\nextra_reports: [html, md]\nreport_dir: reports\nhtml_open_metadata: true\nhtml_preview_limit: 10\n</code></pre>"},{"location":"configuration/#supported-formats-extra-reports","title":"Supported formats (extra reports)","text":"<ul> <li>json</li> <li>md (markdown)</li> <li>html</li> </ul> <p>Reference: extra reports are generated via <code>ReportFactory</code> in <code>bench/reports/factory.py</code>.</p> <p>CLI usage with config file:</p> <pre><code>MEDAISURE_NO_RICH=1 python -m bench.cli_typer evaluate textattack/bert-base-uncased-MNLI \\\n  --tasks clinical_icd10_classification \\\n  --config-file config.yaml\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>See the repository's CONTRIBUTING guide: https://github.com/junaidi-ai/MedAISure/blob/master/CONTRIBUTING.md</p> <p>Quick checklist: - Open an issue first for major changes - Follow PEP 8, add type hints and docstrings - Write/Update tests; ensure <code>pytest</code> passes - Run linters; ensure CI is green</p>"},{"location":"data_models/","title":"MedAISure Benchmark Data Models","text":"<p>This document provides detailed documentation for the core data models used in the MedAISure Benchmark system.</p>"},{"location":"data_models/#table-of-contents","title":"Table of Contents","text":"<ul> <li>MedicalTask</li> <li>EvaluationResult</li> <li>BenchmarkReport</li> <li>Usage Examples</li> <li>Serialization</li> </ul>"},{"location":"data_models/#medicaltask","title":"MedicalTask","text":"<p>Represents a medical task that a model needs to perform.</p>"},{"location":"data_models/#fields","title":"Fields","text":"<ul> <li><code>task_id</code> (str): Unique identifier for the task</li> <li><code>task_type</code> (TaskType): Type of the task (e.g., \"diagnostic_reasoning\", \"qa\")</li> <li><code>description</code> (str): Human-readable description of the task</li> <li><code>inputs</code> (List[Dict]): List of input examples for the task</li> <li><code>expected_outputs</code> (List[Dict]): List of expected outputs corresponding to inputs</li> <li><code>metrics</code> (List[str]): List of metric names to evaluate the task</li> </ul>"},{"location":"data_models/#example","title":"Example","text":"<pre><code>from bench.models import MedicalTask, TaskType\n\ntask = MedicalTask(\n    task_id=\"task_123\",\n    task_type=TaskType.QA,\n    description=\"Answer medical questions\",\n    inputs=[{\"question\": \"What are the symptoms of COVID-19?\"}],\n    expected_outputs=[{\"answer\": \"Common symptoms include fever, cough, and fatigue.\"}],\n    metrics=[\"accuracy\", \"f1_score\"]\n)\n</code></pre>"},{"location":"data_models/#evaluationresult","title":"EvaluationResult","text":"<p>Represents the result of evaluating a model on a specific task.</p>"},{"location":"data_models/#fields_1","title":"Fields","text":"<ul> <li><code>model_id</code> (str): Identifier for the evaluated model</li> <li><code>task_id</code> (str): Identifier of the task being evaluated</li> <li><code>inputs</code> (List[Dict]): Inputs used for evaluation</li> <li><code>model_outputs</code> (List[Dict]): Model's outputs for the given inputs</li> <li><code>metrics_results</code> (Dict[str, float]): Evaluation metrics and their values</li> <li><code>metadata</code> (Dict[str, Any]): Additional metadata about the evaluation</li> <li><code>timestamp</code> (datetime): When the evaluation was performed (auto-generated)</li> </ul>"},{"location":"data_models/#example_1","title":"Example","text":"<pre><code>from datetime import datetime, timezone\nfrom bench.models import EvaluationResult\n\nresult = EvaluationResult(\n    model_id=\"gpt-4\",\n    task_id=\"task_123\",\n    inputs=[{\"question\": \"What are the symptoms of COVID-19?\"}],\n    model_outputs=[{\"answer\": \"Symptoms include fever, cough, and fatigue.\"}],\n    metrics_results={\"accuracy\": 0.9, \"f1_score\": 0.85},\n    metadata={\"model_version\": \"1.0.0\"},\n    timestamp=datetime.now(timezone.utc)\n)\n</code></pre>"},{"location":"data_models/#benchmarkreport","title":"BenchmarkReport","text":"<p>Aggregates evaluation results across multiple tasks for a model.</p>"},{"location":"data_models/#fields_2","title":"Fields","text":"<ul> <li><code>model_id</code> (str): Identifier for the evaluated model</li> <li><code>timestamp</code> (datetime): When the benchmark was run (auto-generated)</li> <li><code>overall_scores</code> (Dict[str, float]): Aggregated scores across all tasks</li> <li><code>task_scores</code> (Dict[str, Dict[str, float]]): Scores for each task</li> <li><code>detailed_results</code> (List[EvaluationResult]): Individual evaluation results</li> <li><code>metadata</code> (Dict[str, Any]): Additional metadata about the benchmark</li> </ul>"},{"location":"data_models/#methods","title":"Methods","text":"<ul> <li><code>add_evaluation_result(result: EvaluationResult)</code>: Add a new evaluation result</li> <li><code>to_file(file_path: Union[str, Path])</code>: Save the report to a JSON file</li> <li><code>from_file(file_path: Union[str, Path]) -&gt; 'BenchmarkReport'</code>: Load a report from a JSON file</li> </ul>"},{"location":"data_models/#example_2","title":"Example","text":"<pre><code>from pathlib import Path\nfrom bench.models import BenchmarkReport\n\n# Create a new report\nreport = BenchmarkReport(\n    model_id=\"gpt-4\",\n    overall_scores={\"accuracy\": 0.85, \"f1_score\": 0.8},\n    task_scores={\n        \"task_1\": {\"accuracy\": 0.9, \"f1_score\": 0.85},\n        \"task_2\": {\"accuracy\": 0.8, \"f1_score\": 0.75}\n    },\n    metadata={\"run_id\": \"run_123\"}\n)\n\n# Add evaluation results\nreport.add_evaluation_result(result1)\nreport.add_evaluation_result(result2)\n\n# Save to file\nreport.to_file(\"benchmark_results.json\")\n\n# Load from file\nloaded_report = BenchmarkReport.from_file(\"benchmark_results.json\")\n</code></pre>"},{"location":"data_models/#usage-examples","title":"Usage Examples","text":""},{"location":"data_models/#creating-a-medical-task","title":"Creating a Medical Task","text":"<pre><code>from bench.models import MedicalTask, TaskType\n\ntask = MedicalTask(\n    task_id=\"diagnosis_001\",\n    task_type=TaskType.DIAGNOSTIC_REASONING,\n    description=\"Diagnose the most likely condition based on symptoms\",\n    inputs=[\n        {\n            \"symptoms\": [\"fever\", \"cough\", \"shortness of breath\"],\n            \"age\": 45,\n            \"gender\": \"M\"\n        }\n    ],\n    expected_outputs=[\n        {\n            \"diagnosis\": \"pneumonia\",\n            \"confidence\": 0.92,\n            \"differential_diagnosis\": [\"influenza\", \"bronchitis\"]\n        }\n    ],\n    metrics=[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n)\n</code></pre>"},{"location":"data_models/#running-an-evaluation","title":"Running an Evaluation","text":"<pre><code>from bench.models import EvaluationResult\n\n# After running the model on the task\nmodel_outputs = [\n    {\n        \"diagnosis\": \"pneumonia\",\n        \"confidence\": 0.89,\n        \"differential_diagnosis\": [\"influenza\", \"bronchitis\", \"asthma\"]\n    }\n]\n\n# Calculate metrics (simplified example)\nmetrics = {\n    \"accuracy\": 1.0,  # Correct diagnosis\n    \"precision\": 0.9,\n    \"recall\": 0.85,\n    \"f1_score\": 0.87\n}\n\n# Create evaluation result\neval_result = EvaluationResult(\n    model_id=\"our-model-1.0\",\n    task_id=task.task_id,\n    inputs=task.inputs,\n    model_outputs=model_outputs,\n    metrics_results=metrics,\n    metadata={\n        \"model_version\": \"1.0.0\",\n        \"evaluation_time_seconds\": 2.5\n    }\n)\n</code></pre>"},{"location":"data_models/#serialization","title":"Serialization","text":"<p>All models support JSON serialization and deserialization:</p> <pre><code># To JSON string\njson_str = task.model_dump_json(indent=2)\n\n# From JSON string\nloaded_task = MedicalTask.model_validate_json(json_str)\n\n# To dictionary\ndata = task.model_dump()\n\n# From dictionary\nloaded_task = MedicalTask.model_validate(data)\n</code></pre>"},{"location":"data_models/#validation","title":"Validation","text":"<p>Models include built-in validation:</p> <pre><code>from pydantic import ValidationError\n\ntry:\n    invalid_task = MedicalTask(\n        task_id=\"\",  # Empty string not allowed\n        task_type=\"invalid_type\",  # Not a valid TaskType\n        inputs=[],  # Cannot be empty\n        expected_outputs=[{}],\n        metrics=[\"\"]  # Empty metric name not allowed\n    )\nexcept ValidationError as e:\n    print(f\"Validation error: {e}\")\n</code></pre>"},{"location":"data_models/#best-practices","title":"Best Practices","text":"<ol> <li>Always use the provided model classes instead of raw dictionaries</li> <li>Take advantage of built-in validation</li> <li>Use type hints for better IDE support and code clarity</li> <li>Store and share evaluation results using the serialization methods</li> <li>Include relevant metadata for traceability</li> </ol>"},{"location":"data_models/#task-type-schema-requirements","title":"Task-Type Schema Requirements","text":"<p>The evaluation framework applies minimal default schemas per <code>TaskType</code> when tasks do not provide explicit <code>input_schema</code>/<code>output_schema</code>. These defaults are enforced during task load and evaluation. Required keys are:</p> <ul> <li>QA</li> <li>Inputs require: <code>question</code></li> <li> <p>Outputs require: <code>answer</code></p> </li> <li> <p>Summarization</p> </li> <li>Inputs require: <code>document</code></li> <li> <p>Outputs require: <code>summary</code></p> </li> <li> <p>Diagnostic Reasoning</p> </li> <li>Inputs require: <code>symptoms</code></li> <li> <p>Outputs require: <code>diagnosis</code></p> </li> <li> <p>Communication</p> </li> <li>Inputs require: <code>prompt</code></li> <li>Outputs require: <code>response</code></li> </ul> <p>Notes: - These defaults are defined in <code>bench/evaluation/validators.py</code> under <code>DEFAULT_SCHEMAS</code> and are used by <code>ensure_task_schemas()</code>. - Inline datasets may use either flat rows or nested form <code>{ \"input\": { ... }, \"output\": { ... } }</code>. See <code>validate_task_dataset()</code> for details. - Strict validation mode in the harness will raise on schema violations; non-strict mode attaches validation errors to result metadata.</p>"},{"location":"data_models/#task-type-examples","title":"Task-Type Examples","text":"<p>Below are concise, runnable examples for each built-in task type.</p>"},{"location":"data_models/#qa","title":"QA","text":"<pre><code>from bench.models.task_types import MedicalQATask\n\ntask = MedicalQATask(\"qa-demo\")\ntask.dataset = [\n    {\"input\": {\"question\": \"What is BP?\"}, \"output\": {\"answer\": \"blood pressure\"}}\n]\nmetrics = task.evaluate([{ \"answer\": \"blood pressure\" }])  # {\"accuracy\": 1.0, \"clinical_correctness\": 1.0}\n</code></pre>"},{"location":"data_models/#diagnostic-reasoning","title":"Diagnostic Reasoning","text":"<pre><code>from bench.models.task_types import DiagnosticReasoningTask\n\ntask = DiagnosticReasoningTask(\"dx-demo\")\ntask.dataset = [\n    {\"input\": {\"case\": \"60M chest pain\"}, \"output\": {\"diagnosis\": \"ACS\"}}\n]\nmetrics = task.evaluate([{ \"diagnosis\": \"ACS\", \"explanation\": \"because ECG shows ST elevation\" }])\n</code></pre>"},{"location":"data_models/#summarization","title":"Summarization","text":"<pre><code>from bench.models.task_types import ClinicalSummarizationTask\n\ntask = ClinicalSummarizationTask(\"sum-demo\")\ntask.dataset = [\n    {\"input\": {\"document\": \"Patient with HTN and DM.\"}, \"output\": {\"summary\": \"HTN, DM.\"}}\n]\nmetrics = task.evaluate([{ \"summary\": \"HTN, DM.\" }])\n</code></pre>"},{"location":"data_models/#creating-custom-task-instances","title":"Creating Custom Task Instances","text":"<p>You can define your own tasks using <code>MedicalTask</code> (YAML/JSON) and validate/load them via the loader/validators.</p>"},{"location":"data_models/#minimal-yaml-example","title":"Minimal YAML Example","text":"<pre><code>schema_version: 1\ntask_id: qa-custom\ntask_type: qa\ndescription: Answer short questions\ninputs:\n  - { question: \"What is HR?\" }\nexpected_outputs:\n  - { answer: \"heart rate\" }\nmetrics: [accuracy]\ninput_schema:\n  required: [question]\noutput_schema:\n  required: [answer]\ndataset:\n  - input:  { question: \"What is BP?\" }\n    output: { answer: \"blood pressure\" }\n</code></pre>"},{"location":"data_models/#load-and-validate","title":"Load and Validate","text":"<pre><code>from bench.models.medical_task import MedicalTask\nfrom bench.evaluation.validators import validate_task_dataset\n\ntask = MedicalTask.from_file(\"qa-custom.yaml\")\nvalidate_task_dataset(task)  # raises if required keys missing\n</code></pre> <p>See also: - <code>bench/evaluation/task_loader.py</code> for loading tasks from files/URLs - <code>bench/evaluation/model_runner.py</code> for running models on tasks</p>"},{"location":"data_models/#task-specific-metrics","title":"Task-Specific Metrics","text":"<p>This framework ships with lightweight, task-specific metrics implemented in the task classes under <code>bench/models/task_types.py</code>. These are intentionally simple placeholders and should be replaced or extended for production use.</p> <ul> <li>QA (<code>MedicalQATask</code>)</li> <li><code>accuracy</code>: case-insensitive exact match of <code>answer</code></li> <li> <p><code>clinical_correctness</code>: proxy equal to <code>accuracy</code></p> </li> <li> <p>Diagnostic Reasoning (<code>DiagnosticReasoningTask</code>)</p> </li> <li><code>diagnostic_accuracy</code>: case-insensitive exact match of <code>diagnosis</code></li> <li> <p><code>reasoning_quality</code>: heuristic based on presence of explanation cues (e.g., \"because\", \"due to\") in <code>explanation</code>/<code>rationale</code></p> </li> <li> <p>Summarization (<code>ClinicalSummarizationTask</code>)</p> </li> <li><code>rouge_l</code>: unigram-overlap proxy (not true ROUGE-L)</li> <li><code>clinical_relevance</code>: overlap ratio with a small set of medical keywords</li> <li><code>factual_consistency</code>: penalizes hallucinated numbers not present in the reference</li> </ul> <p>For full implementations, consider integrating standard NLP metrics (e.g., official ROUGE, BERTScore) and clinical factuality checks.</p>"},{"location":"data_models/#end-to-end-usage","title":"End-to-End Usage","text":"<p>To see how tasks are loaded and models are executed over them, refer to:</p> <ul> <li><code>bench/evaluation/task_loader.py</code></li> <li>Key methods: <code>TaskLoader.load_task()</code>, <code>TaskLoader.load_tasks()</code>, <code>TaskLoader.list_available_tasks()</code></li> <li> <p>Supports loading by ID, local path, or HTTP(S) URL with validation</p> </li> <li> <p><code>bench/evaluation/model_runner.py</code></p> </li> <li>Key methods: <code>ModelRunner.load_model()</code>, <code>ModelRunner.run_model()</code> / <code>run_model_async()</code>, <code>ModelRunner.unload_model()</code></li> <li>Supports HuggingFace pipelines, local Python module models, and API-based models</li> </ul> <p>You can combine these to evaluate a model end-to-end:</p> <pre><code>from bench.evaluation.task_loader import TaskLoader\nfrom bench.evaluation.model_runner import ModelRunner\n\nloader = TaskLoader()\ntask = loader.load_task(\"bench/tasks/clinical_summarization_basic.yaml\")\n\nrunner = ModelRunner()\nrunner.load_model({\"type\": \"local\", \"path\": \"tests/fixtures/simple_local_model.py\", \"callable\": \"predict\"})\n\noutputs = runner.run_model(task.inputs)\nmetrics = task.evaluate(outputs)\n</code></pre>"},{"location":"data_models/#task-registry","title":"Task Registry","text":"<p>For dynamic registration, discovery, and filtered listing of tasks, use <code>bench/evaluation/task_registry.py</code>.</p> <ul> <li><code>TaskRegistry.register()</code> / <code>register_from_file()</code> / <code>register_from_url()</code></li> <li><code>TaskRegistry.get()</code> to retrieve by ID</li> <li><code>TaskRegistry.discover()</code> to scan a tasks directory</li> <li><code>TaskRegistry.list_available(task_type=..., min_examples=..., has_metrics=...)</code> for simple filtering</li> </ul> <p>Example:</p> <pre><code>from bench.evaluation.task_registry import TaskRegistry\nfrom bench.models.medical_task import TaskType\n\nreg = TaskRegistry(tasks_dir=\"bench/tasks\")\nreg.discover()\n\n# Filter to QA tasks that declare metrics and have &gt;= 1 example\nrows = reg.list_available(task_type=TaskType.QA, min_examples=1, has_metrics=True)\n\n# Load a specific task (from discovery or previously registered)\ntask = reg.get(rows[0].task_id)\n</code></pre>"},{"location":"docker/","title":"Docker Usage for MedAISure","text":"<p>This guide shows how to build and run MedAISure in Docker for both CPU and GPU environments.</p>"},{"location":"docker/#images","title":"Images","text":"<ul> <li>CPU: <code>Dockerfile</code> (Python 3.10 slim)</li> <li>GPU: <code>Dockerfile.gpu</code> (NVIDIA CUDA 11.8 runtime + cuDNN)</li> </ul> <p>Both images install Python deps from <code>requirements.txt</code> and use the CLI entrypoint <code>medaisure-benchmark</code> defined in <code>setup.py</code>.</p>"},{"location":"docker/#build","title":"Build","text":"<pre><code># CPU image\ndocker build -t medaisure/cpu:latest -f Dockerfile .\n\n# GPU image (requires NVIDIA Docker runtime)\ndocker build -t medaisure/gpu:latest -f Dockerfile.gpu .\n</code></pre>"},{"location":"docker/#cuda-enabled-pytorch-wheels","title":"CUDA-enabled PyTorch wheels","text":"<p>The GPU image pins CUDA 11.8 PyTorch wheels during build:</p> <pre><code>python3 -m pip install --index-url https://download.pytorch.org/whl/cu118 \\\n  torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n</code></pre> <p>If you adjust CUDA or PyTorch versions, ensure you use the matching index URL and compatible versions.</p>"},{"location":"docker/#run-docker","title":"Run (Docker)","text":"<pre><code># List tasks (CPU)\ndocker run --rm -v \"$PWD/data:/app/data\" -v \"$PWD/results:/app/results\" medaisure/cpu:latest list --json\n\n# List tasks (GPU)\ndocker run --rm --gpus all -v \"$PWD/data:/app/data\" -v \"$PWD/results:/app/results\" medaisure/gpu:latest list --json\n\n# Show a task (replace &lt;id&gt;)\ndocker run --rm medaisure/cpu:latest show &lt;id&gt;\n</code></pre>"},{"location":"docker/#gpu-smoke-test","title":"GPU Smoke Test","text":"<p>Run a quick CUDA sanity test inside the GPU image:</p> <pre><code>docker run --rm --gpus all medaisure/gpu:latest python3 scripts/gpu_smoke.py\n</code></pre>"},{"location":"docker/#run-docker-compose","title":"Run (Docker Compose)","text":"<pre><code># Build and run CPU service\ndocker compose up --build medaisure-cpu\n\n# Build and run GPU service (requires NVIDIA runtime)\ndocker compose up --build medaisure-gpu\n</code></pre> <p>Volumes: - <code>./data</code> is mounted to <code>/app/data</code> - <code>./results</code> is mounted to <code>/app/results</code></p> <p>To run only the GPU smoke service using profiles:</p> <pre><code>docker compose --profile smoke up --build medaisure-gpu-smoke\n</code></pre>"},{"location":"docker/#environment-variables","title":"Environment Variables","text":"<p>Create a local <code>.env</code> for API keys or configs and mount it if needed:</p> <pre><code>docker run --rm --env-file .env medaisure/cpu:latest list --json\n</code></pre> <p>Alternatively, pass specific vars:</p> <pre><code>docker run --rm -e HF_TOKEN=... medaisure/cpu:latest list --json\n</code></pre>"},{"location":"docker/#notes","title":"Notes","text":"<ul> <li>The entrypoint uses the console script <code>medaisure-benchmark</code> (from <code>bench/cli.py</code>). If you prefer a Python module entrypoint, you can run:   <pre><code>docker run --rm medaisure/cpu:latest python -m bench.cli list --json\n</code></pre></li> <li>GPU runs require a host with NVIDIA drivers and the NVIDIA Container Toolkit installed. Test with <code>docker run --rm --gpus all nvidia/cuda:11.8.0-base nvidia-smi</code>.</li> </ul>"},{"location":"extensions/","title":"Extension Guide","text":"<p>How to extend MedAISure with custom tasks, metrics, and models.</p>"},{"location":"extensions/#add-a-custom-task","title":"Add a Custom Task","text":"<p>1) Create <code>your_task.yaml</code> in your <code>tasks_dir</code> with fields used by <code>MedicalTask</code>: <pre><code>name: \"My Clinical NLI\"\ndescription: \"NLI over clinical text\"\ntask_type: qa\nmetrics: [accuracy, clinical_correctness]\ndataset:\n  - text: \"Patient denies chest pain.\"\n    hypothesis: \"The patient has chest pain.\"\n    label: contradiction\n</code></pre> 2) Load by id (file stem) or path: <pre><code>from bench.evaluation import EvaluationHarness\nh = EvaluationHarness(tasks_dir=\"bench/tasks\")\nreport = h.evaluate(model_id=\"textattack/bert-base-uncased-MNLI\", task_ids=[\"your_task\"], model_type=\"huggingface\")\n</code></pre></p>"},{"location":"extensions/#register-a-custom-metric","title":"Register a Custom Metric","text":"<p><pre><code>from bench.evaluation.metric_calculator import MetricCalculator\n\ndef my_metric(y_true, y_pred, **kwargs):\n    # return a float, or (float, metadata_dict)\n    return float(sum(int(t == p) for t, p in zip(y_true, y_pred)) / len(y_true))\n\nmc = MetricCalculator()\nmc.register_metric(\"my_metric\", my_metric)\n</code></pre> Use it by including <code>my_metric</code> in your task's <code>metrics:</code> or by passing <code>metric_names=[...]</code> to <code>calculate_metrics()</code> manually.</p>"},{"location":"extensions/#use-a-local-model","title":"Use a Local Model","text":"<p>Your local module should expose a loader (default name: <code>load_model</code>): <pre><code># mypkg/mylocal.py\nclass MyModel:\n    def __call__(self, batch, **kwargs):\n        # return list[dict] with keys like \"label\" or \"prediction\"\n        return [{\"label\": \"entailment\", \"score\": 0.9} for _ in batch]\n\ndef load_model(model_path, **kwargs):\n    return MyModel()\n</code></pre> Load via <code>ModelRunner</code> (automatically used by <code>EvaluationHarness.evaluate()</code>): <pre><code>report = h.evaluate(\n    model_id=\"my_local_model\",\n    task_ids=[\"your_task\"],\n    model_type=\"local\",\n    model_path=\"/path/to/artifacts\",\n    module_path=\"mypkg.mylocal\",\n)\n</code></pre></p>"},{"location":"extensions/#use-an-api-model","title":"Use an API Model","text":"<p>Provide <code>endpoint</code> and <code>api_key</code>. Predictions should be a list of dicts with <code>label</code>/<code>score</code> or <code>prediction/text/summary</code>. <pre><code>report = h.evaluate(\n    model_id=\"my_api\",\n    task_ids=[\"your_task\"],\n    model_type=\"api\",\n    endpoint=\"https://api.example.com/predict\",\n    api_key=os.environ[\"MY_API_KEY\"],\n)\n</code></pre></p> <p>See <code>bench/examples/</code> for runnable samples.</p>"},{"location":"getting_started/","title":"Getting Started with MedAISure","text":"<p>This project ships a Quick Start guide. For a concise intro, see Quick Start.</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<pre><code># From PyPI (recommended when published)\npip install medaisure-benchmark\n\n# Or from source\ngit clone https://github.com/junaidi-ai/MedAISure.git\ncd MedAISure\npip install -e .\n</code></pre>"},{"location":"getting_started/#cli-usage","title":"CLI Usage","text":"<pre><code># List available tasks\nmedaisure list-tasks\n\n# Evaluate a model on a task\nmedaisure evaluate my-model --tasks medical_qa_basic\n\n# Generate a report\nmedaisure generate-report ./results/my-model_results.json --format md\n</code></pre>"},{"location":"getting_started/#python-api","title":"Python API","text":"<pre><code>from bench.evaluation.harness import EvaluationHarness\nfrom bench.models import registry as model_registry\n\nh = EvaluationHarness()\nmodel = model_registry.load(\"gpt2\")\nreport = h.evaluate(model, tasks=[\"medical_qa_basic\"])\nprint(report.overall_scores)\n</code></pre>"},{"location":"metrics_guidelines/","title":"Metrics Guidelines","text":"<ul> <li>Centralized synonyms: Use the shared diagnosis label map <code>LABEL_SYNONYMS</code> for clinical abbreviations and common variants.</li> <li>Location: <code>bench/evaluation/metrics/clinical.py</code> (exported via <code>bench.evaluation.metrics</code>)</li> <li>Import example:     <pre><code>from bench.evaluation.metrics import LABEL_SYNONYMS\n# normalized = LABEL_SYNONYMS.get(_normalize_text(label), _normalize_text(label))\n</code></pre></li> <li>Why: Ensures consistent normalization across metrics (e.g., <code>mi \u2192 myocardial infarction</code>, <code>copd \u2192 chronic obstructive pulmonary disease</code>, <code>htn \u2192 hypertension</code>).</li> <li>Where to use:</li> <li>Anywhere you compare predicted/expected diagnoses or labels.</li> <li>Prefer mapping after lowercasing/basic punctuation cleanup (use <code>_normalize_text()</code> if available).</li> <li>Extending the map:</li> <li>Add new entries in <code>bench/evaluation/metrics/clinical.py</code> under the <code>LABEL_SYNONYMS</code> dict.</li> <li>Keep entries lowercased, punctuation-free keys and values.</li> <li>Group additions by domain (cardiology, pulmonary, etc.) with comments.</li> <li>Testing requirements:</li> <li>Add known-value tests in <code>tests/test_metrics_known_values.py</code> for each new synonym, asserting a 1.0 match for the normalized pair.</li> <li>Example:     <pre><code>def test_diagnostic_accuracy_htn_synonym_match():\n    m = DiagnosticAccuracyMetric()\n    expected = [{\"diagnosis\": \"hypertension\"}]\n    outputs = [{\"prediction\": \"htn\"}]\n    assert m.calculate(expected, outputs) == 1.0\n</code></pre></li> <li>Coordination with other metrics:</li> <li>If a metric relies on diagnosis-specific rules, consider normalizing inputs via <code>LABEL_SYNONYMS</code> before rule checks to reduce duplication.</li> <li>Avoid changing existing lexicon logic unless tests are added to cover behavior changes.</li> <li>Contribution tip:</li> <li>When proposing new synonyms, include short clinical justification or references if ambiguity is possible.</li> </ul>"},{"location":"metrics_testing/","title":"Metrics Testing Coverage and How to Run","text":"<p>This document summarizes the unit/property/performance tests added for metric implementations in <code>bench/evaluation/metrics/</code> and how to run them.</p>"},{"location":"metrics_testing/#implemented-tests","title":"Implemented Tests","text":"<ul> <li>Malformed/Unexpected Inputs</li> <li>File: <code>tests/test_metrics_malformed_inputs.py</code></li> <li>Covers resilience to missing keys, <code>None</code>, non-string values, and extras for:<ul> <li><code>ClinicalAccuracyMetric</code></li> <li><code>ReasoningQualityMetric</code></li> <li><code>DiagnosticAccuracyMetric</code></li> <li><code>ClinicalRelevanceMetric</code></li> </ul> </li> <li> <p>Also asserts list shape/type validation via <code>Metric.validate_inputs()</code>.</p> </li> <li> <p>Property-Based Tests (Hypothesis)</p> </li> <li>File: <code>tests/test_metrics_property_based.py</code></li> <li> <p>Generates randomized inputs of equal lengths; checks:</p> <ul> <li>Scores are floats in <code>[0, 1]</code></li> <li>Determinism for identical inputs (ReasoningQuality)</li> </ul> </li> <li> <p>Known-Values / Deterministic Scenarios</p> </li> <li>File: <code>tests/test_metrics_known_values.py</code></li> <li> <p>Asserts exact scores for simple deterministic cases (e.g., identical texts -&gt; 1.0, no overlap -&gt; 0.0).</p> </li> <li> <p>Performance Benchmarks (pytest-benchmark)</p> </li> <li>File: <code>tests/test_metrics_performance_benchmark.py</code></li> <li>Benchmarks runtime on ~1000-item batches for each metric and asserts valid score ranges.</li> </ul>"},{"location":"metrics_testing/#how-to-run","title":"How to Run","text":"<ul> <li> <p>Run only new tests quickly: <pre><code>pytest -q tests/test_metrics_malformed_inputs.py \\\n          tests/test_metrics_property_based.py \\\n          tests/test_metrics_known_values.py \\\n          tests/test_metrics_performance_benchmark.py\n</code></pre></p> </li> <li> <p>Run benchmarks with more iterations: <pre><code>pytest tests/test_metrics_performance_benchmark.py --benchmark-min-time=0.1\n</code></pre></p> </li> </ul>"},{"location":"metrics_testing/#notes","title":"Notes","text":"<ul> <li><code>DiagnosticAccuracyMetric</code> now normalizes the label <code>\"mi\"</code> to <code>\"myocardial infarction\"</code> to align synonyms before comparison.</li> <li>All tests keep metrics dependency-light and validate score normalization <code>[0, 1]</code>.</li> </ul>"},{"location":"performance/","title":"Performance Tips","text":"<ul> <li>Prefer batching via <code>batch_size</code> in <code>EvaluationHarness.evaluate()</code>; default is 8</li> <li>Use GPU for HF models by setting <code>device</code> (e.g., <code>device=0</code>) in <code>model_kwargs</code>/<code>pipeline_kwargs</code></li> <li>Cache predictions by providing <code>cache_dir</code>; reuse with <code>use_cache=True</code></li> <li>Minimize heavy metrics; some (e.g., ROUGE) are slower and optional</li> <li>Filter tasks to a subset during development</li> <li>For large runs, export CSV/Markdown and analyze externally</li> </ul>"},{"location":"quick_start/","title":"Quick Start","text":"<p>This guide helps you get up and running with MedAISure quickly using either local Python or Docker (CPU/GPU).</p>"},{"location":"quick_start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>Docker (optional but recommended for reproducibility)</li> <li>NVIDIA drivers + NVIDIA Container Toolkit (optional, for GPU)</li> </ul>"},{"location":"quick_start/#1-local-installation","title":"1) Local Installation","text":"<pre><code># Clone\ngit clone https://github.com/junaidi-ai/MedAISure.git\ncd MedAISure\n\n# Create and activate venv\npython -m venv .venv\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\n\n# Install package + dev extras\npip install -e .[dev]\n\n# (Optional) set env vars\ncp .env.example .env\n# edit .env as needed (API keys, etc.)\n</code></pre> <p>List available tasks via CLI:</p> <pre><code>medaisure-benchmark list --json\n</code></pre>"},{"location":"quick_start/#2-docker-cpu","title":"2) Docker (CPU)","text":"<pre><code># Build CPU image\ndocker build -t medaisure/cpu:latest -f Dockerfile .\n\n# List tasks\ndocker run --rm -v \"$PWD/data:/app/data\" -v \"$PWD/results:/app/results\" \\\n  medaisure/cpu:latest list --json\n</code></pre>"},{"location":"quick_start/#3-docker-gpu","title":"3) Docker (GPU)","text":"<pre><code># Build GPU image (requires NVIDIA Container Toolkit)\ndocker build -t medaisure/gpu:latest -f Dockerfile.gpu .\n\n# Quick GPU smoke test (sanity check)\ndocker run --rm --gpus all medaisure/gpu:latest \\\n  python3 scripts/gpu_smoke.py\n\n# List tasks with GPU image\ndocker run --rm --gpus all -v \"$PWD/data:/app/data\" -v \"$PWD/results:/app/results\" \\\n  medaisure/gpu:latest list --json\n</code></pre>"},{"location":"quick_start/#4-docker-compose","title":"4) Docker Compose","text":"<pre><code># CPU service\ndocker compose up --build medaisure-cpu\n\n# GPU service (requires NVIDIA runtime)\ndocker compose up --build medaisure-gpu\n\n# GPU smoke-only (profile)\ndocker compose --profile smoke up --build medaisure-gpu-smoke\n</code></pre>"},{"location":"quick_start/#cpu-vs-gpu-differences","title":"CPU vs GPU Differences","text":"<ul> <li>GPU image installs CUDA 11.8-enabled PyTorch wheels. Use <code>--gpus all</code> when running.</li> <li>CPU image uses generic wheels; it will run on systems without NVIDIA GPUs.</li> <li>Performance: GPU path accelerates tensor ops; see <code>scripts/gpu_smoke.py</code> for a quick timing check.</li> </ul>"},{"location":"quick_start/#volumes-data-best-practices","title":"Volumes &amp; Data Best Practices","text":"<ul> <li>Mount <code>./data</code> to <code>/app/data</code> for inputs and <code>./results</code> to <code>/app/results</code> for outputs.</li> <li>Keep large datasets outside of the image; prefer volume mounts.</li> <li>For reproducibility, keep a copy of configs under version control.</li> </ul>"},{"location":"quick_start/#environment-variables","title":"Environment Variables","text":"<ul> <li>Copy <code>.env.example</code> to <code>.env</code> and set any required keys.</li> <li>With Docker, pass <code>--env-file .env</code> or <code>-e KEY=value</code> for specific variables.</li> </ul>"},{"location":"quick_start/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>See docs/troubleshooting.md</li> <li>Verify Docker GPU support: <code>docker run --rm --gpus all nvidia/cuda:11.8.0-base nvidia-smi</code></li> <li>If GPU is not available, use the CPU image/commands.</li> </ul>"},{"location":"security/","title":"Secure Data Handling","text":"<p>This document explains how to use <code>SecureDataHandler</code> (in <code>bench/data/security.py</code>) to protect sensitive data, including algorithm selection, PHI anonymization, field-level policies, audit logging, and key rotation.</p>"},{"location":"security/#quick-start","title":"Quick Start","text":"<pre><code>from bench.data.security import SecureDataHandler\n\nhandler = SecureDataHandler(\"secret-key\")  # Fernet by default\ncipher = handler.encrypt_data({\"name\": \"Alice\", \"age\": 30})\nplain = handler.decrypt_data(cipher)\n</code></pre> <ul> <li>Only string values are encrypted/decrypted; non-strings pass through.</li> <li>If no key is provided, encrypt/decrypt are no-ops.</li> </ul>"},{"location":"security/#algorithm-selection","title":"Algorithm Selection","text":"<p>Supported algorithms: - <code>fernet</code> (default) \u2014 simple, URL-safe tokens, includes integrity. - <code>aes-gcm</code> \u2014 AEAD with GCM; encodes <code>nonce+ciphertext</code> in URL-safe base64. - <code>chacha20</code> \u2014 AEAD ChaCha20-Poly1305; encodes <code>nonce+ciphertext</code> in URL-safe base64.</p> <pre><code>SecureDataHandler(\"pass\", algorithm=\"aes-gcm\")\nSecureDataHandler(\"pass\", algorithm=\"chacha20\")\n</code></pre>"},{"location":"security/#field-level-policies","title":"Field-Level Policies","text":"<p>Control which fields are processed (encrypted/decrypted):</p> <pre><code># Only encrypt these fields\nhandler = SecureDataHandler(\"pass\", include_fields={\"name\", \"email\"})\n\n# Encrypt all string fields except these\nhandler = SecureDataHandler(\"pass\", exclude_fields={\"public_note\"})\n</code></pre> <p><code>include_fields</code> takes precedence over <code>exclude_fields</code>.</p>"},{"location":"security/#phi-detection-anonymization","title":"PHI Detection &amp; Anonymization","text":"<p>Enable anonymization to automatically redact likely PHI before encryption:</p> <pre><code>handler = SecureDataHandler(\"pass\", anonymize=True)\nredacted_then_encrypted = handler.encrypt_data({\n    \"msg\": \"Contact john.doe@example.com or 555-123-4567 on 2024-01-31\",\n})\n</code></pre> <ul> <li>Emails, phone numbers, SSN-like patterns, MRN-like IDs, and <code>YYYY-MM-DD</code> dates are detected.</li> <li>Redaction is deterministic, producing tokens like <code>[REDACTED:ABCDEFGH]</code>.</li> <li>You can also call <code>anonymize_data()</code> directly.</li> </ul>"},{"location":"security/#compliance-audit-logging","title":"Compliance Audit Logging","text":"<p>Record audits to JSONL for encrypt/decrypt/anonymize/rotate:</p> <pre><code>handler = SecureDataHandler(\"pass\", audit_log_path=\"audit.jsonl\")\nhandler.encrypt_data({\"x\": \"1\"})\nhandler.decrypt_data({\"x\": \"...\"})\nhandler.anonymize_data({\"x\": \"...\"})\nhandler.rotate_key(\"new-pass\")\n</code></pre> <p>Each line contains a JSON object with: - <code>ts</code> (ms timestamp) - <code>action</code> (encrypt|decrypt|anonymize|rotate_key) - <code>alg</code>, <code>kid</code> (current key id), and action <code>details</code></p> <p>If logging fails, operations continue (best-effort logging).</p>"},{"location":"security/#key-management-rotation","title":"Key Management &amp; Rotation","text":"<p>Create with an initial key (derived from passphrase). Rotate to a new key while retaining old keys for decryption:</p> <pre><code>handler = SecureDataHandler(\"pass\", algorithm=\"aes-gcm\")\nold_ct = handler.encrypt_data({\"s\": \"one\"})\nhandler.rotate_key(\"new-pass\")  # new key used for future encryptions\nplain_old = handler.decrypt_data(old_ct)  # still decrypts\n</code></pre> <ul> <li>Keys are referenced by <code>kid</code> (e.g., <code>k0</code>, <code>k1</code>, ...).</li> <li>Current key is preferred for decryption; fallbacks attempt older keys.</li> </ul>"},{"location":"security/#batch-helpers","title":"Batch Helpers","text":"<p>For large datasets, use batch helpers to avoid per-item overhead:</p> <pre><code>enc_list = handler.encrypt_batch(list_of_dicts)\nplain_list = handler.decrypt_batch(enc_list)\n</code></pre>"},{"location":"security/#operational-guidance","title":"Operational Guidance","text":"<ul> <li>Choose <code>fernet</code> for simplicity and compatibility; <code>aes-gcm</code> or <code>chacha20</code> for AEAD performance.</li> <li>Use <code>include_fields</code> to explicitly protect PII/PHI fields and avoid unexpected ciphertext expansion.</li> <li>Enable <code>anonymize=True</code> when ingesting free-text inputs that may contain PHI.</li> <li>Configure <code>audit_log_path</code> in environments requiring auditability. Persist logs to secure storage.</li> <li>Rotate keys periodically with <code>rotate_key()</code>. Keep audit logs of rotations and store passphrases in a secure secret manager.</li> <li>Test representative payloads. Some tokens (e.g., long base64) can expand storage size.</li> <li>Be cautious with double-encrypting fields. The handler attempts best-effort decryption and will pass through unknown/plaintext strings.</li> </ul>"},{"location":"security/#references","title":"References","text":"<ul> <li>Implementation: <code>bench/data/security.py</code></li> <li>Usage in data connectors: <code>bench/data/local.py</code></li> <li>Tests: <code>tests/test_security_handler.py</code></li> </ul>"},{"location":"testing/","title":"Testing Guide","text":"<p>This project includes a comprehensive testing suite for dataset connectors and related utilities.</p>"},{"location":"testing/#running-tests","title":"Running Tests","text":"<ul> <li>Local: <code>pytest -q</code></li> <li>With coverage (default via pytest.ini): <code>pytest -v</code></li> <li>Generate HTML coverage report: opens automatically at <code>htmlcov/index.html</code>.</li> </ul> <p>CI runs tests on each push/PR and uploads the HTML coverage report as an artifact named <code>coverage-html</code>.</p>"},{"location":"testing/#test-structure","title":"Test Structure","text":"<ul> <li><code>tests/test_data_connectors.py</code>: Unit tests for <code>JSONDataset</code> and <code>CSVDataset</code> load paths (plain, gzip, zip), validation, and batching.</li> <li><code>tests/test_connector_encrypted_local_datasets.py</code>: Ensures encrypted JSON/CSV files decrypt end-to-end when an <code>encryption_key</code> is provided.</li> <li><code>tests/test_medical_connectors.py</code>: Unit tests for <code>MIMICConnector</code> (SQLite) and <code>PubMedConnector</code> with mocked HTTP responses.</li> <li><code>tests/test_mimic_performance_and_redaction.py</code>: Integration/performance test for <code>MIMICConnector</code> including PHI redaction and cache acceleration check.</li> <li><code>tests/test_security_handler.py</code>: Security-focused tests for <code>SecureDataHandler</code> including algorithms, include/exclude fields, anonymization, audit logging, key rotation, and batch helpers.</li> <li><code>tests/test_encrypted_negative_cases.py</code>: Negative-path tests for encrypted inputs, including corrupted ciphertexts, wrong keys, mixed partially encrypted rows, and truncated base64 tokens.</li> <li><code>tests/test_large_batched_reads.py</code>: Large-scale batched read tests for JSON/CSV with plain, gzip, and zip variants; parameterized batch sizes and CI-safe performance bounds.</li> </ul> <p>Additional end-to-end and performance tests exist across the suite (models, metrics, harness).</p>"},{"location":"testing/#fixtures-and-factories","title":"Fixtures and Factories","text":"<p>Defined in <code>tests/conftest.py</code>:</p> <ul> <li><code>sample_json_file</code>, <code>sample_csv_file</code>: Create simple local datasets.</li> <li><code>encrypted_json_file</code>, <code>encrypted_csv_file</code>: Create encrypted datasets using <code>SecureDataHandler(\"test-pass\")</code> for end-to-end decryption tests.</li> <li>Existing <code>temp_tasks_dir</code>, <code>example_task_definition</code> support task-related tests.</li> </ul>"},{"location":"testing/#external-services-and-mocking","title":"External Services and Mocking","text":"<ul> <li>PubMed API is mocked in unit tests via <code>monkeypatch</code>.</li> <li>A live smoke test exists in <code>tests/test_pubmed_live.py</code> guarded by <code>RUN_PUBMED_LIVE=1</code> and optional <code>NCBI_API_KEY</code>.</li> <li>Databases are simulated using temporary SQLite files created in tests.</li> </ul>"},{"location":"testing/#security-considerations","title":"Security Considerations","text":"<ul> <li>PHI redaction is validated in both connectors and security handler tests.</li> <li>Encryption/decryption is tested across multiple algorithms; audit logging and key rotation are verified.</li> </ul>"},{"location":"testing/#performance-benchmarks","title":"Performance Benchmarks","text":"<ul> <li>Lightweight performance checks exist for connectors and core components.</li> <li>Run <code>pytest -k perf -q</code> to filter performance-oriented tests.</li> <li>Large batched read tests enforce modest time upper bounds tuned for CI to catch regressions without flakiness.</li> </ul>"},{"location":"testing/#coverage","title":"Coverage","text":"<ul> <li>Pytest is configured to collect coverage for <code>bench/*</code> and produce terminal and HTML reports.</li> <li>CI uploads <code>htmlcov</code> so you can review annotated coverage on PRs.</li> </ul>"},{"location":"testing/#tips","title":"Tips","text":"<ul> <li>Use <code>-k &lt;expr&gt;</code> to focus on a subset, e.g., <code>pytest -k medical -q</code>.</li> <li>Use <code>-m integration</code> to include integration tests; mark heavy tests with <code>@pytest.mark.integration</code>.</li> <li>CI splits unit vs. integration suites: the main job excludes <code>integration</code>, while a separate job runs only <code>integration</code> tests.</li> </ul>"},{"location":"testing_guide/","title":"MedAISure Testing Guide","text":"<p>This document summarizes the test coverage added for the core data models under <code>bench/models/</code> and how to run them.</p>"},{"location":"testing_guide/#scope-covered","title":"Scope Covered","text":"<ul> <li>MedicalTask (<code>bench/models/medical_task.py</code>)</li> <li>EvaluationResult (<code>bench/models/evaluation_result.py</code>)</li> <li>BenchmarkReport (<code>bench/models/benchmark_report.py</code>)</li> </ul>"},{"location":"testing_guide/#test-categories","title":"Test Categories","text":"<ul> <li>Property-based tests: <code>tests/test_models_property_based_test.py</code></li> <li>Randomized generation validates model constraints and JSON round-trips.</li> <li>Serialization round-trip tests: <code>tests/test_models_serialization_roundtrip_test.py</code></li> <li>Validates <code>to_dict</code>/<code>from_dict</code>, <code>to_json</code>/<code>from_json</code>, <code>to_yaml</code>/<code>from_yaml</code>, and file I/O helpers.</li> <li>Verifies CSV helpers where applicable.</li> <li>Edge case tests: <code>tests/test_models_edge_cases_test.py</code></li> <li>Invalid inputs, boundary conditions, timezone normalization, metric validation strictness.</li> <li>Performance smoke tests: <code>tests/test_models_performance_test.py</code></li> <li>Aggregation throughput for <code>BenchmarkReport.add_evaluation_result</code>.</li> <li>JSON/YAML serialization throughput for <code>BenchmarkReport</code>.</li> <li>Integration tests: <code>tests/test_models_integration_test.py</code></li> <li><code>EvaluationResult.validate_against_task()</code> with <code>MedicalTask</code> schemas.</li> <li><code>BenchmarkReport.validate_against_tasks()</code> and aggregate correctness across tasks.</li> </ul>"},{"location":"testing_guide/#running-tests","title":"Running Tests","text":"<ul> <li>Fast model-focused subset:</li> </ul> <pre><code>pytest -q tests/test_models_property_based_test.py \\\n         tests/test_models_serialization_roundtrip_test.py \\\n         tests/test_models_edge_cases_test.py \\\n         tests/test_models_integration_test.py\n</code></pre> <ul> <li>Include performance (benchmarks):</li> </ul> <pre><code>pytest -q tests/test_models_performance_test.py --benchmark-only\n</code></pre>"},{"location":"testing_guide/#dev-dependencies","title":"Dev Dependencies","text":"<ul> <li>Property-based tests require <code>hypothesis</code>. Ensure you have dev deps installed:</li> </ul> <pre><code>pip install -r requirements-dev.txt\n</code></pre>"},{"location":"testing_guide/#coverage","title":"Coverage","text":"<ul> <li>Coverage is enabled via <code>pytest-cov</code> in <code>requirements-dev.txt</code>.</li> </ul> <p>Generate an HTML coverage report for the entire suite:</p> <pre><code>pytest --cov=bench --cov-report=html\nopen htmlcov/index.html  # or use a file viewer\n</code></pre>"},{"location":"testing_guide/#notes","title":"Notes","text":"<ul> <li><code>EvaluationResult.metrics_results</code> now rejects non-numeric, NaN, and Infinity values.</li> <li>Timestamp fields are normalized to timezone-aware UTC; tests compare with <code>exclude={\"timestamp\"}</code> when appropriate.</li> </ul>"},{"location":"testing_integration_coverage/","title":"Integration Tests Coverage for Metrics System","text":"<p>This document summarizes the integration tests that validate the end-to-end metrics system in MedAISure.</p>"},{"location":"testing_integration_coverage/#scope-covered","title":"Scope Covered","text":"<ul> <li>End-to-end pipeline</li> <li><code>EvaluationHarness.evaluate()</code> across tasks, including callbacks, caching, and strict validation.</li> <li> <p>Reference: <code>tests/test_integration_end_to_end_local_model_test.py</code></p> </li> <li> <p>Model Runner integration</p> </li> <li>Model loading (local/HF mocked), batched inference, error propagation, and resource cleanup.</li> <li> <p>Reference: <code>tests/test_integration_end_to_end_local_model_test.py</code></p> </li> <li> <p>Metric aggregation and reports</p> </li> <li>Aggregation, statistics, filtering/sorting, and exporters (JSON/CSV/Markdown/HTML).</li> <li> <p>References:</p> <ul> <li><code>tests/test_result_aggregator_extended_test.py</code></li> <li><code>bench/evaluation/result_aggregator.py</code></li> </ul> </li> <li> <p>Performance smoke / large datasets</p> </li> <li>Smoke performance and large dataset scenarios validated.</li> <li> <p>Reference: <code>tests/test_integration_end_to_end_local_model_test.py</code></p> </li> <li> <p>Human judgment comparison</p> </li> <li>Custom <code>human_judgment</code> metric compared across runs using <code>ResultAggregator.compare_runs()</code>.</li> <li> <p>Reference: <code>tests/test_integration_human_judgment_and_regression_test.py::test_human_judgment_comparison_via_compare_runs</code></p> </li> <li> <p>Regression tests (comparisons)</p> </li> <li>Absolute and relative diffs between baseline and current runs using <code>compare_runs(relative=True)</code>.</li> <li>Reference: <code>tests/test_integration_human_judgment_and_regression_test.py::test_regression_detection_relative_diff</code></li> </ul>"},{"location":"testing_integration_coverage/#how-to-run-tests","title":"How to Run Tests","text":"<pre><code>pytest -q\n</code></pre> <p>All tests should pass; as of the latest run: 174 passed.</p>"},{"location":"testing_integration_coverage/#using-run-comparisons-in-practice","title":"Using Run Comparisons in Practice","text":"<ul> <li> <p>Absolute diff <pre><code>diff = agg.compare_runs(\"baseline\", \"current\", metrics=[\"accuracy\"], relative=False)\nprint(diff[\"overall\"][\"accuracy\"])  # positive =&gt; improvement\n</code></pre></p> </li> <li> <p>Relative diff <pre><code>diff = agg.compare_runs(\"baseline\", \"current\", metrics=[\"accuracy\"], relative=True)\n# (b - a) / (|a| + eps)\n</code></pre></p> </li> </ul>"},{"location":"testing_integration_coverage/#exported-reports","title":"Exported Reports","text":"<ul> <li><code>ResultAggregator</code> supports exporting a run report to multiple formats:</li> <li>JSON: <code>export_report_json(run_id, path)</code></li> <li>CSV: <code>export_report_csv(run_id, path)</code></li> <li>Markdown: <code>export_report_markdown(run_id, path)</code></li> <li>HTML: <code>export_report_html(run_id, path)</code></li> </ul> <p>See <code>tests/test_result_aggregator_extended_test.py</code> for usage examples.</p>"},{"location":"testing_integration_coverage/#notes","title":"Notes","text":"<ul> <li>Human judgment comparisons use a dedicated metric name (<code>human_judgment</code>) so they do not interfere with other metrics.</li> <li>Tests are designed to be lightweight and avoid heavy model dependencies.</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#common-issues","title":"Common Issues","text":"<ul> <li>Missing <code>transformers</code> or <code>torch</code> when using Hugging Face</li> <li> <p>Install: <code>pip install transformers torch</code></p> </li> <li> <p><code>rouge-score not available</code> in logs</p> </li> <li> <p>Install: <code>pip install rouge-score</code></p> </li> <li> <p>Model outputs length != inputs length</p> </li> <li> <p>Ensure your model returns one prediction dict per input example.</p> </li> <li> <p>Metrics are NaN</p> </li> <li>Check that reference fields exist in your task dataset (<code>label</code>, <code>answer</code>, <code>summary</code>, <code>note</code>, <code>rationale</code>)</li> <li> <p>Make sure prediction dicts include a compatible field (<code>label</code>, <code>prediction</code>, <code>text</code>, <code>summary</code>)</p> </li> <li> <p>Local model loader errors</p> </li> <li> <p>Provide <code>module_path</code> and ensure it exposes a callable loader (default <code>load_model(model_path, **kwargs)</code>)</p> </li> <li> <p>API request failures</p> </li> <li>Verify <code>endpoint</code>, <code>api_key</code>, and network access</li> <li>Tune <code>timeout</code>, <code>max_retries</code>, <code>backoff_factor</code></li> </ul>"},{"location":"troubleshooting/#debug-tips","title":"Debug Tips","text":"<ul> <li>Set <code>log_level=\"DEBUG\"</code> in <code>EvaluationHarness</code></li> <li>Use <code>use_cache=False</code> to re-run fresh predictions</li> <li>Inspect cached files in <code>&lt;cache_dir&gt;/&lt;run_id&gt;_&lt;task_id&gt;.json</code></li> <li>Enable <code>on_metrics</code> callback to stream metrics after each task</li> </ul>"},{"location":"usage/","title":"Usage Guide","text":"<p>This guide shows how to run evaluations with the MedAISure framework using the public API in <code>bench/evaluation/</code>.</p> <ul> <li>Core class: <code>EvaluationHarness</code> in <code>bench/evaluation/harness.py</code></li> <li>Supporting components: <code>TaskLoader</code>, <code>ModelRunner</code>, <code>MetricCalculator</code>, <code>ResultAggregator</code></li> </ul>"},{"location":"usage/#quick-start","title":"Quick Start","text":"<pre><code>from bench.evaluation import EvaluationHarness\n\nh = EvaluationHarness(\n    tasks_dir=\"bench/tasks\",\n    results_dir=\"bench/results\",\n    cache_dir=\"bench/results/cache\",\n    log_level=\"INFO\",\n)\n\n# Discover tasks and pick one\ntasks = h.list_available_tasks()\nprint([t[\"task_id\"] for t in tasks])\n\nreport = h.evaluate(\n    model_id=\"textattack/bert-base-uncased-MNLI\",\n    task_ids=[tasks[0][\"task_id\"]],\n    model_type=\"huggingface\",\n    model_kwargs={\"num_labels\": 3},\n    pipeline_kwargs={\"top_k\": 1},\n    batch_size=4,\n    use_cache=False,\n)\nprint(report.overall_scores)\n</code></pre>"},{"location":"usage/#running-with-different-model-types","title":"Running with Different Model Types","text":"<ul> <li>Hugging Face: <code>model_type=\"huggingface\"</code> with <code>model_kwargs</code>, <code>pipeline_kwargs</code> (see <code>ModelRunner._load_huggingface_model()</code>)</li> <li>Local model: <code>model_type=\"local\"</code> with <code>model_path</code>, <code>module_path</code>, optional <code>load_func</code> (see <code>ModelRunner._load_local_model()</code>)</li> <li>API model: <code>model_type=\"api\"</code> with <code>endpoint</code>, <code>api_key</code> and optional retry settings (see <code>ModelRunner._load_api_model()</code>)</li> </ul>"},{"location":"usage/#huggingface-advanced-options-generation-quick-start","title":"HuggingFace advanced options &amp; generation quick start","text":"<p>When loading Hugging Face models via <code>ModelRunner.load_model(..., model_type=\"huggingface\", ...)</code>, you can pass:</p> <ul> <li><code>hf_task</code>: <code>text-classification | summarization | text-generation</code></li> <li><code>generation_kwargs</code>: forwarded during inference for generative tasks (e.g., <code>max_new_tokens</code>, <code>temperature</code>, <code>do_sample</code>, <code>top_p</code>, <code>top_k</code>)</li> <li>Advanced loading: <code>device_map</code>, <code>torch_dtype</code>, <code>low_cpu_mem_usage</code>, <code>revision</code>, <code>trust_remote_code</code></li> </ul> <p>Quick start (generation parameters):</p> <pre><code>from bench.evaluation.model_runner import ModelRunner\n\nmr = ModelRunner()\nmr.load_model(\n    \"facebook/bart-large-cnn\",\n    model_type=\"huggingface\",\n    hf_task=\"summarization\",\n    generation_kwargs={\"max_new_tokens\": 96, \"temperature\": 0.7, \"do_sample\": True, \"top_p\": 0.9},\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n)\n\ninputs = [{\"document\": \"Long clinical note ...\"}]\npreds = mr.run_model(\"facebook/bart-large-cnn\", inputs, batch_size=1)\nprint(preds[0][\"summary\"])\n</code></pre> <p>See also the runnable examples: - <code>bench/examples/run_hf_summarization_gen.py</code> - <code>bench/examples/run_hf_text_generation_gen.py</code></p>"},{"location":"usage/#task-selection","title":"Task Selection","text":"<p><code>EvaluationHarness.list_available_tasks()</code> scans <code>tasks_dir</code> for YAML/JSON files. For advanced loading (file path or URL), use <code>TaskLoader.load_task()</code> directly.</p>"},{"location":"usage/#results-and-reports","title":"Results and Reports","text":"<ul> <li>Per-task results: collected as <code>EvaluationResult</code> in <code>bench/models/evaluation_result.py</code></li> <li>Aggregation: <code>ResultAggregator</code> builds a <code>BenchmarkReport</code> and supports CSV/Markdown/HTML export</li> <li>Save path: <code>&lt;results_dir&gt;/&lt;run_id&gt;.json</code></li> </ul>"},{"location":"usage/#caching","title":"Caching","text":"<p>If <code>cache_dir</code> is set, predictions per task are cached to JSON (<code>&lt;run_id&gt;_&lt;task_id&gt;.json</code>). Disable with <code>use_cache=False</code>.</p>"},{"location":"usage/#callbacks","title":"Callbacks","text":"<p><code>EvaluationHarness</code> accepts optional callbacks: - <code>on_task_start(task_id)</code>, <code>on_task_end(task_id, result)</code> - <code>on_progress(idx, total, current_task)</code> - <code>on_error(task_id, exception)</code> - <code>on_metrics(task_id, metrics_dict)</code></p> <p>See their usage throughout <code>EvaluationHarness.evaluate()</code>.</p>"},{"location":"api/cli/","title":"CLI","text":"<p>Command-line usage for running evaluations and utilities. MedAISure does not ship a standalone CLI binary; use Python module invocations or short scripts.</p>"},{"location":"api/cli/#common-invocations","title":"Common invocations","text":"<ul> <li> <p>GPU smoke test <pre><code>python scripts/gpu_smoke.py\n</code></pre></p> </li> <li> <p>Run unit tests <pre><code>pytest -q\n</code></pre></p> </li> <li> <p>Evaluate via a short Python one-liner <pre><code>python - &lt;&lt;'PY'\nfrom bench.evaluation.harness import EvaluationHarness\n\nh = EvaluationHarness(tasks_dir=\"tasks\", results_dir=\"results\", log_level=\"INFO\")\nreport = h.evaluate(\n    model_id=\"hf-sum\",\n    task_ids=[\"medical_qa\"],\n    model_type=\"huggingface\",\n    batch_size=8,\n    use_cache=True,\n    save_results=True,\n    strict_validation=False,\n    report_formats=[\"json\"],\n    model_path=\"sshleifer/tiny-t5\",\n    hf_task=\"summarization\",\n    generation_kwargs={\"max_new_tokens\": 64},\n)\nprint(report.overall_scores)\nPY\n</code></pre></p> </li> </ul>"},{"location":"api/cli/#key-parameters-map-to-evaluationharnessevaluate","title":"Key parameters (map to EvaluationHarness.evaluate)","text":"<ul> <li><code>model_id</code> (str): identifier used to register/load the model in <code>ModelRunner</code>.</li> <li><code>task_ids</code> (list[str]): tasks to evaluate.</li> <li><code>model_type</code> (str): <code>huggingface</code>, <code>local</code>, or <code>api</code>.</li> <li><code>batch_size</code> (int): batch size for inference.</li> <li><code>use_cache</code> (bool): use cached results if available.</li> <li><code>save_results</code> (bool): write results to <code>results_dir</code>.</li> <li><code>strict_validation</code> (bool): raise on schema validation errors.</li> <li><code>report_formats</code> (list[str]): optional extra outputs (e.g., <code>json</code>, <code>md</code>).</li> <li><code>report_dir</code> (str): optional output directory for extra formats.</li> <li><code>**model_kwargs</code>: forwarded to <code>ModelRunner.load_model(...)</code> (e.g., <code>model_path</code>, <code>hf_task</code>, <code>generation_kwargs</code>, <code>endpoint</code>, <code>api_key</code>).</li> </ul>"},{"location":"api/cli/#environment-variables","title":"Environment variables","text":"<ul> <li><code>MEDAISURE_NO_RICH=1</code> disables the tqdm progress bar/animations during evaluation.</li> </ul>"},{"location":"api/cli/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Missing dependencies: ensure requirements are installed   <pre><code>pip install -r requirements.txt\n</code></pre></li> <li>HF model fails to load: verify <code>model_path</code>/<code>revision</code>/<code>trust_remote_code</code>; check GPU availability and <code>device_map</code>.</li> <li>Unexpected HF text output: set appropriate <code>hf_task</code> and <code>generation_kwargs</code> (e.g., <code>max_new_tokens</code>, <code>do_sample=False</code>).</li> <li>API model errors: verify <code>endpoint</code>, auth (<code>api_key</code>/<code>headers</code>), and response shape <code>{ \"outputs\": [ ... ] }</code>.</li> <li>Validation errors: use <code>strict_validation=True</code> to fail fast and inspect schemas in docs/tasks/overview.md and API \u2192 Task schema.</li> <li>Empty metrics: confirm the task defines metrics; see Metrics overview and API \u2192 Metrics (clinical).</li> </ul>"},{"location":"api/core_api/","title":"Core API","text":"<p>This section documents the primary Python interfaces used to run tasks, models, and evaluations.</p> <p>Key Modules - <code>bench.evaluation.harness.EvaluationHarness</code>: orchestrates task loading, model execution, metrics, reports - <code>bench.evaluation.model_runner.ModelRunner</code>: unified runner for HuggingFace, local modules, API models - <code>bench.evaluation.metrics.*</code>: concrete metric implementations (e.g., <code>clinical.ClinicalAccuracyMetric</code>) - <code>bench.models.medical_task.MedicalTask</code>: canonical task schema</p> <p>Typical Flow (end-to-end evaluate) <pre><code>from bench.evaluation.harness import EvaluationHarness\n\nh = EvaluationHarness(\n    tasks_dir=\"bench/tasks\",\n    results_dir=\"bench/results\",\n    cache_dir=\"bench/results/cache\",\n    log_level=\"INFO\",\n)\n\n# Evaluate a HuggingFace pipeline on one or more tasks\nreport = h.evaluate(\n    model_id=\"hf-sum\",\n    task_ids=[\"clinical_summarization_basic\"],\n    model_type=\"huggingface\",\n    model_path=\"sshleifer/tiny-t5\",\n    hf_task=\"summarization\",\n    # Optional advanced HF settings (see ModelRunner):\n    device=-1,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n    trust_remote_code=False,\n    low_cpu_mem_usage=True,\n    generation_kwargs={\"max_new_tokens\": 64, \"do_sample\": False},\n)\nprint(report.overall_scores)\n</code></pre></p> <p>Using local Python model <pre><code>from bench.evaluation.harness import EvaluationHarness\n\nh = EvaluationHarness(tasks_dir=\"bench/tasks\", results_dir=\"bench/results\")\nreport = h.evaluate(\n    model_id=\"my_local_model\",\n    task_ids=[\"some_task_id\"],\n    model_type=\"local\",\n    module_path=\"bench.examples.mypkg.mylocal\",  # provides load_model()\n    # optional: load_func=\"load_model\", model_path=\"/path/to/weights\",\n    batch_size=4,\n)\n</code></pre></p> <p>Using a simple HTTP API model <pre><code>from bench.evaluation.harness import EvaluationHarness\n\nh = EvaluationHarness(tasks_dir=\"bench/tasks\", results_dir=\"bench/results\")\nreport = h.evaluate(\n    model_id=\"api-demo\",\n    task_ids=[\"medical_qa_basic\"],\n    model_type=\"api\",\n    endpoint=\"https://api.example/v1/predict\",\n    api_key=\"sk-...\",\n    timeout=30.0,\n    max_retries=1,\n)\n</code></pre></p> <p>Notes - Generation kwargs pass-through for HF pipelines is supported via <code>generation_kwargs</code> in <code>ModelRunner</code>. - Advanced HF loading options supported: <code>device_map</code>, <code>torch_dtype</code>, <code>trust_remote_code</code>, <code>low_cpu_mem_usage</code>, <code>revision</code>. - Metadata extraction (e.g., config fields, parameter count, dtype, device) is attempted when available.</p> <p>See also: API reference (mkdocstrings) - Python API \u2192 Evaluation Harness: api/reference.md#evaluation-harness - Python API \u2192 Model Runner: api/reference.md#model-runner - Python API \u2192 Metrics (clinical): api/reference.md#metrics-clinical - Python API \u2192 Task schema: api/reference.md#task-schema</p> <p>Source modules: - <code>bench/evaluation/harness.py</code> - <code>bench/evaluation/model_runner.py</code> - <code>bench/evaluation/metrics/clinical.py</code> - <code>bench/models/medical_task.py</code></p> <p>Examples - End-to-end local model run: <code>bench/examples/run_local_model.py</code></p>"},{"location":"api/overview/","title":"API Overview","text":"<p>This section documents the Python API using auto-generated references via mkdocstrings.</p> <ul> <li>Core modules</li> <li>Public classes and methods</li> </ul>"},{"location":"api/reference/","title":"Python API Reference","text":"<p>This page renders the public Python APIs using mkdocstrings for convenient deep links.</p>"},{"location":"api/reference/#evaluation-harness","title":"Evaluation Harness","text":""},{"location":"api/reference/#bench.evaluation.harness","title":"<code>bench.evaluation.harness</code>","text":"<p>Evaluation harness for running benchmarks on medical AI models.</p>"},{"location":"api/reference/#bench.evaluation.harness.EvaluationHarness","title":"<code>EvaluationHarness</code>","text":"<p>Main class for running evaluations on medical AI models.</p> Source code in <code>bench/evaluation/harness.py</code> <pre><code>class EvaluationHarness:\n    \"\"\"Main class for running evaluations on medical AI models.\"\"\"\n\n    def __init__(\n        self,\n        tasks_dir: str = \"tasks\",\n        results_dir: str = \"results\",\n        cache_dir: Optional[str] = None,\n        log_level: str = \"INFO\",\n        # Optional event callbacks\n        on_task_start: Optional[Callable[[str], None]] = None,\n        on_task_end: Optional[Callable[[str, Any], None]] = None,\n        on_progress: Optional[Callable[[int, int, Optional[str]], None]] = None,\n        on_error: Optional[Callable[[str, Exception], None]] = None,\n        on_metrics: Optional[Callable[[str, Dict[str, Any]], None]] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the evaluation harness.\n\n        Args:\n            tasks_dir: Directory containing task definitions\n            results_dir: Directory to save evaluation results\n            cache_dir: Directory for caching model outputs (optional)\n            log_level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n        \"\"\"\n        # Set up logging\n        logging.basicConfig(\n            level=getattr(logging, log_level.upper()),\n            format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n        )\n\n        # Initialize components with type annotations\n        self.task_loader: TaskLoader = TaskLoader(tasks_dir)\n        self.model_runner: ModelRunner[Any, Any, Any] = ModelRunner[Any, Any, Any]()\n        self.metric_calculator: MetricCalculator = MetricCalculator()\n        self.result_aggregator: ResultAggregator = ResultAggregator(results_dir)\n        self.tasks_dir: Path = Path(tasks_dir)\n        self.results_dir: Path = Path(results_dir)\n        self.cache_dir: Optional[Path] = Path(cache_dir) if cache_dir else None\n\n        # Set up directories\n        self.tasks_dir.mkdir(parents=True, exist_ok=True)\n        self.results_dir.mkdir(parents=True, exist_ok=True)\n\n        if self.cache_dir:\n            self.cache_dir.mkdir(parents=True, exist_ok=True)\n\n        # Event callbacks\n        self._on_task_start = on_task_start\n        self._on_task_end = on_task_end\n        self._on_progress = on_progress\n        self._on_error = on_error\n        self._on_metrics = on_metrics\n\n        # Internal: track the active model id for cleanup\n        self._active_model_id: Optional[str] = None\n        # Internal: validation behavior flag set per evaluate() call\n        self._strict_validation: bool = False\n\n        logger.info(\n            \"Initialized EvaluationHarness with \"\n            f\"tasks_dir={tasks_dir}, results_dir={results_dir}\"\n        )\n\n    # ---------------\n    # Context manager\n    # ---------------\n    def __enter__(self) -&gt; \"EvaluationHarness\":\n        return self\n\n    def __exit__(self, exc_type, exc, tb) -&gt; None:\n        try:\n            self.close()\n        except Exception:\n            # Best-effort cleanup\n            pass\n\n    def close(self) -&gt; None:\n        \"\"\"Cleanup resources, e.g., unload the active model if loaded.\"\"\"\n        if self._active_model_id is not None:\n            try:\n                self.model_runner.unload_model(self._active_model_id)\n            finally:\n                self._active_model_id = None\n\n    def evaluate(\n        self,\n        model_id: str,\n        task_ids: List[str],\n        model_type: str = \"huggingface\",\n        batch_size: int = 8,\n        use_cache: bool = True,\n        save_results: bool = True,\n        strict_validation: bool = False,\n        report_formats: Optional[List[str]] = None,\n        report_dir: Optional[str] = None,\n        **model_kwargs: Any,\n    ) -&gt; BenchmarkReport:\n        \"\"\"Run evaluation on the specified model and tasks.\n\n        Args:\n            model_id: Identifier for the model to evaluate\n            task_ids: List of task IDs to evaluate on\n            model_type: Type of model ('huggingface', 'local', 'api')\n            batch_size: Batch size for model inference\n            use_cache: Whether to use cached results if available\n            save_results: Whether to save results to disk\n            **model_kwargs: Additional arguments for model loading\n\n        Returns:\n            BenchmarkReport containing evaluation results\n        \"\"\"\n        # Generate a unique run ID\n        run_id = self._generate_run_id(model_id, task_ids)\n        logger.info(f\"Starting evaluation run {run_id}\")\n\n        # Initialize the model\n        logger.info(f\"Initializing model: {model_id} (type: {model_type})\")\n        # Load the model (modifies the model_runner's internal state)\n        self.model_runner.load_model(\n            model_name=model_id,  # Use model_name parameter\n            model_type=model_type,\n            **model_kwargs,\n        )\n        self._active_model_id = model_id\n        # Get the loaded model from the runner's _models dictionary\n        model = self.model_runner._models[model_id]\n\n        # Set validation mode for this run\n        self._strict_validation = bool(strict_validation)\n\n        # Initialize results\n        task_results = {}\n        start_time = time.time()\n\n        # Evaluate on each task\n        disable_tqdm = os.environ.get(\"MEDAISURE_NO_RICH\") == \"1\"\n        for idx, task_id in enumerate(\n            tqdm(task_ids, desc=\"Evaluating tasks\", disable=disable_tqdm), start=1\n        ):\n            task: Optional[MedicalTask] = None\n            try:\n                if self._on_progress:\n                    self._on_progress(idx, len(task_ids), task_id)\n                if self._on_task_start:\n                    self._on_task_start(task_id)\n\n                logger.info(f\"Evaluating on task: {task_id}\")\n\n                # Load task\n                task = self.task_loader.load_task(task_id)\n\n                # Check cache\n                cache_key = self._get_cache_key(run_id, task_id)\n                cached_result = self._load_from_cache(cache_key) if use_cache else None\n\n                if cached_result:\n                    logger.info(f\"Using cached results for task {task_id}\")\n                    task_result = cached_result\n                else:\n                    # Run evaluation\n                    task_result = self._evaluate_task(\n                        model=model,\n                        model_id=model_id,\n                        task=task,\n                        batch_size=batch_size,\n                    )\n\n                    # Cache results\n                    if self.cache_dir:\n                        self._save_to_cache(cache_key, task_result)\n\n                # Store results\n                task_results[task_id] = task_result\n\n                # Call metrics hook\n                metrics_payload: Dict[str, Any] = {}\n                if hasattr(task_result, \"metrics_results\") and isinstance(\n                    task_result.metrics_results, dict\n                ):\n                    metrics_payload = task_result.metrics_results\n                elif (\n                    hasattr(task_result, \"detailed_results\")\n                    and task_result.detailed_results\n                ):\n                    metrics_payload = task_result.detailed_results[0].metrics_results\n                if self._on_metrics and metrics_payload:\n                    self._on_metrics(task_id, metrics_payload)\n\n                # Log progress\n                if metrics_payload:\n                    logger.info(\n                        f\"Completed task {task_id} - \"\n                        f\"Metrics: {json.dumps(metrics_payload, indent=2)}\"\n                    )\n                else:\n                    logger.warning(f\"No detailed results available for task {task_id}\")\n\n                if self._on_task_end:\n                    self._on_task_end(task_id, task_result)\n\n            except Exception as e:\n                # In strict validation mode, propagate exceptions to fail fast\n                if self._strict_validation:\n                    raise\n                if self._on_error:\n                    try:\n                        self._on_error(task_id, e)\n                    except Exception:\n                        # Avoid callback exceptions breaking flow\n                        pass\n                logger.error(\n                    f\"Error evaluating task {task_id}: {str(e)}\", exc_info=True\n                )\n                # Insert a placeholder result so the task key appears in the final report\n                try:\n                    # Attempt to use the task's declared metrics for placeholder keys\n                    metrics_dict: Dict[str, float] = {}\n                    if task is not None and getattr(task, \"metrics\", None):\n                        names: List[str] = []\n                        for m in task.metrics:  # type: ignore[union-attr]\n                            if isinstance(m, dict) and \"name\" in m:\n                                names.append(str(m[\"name\"]))\n                            elif isinstance(m, str):\n                                names.append(m)\n                        metrics_dict = {name: 0.0 for name in names}\n                    else:\n                        # Fallback if task isn't available\n                        metrics_dict = {\"score\": 0.0}\n\n                    placeholder = EvaluationResult(\n                        model_id=model_id,\n                        task_id=task_id,\n                        inputs=[],\n                        model_outputs=[],\n                        metrics_results=metrics_dict,\n                        metadata={\n                            \"error\": str(e),\n                            \"note\": \"Placeholder result due to evaluation failure\",\n                        },\n                    )\n                    task_results[task_id] = placeholder\n                    # Intentionally do NOT call on_task_end for error placeholders to\n                    # maintain semantics expected by tests (only successful tasks end).\n                except Exception:\n                    # As a last resort, skip adding results for this task\n                    pass\n                # Continue with other tasks on error\n                continue\n\n        # Calculate total time\n        total_time = time.time() - start_time\n\n        # Build report via ResultAggregator/BenchmarkReport to get consistent aggregation\n        # Use the run_id for grouping\n        for task_id, res in task_results.items():\n            self.result_aggregator.add_evaluation_result(res, run_id=run_id)\n\n        # Try to get report from aggregator; if mocked or unavailable, fall back to manual construction\n        try:\n            report = self.result_aggregator.get_report(run_id)\n        except Exception:\n            report = None\n\n        if not isinstance(report, BenchmarkReport):\n            # Fallback: construct a minimal BenchmarkReport from task_results\n            task_scores: Dict[str, Dict[str, float]] = {}\n            detailed_results: List[EvaluationResult] = []\n\n            for t_id, res in task_results.items():\n                # Metrics may be in metrics_results or detailed_results[0].metrics_results\n                if hasattr(res, \"metrics_results\") and isinstance(\n                    res.metrics_results, dict\n                ):\n                    metrics = res.metrics_results\n                elif hasattr(res, \"detailed_results\") and res.detailed_results:\n                    metrics = res.detailed_results[0].metrics_results\n                else:\n                    metrics = {}\n\n                # Compute a simple average if numeric metrics exist; else 0.0\n                avg = 0.0\n                numeric_vals = [\n                    float(v) for v in metrics.values() if isinstance(v, (int, float))\n                ]\n                if numeric_vals:\n                    avg = sum(numeric_vals) / len(numeric_vals)\n                task_scores[t_id] = {\"average_score\": avg}\n\n                # Collect detailed results only if present (to preserve legacy placeholder behavior)\n                if hasattr(res, \"detailed_results\") and res.detailed_results:\n                    detailed_results.extend(res.detailed_results)\n\n            overall_avg = 0.0\n            if task_scores:\n                overall_avg = sum(\n                    s[\"average_score\"] for s in task_scores.values()\n                ) / len(task_scores)\n\n            # If no detailed results collected, add legacy placeholder result\n            if not detailed_results:\n                detailed_results = [\n                    EvaluationResult(\n                        model_id=model_id,\n                        task_id=\"unknown\",\n                        inputs=[],\n                        model_outputs=[],\n                        metrics_results={\"score\": 0.0},\n                        timestamp=datetime.now(timezone.utc).isoformat(),\n                        metadata={\n                            \"note\": \"Placeholder result - no tasks completed successfully\"\n                        },\n                    )\n                ]\n\n            report = BenchmarkReport(\n                model_id=model_id,\n                overall_scores={\"average_score\": overall_avg},\n                task_scores=task_scores,\n                detailed_results=detailed_results,\n                metadata={},\n            )\n        # Enrich metadata\n        report.metadata.update(\n            {\n                \"run_id\": run_id,\n                \"model_name\": model_id,\n                \"total_time_seconds\": total_time,\n                \"num_tasks\": len(task_results),\n                \"batch_size\": batch_size,\n                \"model_type\": model_type,\n                **model_kwargs,\n            }\n        )\n\n        # If any EvaluationResult captured validation issues, surface them at report level\n        try:\n            all_val_errors: list[str] = []\n            for res in report.detailed_results or []:\n                errs = []\n                meta = getattr(res, \"metadata\", {}) or {}\n                if isinstance(meta, dict):\n                    errs = meta.get(\"validation_errors\", []) or []\n                if errs:\n                    # Flatten to strings\n                    for e in errs:\n                        all_val_errors.append(str(e))\n            if all_val_errors:\n                # De-duplicate while preserving order\n                seen = set()\n                deduped: list[str] = []\n                for e in all_val_errors:\n                    if e not in seen:\n                        deduped.append(e)\n                        seen.add(e)\n                report.metadata[\"validation_errors\"] = deduped\n        except Exception:\n            # Best-effort enrichment; never block evaluation on metadata issues\n            pass\n\n        # Save results\n        try:\n            if save_results:\n                output_file = self.results_dir / f\"{run_id}.json\"\n                report.save(output_file)\n                logger.info(f\"Saved evaluation results to {output_file}\")\n                # Optionally export additional report formats via report generators\n                if report_formats:\n                    from ..reports import ReportFactory  # lazy import\n\n                    out_dir = Path(report_dir) if report_dir else self.results_dir\n                    out_dir.mkdir(parents=True, exist_ok=True)\n                    for fmt in report_formats:\n                        fmt_norm = str(fmt).lower().strip()\n                        try:\n                            gen = ReportFactory.create_generator(fmt_norm)\n                        except Exception as e:\n                            logger.warning(\n                                \"Skipping unknown report format '%s': %s\",\n                                fmt_norm,\n                                e,\n                            )\n                            continue\n                        content = gen.generate(report)\n                        # Choose extension\n                        ext = \"md\" if fmt_norm in {\"md\", \"markdown\"} else fmt_norm\n                        target = out_dir / f\"{run_id}.{ext}\"\n                        try:\n                            gen.save(content, target)\n                            logger.info(\"Exported %s report to %s\", fmt_norm, target)\n                        except Exception as e:\n                            logger.warning(\n                                \"Failed to export %s report to %s: %s\",\n                                fmt_norm,\n                                target,\n                                e,\n                            )\n            return report\n        finally:\n            # Always attempt cleanup\n            self.close()\n\n    def _evaluate_task(  # noqa: C901\n        self,\n        model: Any,\n        model_id: str,\n        task: MedicalTask,\n        batch_size: int = 8,\n    ) -&gt; EvaluationResult:\n        \"\"\"Evaluate a single task.\n\n        Args:\n            model: The model to evaluate\n            model_id: ID of the model\n            task: The task to evaluate on\n            batch_size: Batch size for evaluation\n\n        Returns:\n            EvaluationResult containing evaluation results\n        \"\"\"\n\n        # Prepare inputs\n        inputs = task.dataset\n        if not inputs:\n            raise ValueError(f\"No dataset found for task {task.task_id}\")\n\n        # Validate inputs against schema (non-strict by default)\n        validation_errors: List[str] = []\n        try:\n            in_schema, out_schema = ensure_task_schemas(\n                task.task_type, task.input_schema, task.output_schema\n            )\n            for idx, rec in enumerate(inputs):\n                # Support nested dataset rows with 'input'\n                payload = rec.get(\"input\") if isinstance(rec, dict) else None\n                payload = payload if isinstance(payload, dict) else rec\n                validate_record_against_schema(\n                    payload, in_schema, label=\"inputs\", index=idx\n                )\n        except Exception as e:\n            if self._strict_validation:\n                raise\n            validation_errors.append(str(e))\n\n        # Run model inference\n        logger.info(f\"Running inference on {len(inputs)} examples...\")\n        # Extract input payloads for the model (support nested dataset rows)\n        model_inputs: List[Dict[str, Any]] = []\n        reference_outputs: List[Dict[str, Any]] = []\n        for rec in inputs:\n            if isinstance(rec, dict):\n                in_payload = (\n                    rec.get(\"input\") if isinstance(rec.get(\"input\"), dict) else rec\n                )\n                out_payload = (\n                    rec.get(\"output\") if isinstance(rec.get(\"output\"), dict) else {}\n                )\n                if isinstance(in_payload, dict):\n                    model_inputs.append(in_payload)\n                else:\n                    model_inputs.append(rec)  # fallback\n                if isinstance(out_payload, dict):\n                    reference_outputs.append(out_payload)\n                else:\n                    reference_outputs.append({})\n            else:\n                model_inputs.append({\"text\": str(rec)})\n                reference_outputs.append({})\n\n        predictions = self.model_runner.run_model(\n            model_id=model_id, inputs=model_inputs, batch_size=batch_size\n        )\n\n        # Validate outputs against schema (non-strict by default)\n        try:\n            # predictions are list[dict]; support models returning nested structures\n            for idx, rec in enumerate(predictions):\n                payload = rec.get(\"output\") if isinstance(rec, dict) else None\n                payload = payload if isinstance(payload, dict) else rec\n                validate_record_against_schema(\n                    payload, out_schema, label=\"model_outputs\", index=idx\n                )\n        except Exception as e:\n            if self._strict_validation:\n                raise\n            validation_errors.append(str(e))\n\n        # Calculate metrics\n        logger.info(\"Calculating metrics...\")\n        # Extract metric names, handling both dict and str cases\n        metric_names: List[str] = []\n        if hasattr(task, \"metrics\") and task.metrics:\n            for m in task.metrics:\n                if isinstance(m, dict) and \"name\" in m:\n                    metric_names.append(str(m[\"name\"]))\n                elif isinstance(m, str):\n                    metric_names.append(m)\n\n        metric_results = self.metric_calculator.calculate_metrics(\n            task_id=task.task_id,\n            predictions=predictions,\n            references=reference_outputs if any(reference_outputs) else inputs,\n            metric_names=metric_names,\n        )\n\n        # Extract metric values\n        metrics = {name: result.value for name, result in metric_results.items()}\n\n        # Create evaluation result\n        result = EvaluationResult(\n            model_id=model_id,\n            task_id=task.task_id,\n            inputs=inputs,\n            model_outputs=predictions,\n            metrics_results=metrics,\n            metadata={\n                \"timestamp\": time.time(),\n                \"batch_size\": batch_size,\n                \"metrics_metadata\": {\n                    name: asdict(result) for name, result in metric_results.items()\n                },\n                \"predictions\": predictions[:10],  # First few predictions\n            },\n        )\n        if validation_errors:\n            # Attach collected validation issues for user inspection\n            result.metadata[\"validation_errors\"] = validation_errors\n            logger.warning(\"Validation issues encountered: %s\", validation_errors)\n        return result\n\n    def _generate_run_id(self, model_id: str, task_ids: List[str]) -&gt; str:\n        \"\"\"Generate a unique run ID based on model and task IDs.\n\n        Args:\n            model_id: ID of the model being evaluated\n            task_ids: List of task IDs being evaluated\n\n        Returns:\n            A unique 8-character run ID\n        \"\"\"\n        import hashlib\n\n        # Create a unique string from model_id and task_ids\n        unique_str = f\"{model_id}_{'_'.join(sorted(task_ids))}\"\n\n        # Create a hash of the unique string\n        return hashlib.md5(unique_str.encode()).hexdigest()[:8]\n\n    def _get_cache_key(self, run_id: str, task_id: str) -&gt; Optional[str]:\n        \"\"\"Generate a cache key for storing/retrieving results.\n\n        Args:\n            run_id: ID of the current run\n            task_id: ID of the task\n\n        Returns:\n            Cache key as a string, or None if cache is disabled\n        \"\"\"\n        if not self.cache_dir:\n            return None\n        return str(self.cache_dir / f\"{run_id}_{task_id}.json\")\n\n    def _load_from_cache(self, cache_key: Optional[str]) -&gt; Optional[Any]:\n        \"\"\"Load results from cache if available.\n\n        Args:\n            cache_key: Path to the cache file, or None if caching is disabled\n\n        Returns:\n            Cached EvaluationResult if available and valid, None otherwise\n        \"\"\"\n        if not cache_key or not Path(cache_key).exists():\n            return None\n\n        try:\n            with open(cache_key, \"r\") as f:\n                data = json.load(f)\n\n            # Convert the loaded data into an EvaluationResult\n            from ..models.evaluation_result import EvaluationResult\n\n            return EvaluationResult(**data)\n\n        except Exception as e:\n            logger.warning(f\"Error loading from cache {cache_key}: {str(e)}\")\n            return None\n\n    def _save_to_cache(self, cache_key: Optional[str], data: Any) -&gt; None:\n        \"\"\"Save results to cache.\n\n        Args:\n            cache_key: Path to the cache file, or None if caching is disabled\n            data: Data to cache (must be an EvaluationResult or serializable dict)\n        \"\"\"\n        if not cache_key:\n            return\n\n        try:\n            # Create parent directories if they don't exist\n            cache_path = Path(cache_key)\n            cache_path.parent.mkdir(parents=True, exist_ok=True)\n\n            # Convert data to dict if it's an EvaluationResult\n            if hasattr(data, \"model_dump\"):\n                data_dict = data.model_dump()\n            elif hasattr(data, \"dict\"):\n                data_dict = data.dict()  # Fallback for older Pydantic versions\n            else:\n                data_dict = data\n\n            with open(cache_path, \"w\") as f:\n                json.dump(data_dict, f, indent=2, default=str)\n        except Exception as e:\n            logger.warning(f\"Error saving to cache {cache_key}: {str(e)}\")\n\n    def list_available_tasks(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"List all available tasks in the tasks directory.\"\"\"\n        tasks = []\n\n        # Look for YAML task definitions\n        for task_file in self.tasks_dir.glob(\"*.yaml\"):\n            try:\n                with open(task_file, \"r\") as f:\n                    task_data = yaml.safe_load(f)\n                    tasks.append(\n                        {\n                            \"task_id\": task_file.stem,\n                            \"name\": task_data.get(\"name\", \"\"),\n                            \"description\": task_data.get(\"description\", \"\"),\n                            \"metrics\": [\n                                m[\"name\"] for m in task_data.get(\"metrics\", [])\n                            ],\n                            \"num_examples\": len(task_data.get(\"dataset\", [])),\n                            \"file\": str(task_file),\n                        }\n                    )\n            except Exception as e:\n                logger.warning(f\"Error loading task from {task_file}: {str(e)}\")\n\n        # Look for JSON task definitions\n        for task_file in self.tasks_dir.glob(\"*.json\"):\n            if task_file.stem == \"tasks\":  # Skip the tasks index file\n                continue\n\n            try:\n                with open(task_file, \"r\") as f:\n                    task_data = json.load(f)\n                    tasks.append(\n                        {\n                            \"task_id\": task_file.stem,\n                            \"name\": task_data.get(\"name\", \"\"),\n                            \"description\": task_data.get(\"description\", \"\"),\n                            \"metrics\": [\n                                m[\"name\"] for m in task_data.get(\"metrics\", [])\n                            ],\n                            \"num_examples\": len(task_data.get(\"dataset\", [])),\n                            \"file\": str(task_file),\n                        }\n                    )\n            except Exception as e:\n                logger.warning(f\"Error loading task from {task_file}: {str(e)}\")\n\n        return tasks\n\n    def get_task_info(self, task_id: str) -&gt; Dict[str, Any]:\n        \"\"\"Get detailed information about a specific task.\"\"\"\n        task = self.task_loader.load_task(task_id)\n\n        return {\n            \"task_id\": task.task_id,\n            \"name\": task.name,\n            \"description\": task.description,\n            \"input_schema\": task.input_schema,\n            \"output_schema\": task.output_schema,\n            \"metrics\": task.metrics,\n            \"num_examples\": len(task.dataset) if task.dataset else 0,\n            \"example_input\": task.dataset[0] if task.dataset else None,\n        }\n</code></pre>"},{"location":"api/reference/#bench.evaluation.harness.EvaluationHarness.__init__","title":"<code>__init__(tasks_dir='tasks', results_dir='results', cache_dir=None, log_level='INFO', on_task_start=None, on_task_end=None, on_progress=None, on_error=None, on_metrics=None)</code>","text":"<p>Initialize the evaluation harness.</p> <p>Parameters:</p> Name Type Description Default <code>tasks_dir</code> <code>str</code> <p>Directory containing task definitions</p> <code>'tasks'</code> <code>results_dir</code> <code>str</code> <p>Directory to save evaluation results</p> <code>'results'</code> <code>cache_dir</code> <code>Optional[str]</code> <p>Directory for caching model outputs (optional)</p> <code>None</code> <code>log_level</code> <code>str</code> <p>Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)</p> <code>'INFO'</code> Source code in <code>bench/evaluation/harness.py</code> <pre><code>def __init__(\n    self,\n    tasks_dir: str = \"tasks\",\n    results_dir: str = \"results\",\n    cache_dir: Optional[str] = None,\n    log_level: str = \"INFO\",\n    # Optional event callbacks\n    on_task_start: Optional[Callable[[str], None]] = None,\n    on_task_end: Optional[Callable[[str, Any], None]] = None,\n    on_progress: Optional[Callable[[int, int, Optional[str]], None]] = None,\n    on_error: Optional[Callable[[str, Exception], None]] = None,\n    on_metrics: Optional[Callable[[str, Dict[str, Any]], None]] = None,\n) -&gt; None:\n    \"\"\"Initialize the evaluation harness.\n\n    Args:\n        tasks_dir: Directory containing task definitions\n        results_dir: Directory to save evaluation results\n        cache_dir: Directory for caching model outputs (optional)\n        log_level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n    \"\"\"\n    # Set up logging\n    logging.basicConfig(\n        level=getattr(logging, log_level.upper()),\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    )\n\n    # Initialize components with type annotations\n    self.task_loader: TaskLoader = TaskLoader(tasks_dir)\n    self.model_runner: ModelRunner[Any, Any, Any] = ModelRunner[Any, Any, Any]()\n    self.metric_calculator: MetricCalculator = MetricCalculator()\n    self.result_aggregator: ResultAggregator = ResultAggregator(results_dir)\n    self.tasks_dir: Path = Path(tasks_dir)\n    self.results_dir: Path = Path(results_dir)\n    self.cache_dir: Optional[Path] = Path(cache_dir) if cache_dir else None\n\n    # Set up directories\n    self.tasks_dir.mkdir(parents=True, exist_ok=True)\n    self.results_dir.mkdir(parents=True, exist_ok=True)\n\n    if self.cache_dir:\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n\n    # Event callbacks\n    self._on_task_start = on_task_start\n    self._on_task_end = on_task_end\n    self._on_progress = on_progress\n    self._on_error = on_error\n    self._on_metrics = on_metrics\n\n    # Internal: track the active model id for cleanup\n    self._active_model_id: Optional[str] = None\n    # Internal: validation behavior flag set per evaluate() call\n    self._strict_validation: bool = False\n\n    logger.info(\n        \"Initialized EvaluationHarness with \"\n        f\"tasks_dir={tasks_dir}, results_dir={results_dir}\"\n    )\n</code></pre>"},{"location":"api/reference/#bench.evaluation.harness.EvaluationHarness.close","title":"<code>close()</code>","text":"<p>Cleanup resources, e.g., unload the active model if loaded.</p> Source code in <code>bench/evaluation/harness.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Cleanup resources, e.g., unload the active model if loaded.\"\"\"\n    if self._active_model_id is not None:\n        try:\n            self.model_runner.unload_model(self._active_model_id)\n        finally:\n            self._active_model_id = None\n</code></pre>"},{"location":"api/reference/#bench.evaluation.harness.EvaluationHarness.evaluate","title":"<code>evaluate(model_id, task_ids, model_type='huggingface', batch_size=8, use_cache=True, save_results=True, strict_validation=False, report_formats=None, report_dir=None, **model_kwargs)</code>","text":"<p>Run evaluation on the specified model and tasks.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Identifier for the model to evaluate</p> required <code>task_ids</code> <code>List[str]</code> <p>List of task IDs to evaluate on</p> required <code>model_type</code> <code>str</code> <p>Type of model ('huggingface', 'local', 'api')</p> <code>'huggingface'</code> <code>batch_size</code> <code>int</code> <p>Batch size for model inference</p> <code>8</code> <code>use_cache</code> <code>bool</code> <p>Whether to use cached results if available</p> <code>True</code> <code>save_results</code> <code>bool</code> <p>Whether to save results to disk</p> <code>True</code> <code>**model_kwargs</code> <code>Any</code> <p>Additional arguments for model loading</p> <code>{}</code> <p>Returns:</p> Type Description <code>BenchmarkReport</code> <p>BenchmarkReport containing evaluation results</p> Source code in <code>bench/evaluation/harness.py</code> <pre><code>def evaluate(\n    self,\n    model_id: str,\n    task_ids: List[str],\n    model_type: str = \"huggingface\",\n    batch_size: int = 8,\n    use_cache: bool = True,\n    save_results: bool = True,\n    strict_validation: bool = False,\n    report_formats: Optional[List[str]] = None,\n    report_dir: Optional[str] = None,\n    **model_kwargs: Any,\n) -&gt; BenchmarkReport:\n    \"\"\"Run evaluation on the specified model and tasks.\n\n    Args:\n        model_id: Identifier for the model to evaluate\n        task_ids: List of task IDs to evaluate on\n        model_type: Type of model ('huggingface', 'local', 'api')\n        batch_size: Batch size for model inference\n        use_cache: Whether to use cached results if available\n        save_results: Whether to save results to disk\n        **model_kwargs: Additional arguments for model loading\n\n    Returns:\n        BenchmarkReport containing evaluation results\n    \"\"\"\n    # Generate a unique run ID\n    run_id = self._generate_run_id(model_id, task_ids)\n    logger.info(f\"Starting evaluation run {run_id}\")\n\n    # Initialize the model\n    logger.info(f\"Initializing model: {model_id} (type: {model_type})\")\n    # Load the model (modifies the model_runner's internal state)\n    self.model_runner.load_model(\n        model_name=model_id,  # Use model_name parameter\n        model_type=model_type,\n        **model_kwargs,\n    )\n    self._active_model_id = model_id\n    # Get the loaded model from the runner's _models dictionary\n    model = self.model_runner._models[model_id]\n\n    # Set validation mode for this run\n    self._strict_validation = bool(strict_validation)\n\n    # Initialize results\n    task_results = {}\n    start_time = time.time()\n\n    # Evaluate on each task\n    disable_tqdm = os.environ.get(\"MEDAISURE_NO_RICH\") == \"1\"\n    for idx, task_id in enumerate(\n        tqdm(task_ids, desc=\"Evaluating tasks\", disable=disable_tqdm), start=1\n    ):\n        task: Optional[MedicalTask] = None\n        try:\n            if self._on_progress:\n                self._on_progress(idx, len(task_ids), task_id)\n            if self._on_task_start:\n                self._on_task_start(task_id)\n\n            logger.info(f\"Evaluating on task: {task_id}\")\n\n            # Load task\n            task = self.task_loader.load_task(task_id)\n\n            # Check cache\n            cache_key = self._get_cache_key(run_id, task_id)\n            cached_result = self._load_from_cache(cache_key) if use_cache else None\n\n            if cached_result:\n                logger.info(f\"Using cached results for task {task_id}\")\n                task_result = cached_result\n            else:\n                # Run evaluation\n                task_result = self._evaluate_task(\n                    model=model,\n                    model_id=model_id,\n                    task=task,\n                    batch_size=batch_size,\n                )\n\n                # Cache results\n                if self.cache_dir:\n                    self._save_to_cache(cache_key, task_result)\n\n            # Store results\n            task_results[task_id] = task_result\n\n            # Call metrics hook\n            metrics_payload: Dict[str, Any] = {}\n            if hasattr(task_result, \"metrics_results\") and isinstance(\n                task_result.metrics_results, dict\n            ):\n                metrics_payload = task_result.metrics_results\n            elif (\n                hasattr(task_result, \"detailed_results\")\n                and task_result.detailed_results\n            ):\n                metrics_payload = task_result.detailed_results[0].metrics_results\n            if self._on_metrics and metrics_payload:\n                self._on_metrics(task_id, metrics_payload)\n\n            # Log progress\n            if metrics_payload:\n                logger.info(\n                    f\"Completed task {task_id} - \"\n                    f\"Metrics: {json.dumps(metrics_payload, indent=2)}\"\n                )\n            else:\n                logger.warning(f\"No detailed results available for task {task_id}\")\n\n            if self._on_task_end:\n                self._on_task_end(task_id, task_result)\n\n        except Exception as e:\n            # In strict validation mode, propagate exceptions to fail fast\n            if self._strict_validation:\n                raise\n            if self._on_error:\n                try:\n                    self._on_error(task_id, e)\n                except Exception:\n                    # Avoid callback exceptions breaking flow\n                    pass\n            logger.error(\n                f\"Error evaluating task {task_id}: {str(e)}\", exc_info=True\n            )\n            # Insert a placeholder result so the task key appears in the final report\n            try:\n                # Attempt to use the task's declared metrics for placeholder keys\n                metrics_dict: Dict[str, float] = {}\n                if task is not None and getattr(task, \"metrics\", None):\n                    names: List[str] = []\n                    for m in task.metrics:  # type: ignore[union-attr]\n                        if isinstance(m, dict) and \"name\" in m:\n                            names.append(str(m[\"name\"]))\n                        elif isinstance(m, str):\n                            names.append(m)\n                    metrics_dict = {name: 0.0 for name in names}\n                else:\n                    # Fallback if task isn't available\n                    metrics_dict = {\"score\": 0.0}\n\n                placeholder = EvaluationResult(\n                    model_id=model_id,\n                    task_id=task_id,\n                    inputs=[],\n                    model_outputs=[],\n                    metrics_results=metrics_dict,\n                    metadata={\n                        \"error\": str(e),\n                        \"note\": \"Placeholder result due to evaluation failure\",\n                    },\n                )\n                task_results[task_id] = placeholder\n                # Intentionally do NOT call on_task_end for error placeholders to\n                # maintain semantics expected by tests (only successful tasks end).\n            except Exception:\n                # As a last resort, skip adding results for this task\n                pass\n            # Continue with other tasks on error\n            continue\n\n    # Calculate total time\n    total_time = time.time() - start_time\n\n    # Build report via ResultAggregator/BenchmarkReport to get consistent aggregation\n    # Use the run_id for grouping\n    for task_id, res in task_results.items():\n        self.result_aggregator.add_evaluation_result(res, run_id=run_id)\n\n    # Try to get report from aggregator; if mocked or unavailable, fall back to manual construction\n    try:\n        report = self.result_aggregator.get_report(run_id)\n    except Exception:\n        report = None\n\n    if not isinstance(report, BenchmarkReport):\n        # Fallback: construct a minimal BenchmarkReport from task_results\n        task_scores: Dict[str, Dict[str, float]] = {}\n        detailed_results: List[EvaluationResult] = []\n\n        for t_id, res in task_results.items():\n            # Metrics may be in metrics_results or detailed_results[0].metrics_results\n            if hasattr(res, \"metrics_results\") and isinstance(\n                res.metrics_results, dict\n            ):\n                metrics = res.metrics_results\n            elif hasattr(res, \"detailed_results\") and res.detailed_results:\n                metrics = res.detailed_results[0].metrics_results\n            else:\n                metrics = {}\n\n            # Compute a simple average if numeric metrics exist; else 0.0\n            avg = 0.0\n            numeric_vals = [\n                float(v) for v in metrics.values() if isinstance(v, (int, float))\n            ]\n            if numeric_vals:\n                avg = sum(numeric_vals) / len(numeric_vals)\n            task_scores[t_id] = {\"average_score\": avg}\n\n            # Collect detailed results only if present (to preserve legacy placeholder behavior)\n            if hasattr(res, \"detailed_results\") and res.detailed_results:\n                detailed_results.extend(res.detailed_results)\n\n        overall_avg = 0.0\n        if task_scores:\n            overall_avg = sum(\n                s[\"average_score\"] for s in task_scores.values()\n            ) / len(task_scores)\n\n        # If no detailed results collected, add legacy placeholder result\n        if not detailed_results:\n            detailed_results = [\n                EvaluationResult(\n                    model_id=model_id,\n                    task_id=\"unknown\",\n                    inputs=[],\n                    model_outputs=[],\n                    metrics_results={\"score\": 0.0},\n                    timestamp=datetime.now(timezone.utc).isoformat(),\n                    metadata={\n                        \"note\": \"Placeholder result - no tasks completed successfully\"\n                    },\n                )\n            ]\n\n        report = BenchmarkReport(\n            model_id=model_id,\n            overall_scores={\"average_score\": overall_avg},\n            task_scores=task_scores,\n            detailed_results=detailed_results,\n            metadata={},\n        )\n    # Enrich metadata\n    report.metadata.update(\n        {\n            \"run_id\": run_id,\n            \"model_name\": model_id,\n            \"total_time_seconds\": total_time,\n            \"num_tasks\": len(task_results),\n            \"batch_size\": batch_size,\n            \"model_type\": model_type,\n            **model_kwargs,\n        }\n    )\n\n    # If any EvaluationResult captured validation issues, surface them at report level\n    try:\n        all_val_errors: list[str] = []\n        for res in report.detailed_results or []:\n            errs = []\n            meta = getattr(res, \"metadata\", {}) or {}\n            if isinstance(meta, dict):\n                errs = meta.get(\"validation_errors\", []) or []\n            if errs:\n                # Flatten to strings\n                for e in errs:\n                    all_val_errors.append(str(e))\n        if all_val_errors:\n            # De-duplicate while preserving order\n            seen = set()\n            deduped: list[str] = []\n            for e in all_val_errors:\n                if e not in seen:\n                    deduped.append(e)\n                    seen.add(e)\n            report.metadata[\"validation_errors\"] = deduped\n    except Exception:\n        # Best-effort enrichment; never block evaluation on metadata issues\n        pass\n\n    # Save results\n    try:\n        if save_results:\n            output_file = self.results_dir / f\"{run_id}.json\"\n            report.save(output_file)\n            logger.info(f\"Saved evaluation results to {output_file}\")\n            # Optionally export additional report formats via report generators\n            if report_formats:\n                from ..reports import ReportFactory  # lazy import\n\n                out_dir = Path(report_dir) if report_dir else self.results_dir\n                out_dir.mkdir(parents=True, exist_ok=True)\n                for fmt in report_formats:\n                    fmt_norm = str(fmt).lower().strip()\n                    try:\n                        gen = ReportFactory.create_generator(fmt_norm)\n                    except Exception as e:\n                        logger.warning(\n                            \"Skipping unknown report format '%s': %s\",\n                            fmt_norm,\n                            e,\n                        )\n                        continue\n                    content = gen.generate(report)\n                    # Choose extension\n                    ext = \"md\" if fmt_norm in {\"md\", \"markdown\"} else fmt_norm\n                    target = out_dir / f\"{run_id}.{ext}\"\n                    try:\n                        gen.save(content, target)\n                        logger.info(\"Exported %s report to %s\", fmt_norm, target)\n                    except Exception as e:\n                        logger.warning(\n                            \"Failed to export %s report to %s: %s\",\n                            fmt_norm,\n                            target,\n                            e,\n                        )\n        return report\n    finally:\n        # Always attempt cleanup\n        self.close()\n</code></pre>"},{"location":"api/reference/#bench.evaluation.harness.EvaluationHarness.get_task_info","title":"<code>get_task_info(task_id)</code>","text":"<p>Get detailed information about a specific task.</p> Source code in <code>bench/evaluation/harness.py</code> <pre><code>def get_task_info(self, task_id: str) -&gt; Dict[str, Any]:\n    \"\"\"Get detailed information about a specific task.\"\"\"\n    task = self.task_loader.load_task(task_id)\n\n    return {\n        \"task_id\": task.task_id,\n        \"name\": task.name,\n        \"description\": task.description,\n        \"input_schema\": task.input_schema,\n        \"output_schema\": task.output_schema,\n        \"metrics\": task.metrics,\n        \"num_examples\": len(task.dataset) if task.dataset else 0,\n        \"example_input\": task.dataset[0] if task.dataset else None,\n    }\n</code></pre>"},{"location":"api/reference/#bench.evaluation.harness.EvaluationHarness.list_available_tasks","title":"<code>list_available_tasks()</code>","text":"<p>List all available tasks in the tasks directory.</p> Source code in <code>bench/evaluation/harness.py</code> <pre><code>def list_available_tasks(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"List all available tasks in the tasks directory.\"\"\"\n    tasks = []\n\n    # Look for YAML task definitions\n    for task_file in self.tasks_dir.glob(\"*.yaml\"):\n        try:\n            with open(task_file, \"r\") as f:\n                task_data = yaml.safe_load(f)\n                tasks.append(\n                    {\n                        \"task_id\": task_file.stem,\n                        \"name\": task_data.get(\"name\", \"\"),\n                        \"description\": task_data.get(\"description\", \"\"),\n                        \"metrics\": [\n                            m[\"name\"] for m in task_data.get(\"metrics\", [])\n                        ],\n                        \"num_examples\": len(task_data.get(\"dataset\", [])),\n                        \"file\": str(task_file),\n                    }\n                )\n        except Exception as e:\n            logger.warning(f\"Error loading task from {task_file}: {str(e)}\")\n\n    # Look for JSON task definitions\n    for task_file in self.tasks_dir.glob(\"*.json\"):\n        if task_file.stem == \"tasks\":  # Skip the tasks index file\n            continue\n\n        try:\n            with open(task_file, \"r\") as f:\n                task_data = json.load(f)\n                tasks.append(\n                    {\n                        \"task_id\": task_file.stem,\n                        \"name\": task_data.get(\"name\", \"\"),\n                        \"description\": task_data.get(\"description\", \"\"),\n                        \"metrics\": [\n                            m[\"name\"] for m in task_data.get(\"metrics\", [])\n                        ],\n                        \"num_examples\": len(task_data.get(\"dataset\", [])),\n                        \"file\": str(task_file),\n                    }\n                )\n        except Exception as e:\n            logger.warning(f\"Error loading task from {task_file}: {str(e)}\")\n\n    return tasks\n</code></pre>"},{"location":"api/reference/#model-runner","title":"Model Runner","text":""},{"location":"api/reference/#bench.evaluation.model_runner","title":"<code>bench.evaluation.model_runner</code>","text":"<p>Model runner for MedAISure benchmark.</p>"},{"location":"api/reference/#bench.evaluation.model_runner.ModelRunner","title":"<code>ModelRunner</code>","text":"<p>               Bases: <code>Generic[M, T, R]</code></p> <p>Handles loading and running different types of models.</p> <p>Supports three model sources: HuggingFace pipelines, local Python modules, and simple HTTP API-backed models. Caches loaded models and their configs.</p> <p>Parameters:</p> Name Type Description Default <code>M</code> <p>Type variable for the model class</p> required <code>T</code> <p>Type variable for input data</p> required <code>R</code> <p>Type variable for result data</p> required <p>Example \u2013 quick start:     &gt;&gt;&gt; from bench.evaluation.model_runner import ModelRunner     &gt;&gt;&gt; mr = ModelRunner()     &gt;&gt;&gt; # Load a HuggingFace pipeline (requires transformers, may download weights)     &gt;&gt;&gt; # _ = mr.load_model(\"hf-cls\", model_type=\"huggingface\", model_path=\"prajjwal1/bert-tiny\", hf_task=\"text-classification\")  # doctest: +SKIP     &gt;&gt;&gt; # Run on small inputs     &gt;&gt;&gt; # preds = mr.run_model(\"hf-cls\", inputs=[{\"text\": \"Patient is stable.\"}])  # doctest: +SKIP     &gt;&gt;&gt; isinstance(mr, ModelRunner)     True</p> Source code in <code>bench/evaluation/model_runner.py</code> <pre><code>class ModelRunner(Generic[M, T, R]):\n    \"\"\"Handles loading and running different types of models.\n\n    Supports three model sources: HuggingFace pipelines, local Python modules,\n    and simple HTTP API-backed models. Caches loaded models and their configs.\n\n    Args:\n        M: Type variable for the model class\n        T: Type variable for input data\n        R: Type variable for result data\n\n    Example \u2013 quick start:\n        &gt;&gt;&gt; from bench.evaluation.model_runner import ModelRunner\n        &gt;&gt;&gt; mr = ModelRunner()\n        &gt;&gt;&gt; # Load a HuggingFace pipeline (requires transformers, may download weights)\n        &gt;&gt;&gt; # _ = mr.load_model(\"hf-cls\", model_type=\"huggingface\", model_path=\"prajjwal1/bert-tiny\", hf_task=\"text-classification\")  # doctest: +SKIP\n        &gt;&gt;&gt; # Run on small inputs\n        &gt;&gt;&gt; # preds = mr.run_model(\"hf-cls\", inputs=[{\"text\": \"Patient is stable.\"}])  # doctest: +SKIP\n        &gt;&gt;&gt; isinstance(mr, ModelRunner)\n        True\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the model runner with empty model and tokenizer caches.\"\"\"\n        self._models: Dict[str, Any] = {}\n        self._model_configs: Dict[str, Dict[str, Any]] = {}\n        self._tokenizers: Dict[str, Any] = {}\n\n    def register_interface_model(self, model: \"ModelInterface\") -&gt; None:\n        \"\"\"Register a model implementing the ModelInterface.\n\n        This allows external callers to construct rich model wrappers and plug\n        them into the runner for use with run_model().\n\n        Args:\n            model: An instance implementing ModelInterface.\n        \"\"\"\n        # Store object and minimal config\n        self._models[model.model_id] = model\n        self._model_configs[model.model_id] = {\n            \"type\": \"interface\",\n        }\n        logger.info(f\"Registered interface model: {model.model_id}\")\n\n    def load_model(\n        self,\n        model_name: str,\n        model_type: str = \"local\",\n        model_path: Optional[Union[str, Path]] = None,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Load a model for inference.\n\n        Args:\n            model_name: Unique name for the model.\n            model_type: Type of model (huggingface, local, api).\n            model_path: Path to the model or module.\n            **kwargs: Additional arguments for model loading.\n\n        Returns:\n            The loaded model object.\n\n        Raises:\n            ValueError: If the model type is unsupported or required\n                arguments are missing.\n\n        Examples:\n            HuggingFace (pipeline):\n                &gt;&gt;&gt; mr = ModelRunner()\n                &gt;&gt;&gt; # Tiny model to keep downloads small; requires transformers\n                &gt;&gt;&gt; # pipe = mr.load_model(\n                ... #     \"hf-sum\",\n                ... #     model_type=\"huggingface\",\n                ... #     model_path=\"sshleifer/tiny-t5\",\n                ... #     hf_task=\"summarization\",\n                ... # )  # doctest: +SKIP\n\n            Local module:\n                &gt;&gt;&gt; # Suppose mypkg.model has a load_model(path) -&gt; callable\n                &gt;&gt;&gt; mr = ModelRunner()\n                &gt;&gt;&gt; # model = mr.load_model(\n                ... #     \"local-demo\",\n                ... #     model_type=\"local\",\n                ... #     model_path=\"/tmp/model.bin\",\n                ... #     module_path=\"bench.examples.mypkg.model\",\n                ... # )  # doctest: +SKIP\n\n            API model registration:\n                &gt;&gt;&gt; mr = ModelRunner()\n                &gt;&gt;&gt; cfg = mr.load_model(\n                ...     \"api-demo\", model_type=\"api\", endpoint=\"https://api.example/v1/predict\", api_key=\"sk-xxx\"\n                ... )\n                &gt;&gt;&gt; isinstance(cfg, dict)\n                True\n        \"\"\"\n        if model_name in self._models:\n            logger.warning(f\"Model {model_name} is already loaded. Unload it first.\")\n            return self._models[model_name]\n\n        model_path_str = str(model_path) if model_path is not None else None\n        model = None\n\n        if model_type == \"huggingface\":\n            model = self._load_huggingface_model(model_name, model_path_str, **kwargs)\n        elif model_type == \"local\":\n            model = self._load_local_model(model_name, model_path_str, **kwargs)\n        elif model_type == \"api\":\n            model = self._load_api_model(model_name, **kwargs)\n        elif model_type == \"interface\":\n            # Expect a concrete ModelInterface instance passed as 'model'\n            mi = kwargs.get(\"model\")\n            if mi is None:\n                raise ValueError(\n                    \"For model_type='interface', pass a 'model' instance implementing ModelInterface\"\n                )\n            # Delay import for typing only to avoid hard dependency here\n            self.register_interface_model(mi)\n            model = mi\n        else:\n            raise ValueError(f\"Unsupported model type: {model_type}\")\n\n        return model\n\n    def _load_huggingface_model(\n        self, model_name: str, model_path: Optional[str] = None, **kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Load a HuggingFace model.\n\n        Args:\n            model_name: Name to register the model under.\n            model_path: Path to the model or model name from HuggingFace Hub.\n                      If None, model_name will be used as the model identifier.\n            **kwargs: Additional arguments for model loading.\n\n        Returns:\n            The loaded HuggingFace pipeline.\n\n        Raises:\n            ImportError: If transformers is not installed.\n            ValueError: If model loading fails.\n        \"\"\"\n        model_type = \"huggingface\"\n        try:\n            from transformers import (\n                AutoTokenizer,\n                pipeline,\n            )\n\n            # Use model_name as model identifier if model_path is not provided\n            model_identifier = model_path if model_path else model_name\n            model_kwargs = kwargs.get(\"model_kwargs\", {})\n            tokenizer_kwargs = kwargs.get(\"tokenizer_kwargs\", {})\n            generation_kwargs = kwargs.get(\"generation_kwargs\", {})\n            num_labels = kwargs.get(\"num_labels\")\n            hf_task = kwargs.get(\"hf_task\", kwargs.get(\"task\", \"text-classification\"))\n            # Normalize common aliases\n            if hf_task == \"text2text-generation\":\n                hf_task = \"summarization\"\n\n            logger.info(f\"Loading {model_type} model: {model_identifier}\")\n\n            # Select appropriate AutoModel depending on task\n            AutoModel = None\n            tokenizer = None\n            loaded_model = None\n            # Optional advanced settings\n            trust_remote_code = bool(kwargs.get(\"trust_remote_code\", False))\n            device = kwargs.get(\"device\", -1)\n            device_map = kwargs.get(\"device_map\")\n            torch_dtype = kwargs.get(\"torch_dtype\")  # e.g., \"auto\", \"float16\"\n            low_cpu_mem_usage = kwargs.get(\"low_cpu_mem_usage\")\n            revision = kwargs.get(\"revision\")\n            if hf_task == \"text-classification\":\n                from transformers import (\n                    AutoModelForSequenceClassification as _AutoModel,\n                )  # type: ignore\n\n                AutoModel = _AutoModel\n            elif hf_task in {\"summarization\"}:\n                try:\n                    from transformers import AutoModelForSeq2SeqLM as _AutoModel  # type: ignore\n\n                    AutoModel = _AutoModel\n                except Exception:\n                    AutoModel = None\n            elif hf_task in {\"text-generation\"}:\n                try:\n                    from transformers import AutoModelForCausalLM as _AutoModel  # type: ignore\n\n                    AutoModel = _AutoModel\n                except Exception:\n                    AutoModel = None\n\n            # Load model/tokenizer if available; otherwise rely on pipeline to resolve by id\n            if AutoModel is not None:\n                # Prepare from_pretrained kwargs\n                fp_kwargs = {\n                    **({\"num_labels\": num_labels} if num_labels is not None else {}),\n                    **model_kwargs,\n                }\n                if trust_remote_code:\n                    fp_kwargs[\"trust_remote_code\"] = True\n                if device_map is not None:\n                    fp_kwargs[\"device_map\"] = device_map\n                if torch_dtype is not None:\n                    fp_kwargs[\"torch_dtype\"] = torch_dtype\n                if low_cpu_mem_usage is not None:\n                    fp_kwargs[\"low_cpu_mem_usage\"] = low_cpu_mem_usage\n                if revision is not None:\n                    fp_kwargs[\"revision\"] = revision\n\n                loaded_model = AutoModel.from_pretrained(\n                    model_identifier,\n                    **fp_kwargs,\n                )\n                try:\n                    # Disable gradients and set eval mode to reduce overhead\n                    import torch  # type: ignore\n\n                    torch.set_grad_enabled(False)\n                except Exception:\n                    pass\n                try:\n                    loaded_model.eval()  # type: ignore[attr-defined]\n                except Exception:\n                    pass\n\n                tokenizer = AutoTokenizer.from_pretrained(\n                    model_identifier, **tokenizer_kwargs\n                )\n                pipe = pipeline(\n                    hf_task,\n                    model=loaded_model,\n                    tokenizer=tokenizer,\n                    device=device,\n                    **kwargs.get(\"pipeline_kwargs\", {}),\n                )\n            else:\n                pipe = pipeline(\n                    hf_task,\n                    model=model_identifier,\n                    device=device,\n                    **kwargs.get(\"pipeline_kwargs\", {}),\n                )\n\n            # Store the pipeline and its components\n            self._models[model_name] = pipe\n            self._tokenizers[model_name] = tokenizer\n\n            # Build metadata/config snapshot\n            meta: Dict[str, Any] = {\n                \"type\": model_type,\n                \"path\": model_identifier,\n                \"hf_task\": hf_task,\n                \"generation_kwargs\": generation_kwargs,\n                \"device\": device,\n                \"device_map\": device_map,\n                \"torch_dtype\": str(torch_dtype) if torch_dtype is not None else None,\n                \"trust_remote_code\": trust_remote_code,\n                **kwargs,\n            }\n            try:\n                model_ref = getattr(pipe, \"model\", None)\n                if model_ref is not None and hasattr(model_ref, \"config\"):\n                    cfg = model_ref.config\n                    meta.update(\n                        {\n                            \"model_type_name\": getattr(cfg, \"model_type\", None),\n                            \"vocab_size\": getattr(cfg, \"vocab_size\", None),\n                            \"max_position_embeddings\": getattr(\n                                cfg, \"max_position_embeddings\", None\n                            ),\n                            \"num_hidden_layers\": getattr(\n                                cfg, \"num_hidden_layers\", None\n                            ),\n                            \"hidden_size\": getattr(cfg, \"hidden_size\", None),\n                            \"id2label\": getattr(cfg, \"id2label\", None),\n                            \"label2id\": getattr(cfg, \"label2id\", None),\n                        }\n                    )\n                # Number of parameters\n                if loaded_model is None:\n                    loaded_model = getattr(pipe, \"model\", None)\n                if loaded_model is not None:\n                    try:\n                        total_params = sum(p.numel() for p in loaded_model.parameters())  # type: ignore[attr-defined]\n                        meta[\"num_parameters\"] = int(total_params)\n                        meta[\"dtype\"] = str(next(loaded_model.parameters()).dtype)  # type: ignore[attr-defined]\n                        # Device best-effort\n                        try:\n                            meta[\"model_device\"] = str(\n                                next(loaded_model.parameters()).device\n                            )  # type: ignore[attr-defined]\n                        except Exception:\n                            pass\n                    except Exception:\n                        pass\n            except Exception:\n                # Best-effort metadata extraction\n                pass\n\n            self._model_configs[model_name] = meta\n\n            logger.info(\n                f\"Successfully loaded {model_type} model: {model_name} \"\n                f\"from {model_identifier}\"\n            )\n            return pipe\n\n        except ImportError as e:\n            raise ImportError(\n                \"transformers library is required for HuggingFace models. \"\n                \"Install with: pip install transformers torch\"\n            ) from e\n        except Exception as e:\n            logger.error(\n                f\"Error loading model {model_identifier} with error: {str(e)}\",\n                exc_info=True,\n            )\n            raise ValueError(\n                f\"Failed to load HuggingFace model '{model_identifier}': {str(e)}\"\n            ) from e\n\n    def unload_model(self, model_name: str) -&gt; None:\n        \"\"\"Unload a model and clean up resources.\n\n        Args:\n            model_name: Name of the model to unload.\n\n        Example:\n            &gt;&gt;&gt; mr = ModelRunner()\n            &gt;&gt;&gt; _ = mr.load_model(\"api-x\", model_type=\"api\", endpoint=\"https://api.example\", api_key=\"k\")\n            &gt;&gt;&gt; mr.unload_model(\"api-x\")\n            &gt;&gt;&gt; # No exception means success\n        \"\"\"\n        if model_name in self._models:\n            # Best-effort cleanup of torch resources\n            try:\n                model = self._models.get(model_name)\n                model_ref = getattr(model, \"model\", None)\n                if model_ref is not None and hasattr(model_ref, \"cpu\"):\n                    model_ref.cpu()\n                # Empty CUDA cache if available\n                try:\n                    import torch  # type: ignore\n\n                    if torch.cuda.is_available():  # pragma: no cover - env-dependent\n                        torch.cuda.empty_cache()\n                except Exception:\n                    pass\n            except Exception:\n                pass\n            del self._models[model_name]\n        if model_name in self._model_configs:\n            del self._model_configs[model_name]\n        if model_name in self._tokenizers:\n            del self._tokenizers[model_name]\n        logger.info(f\"Unloaded model: {model_name}\")\n\n    def _load_local_model(\n        self, model_name: str, model_path: Optional[str] = None, **kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Load a local model from a Python module.\n\n        Args:\n            model_name: Name to register the model under.\n            model_path: Path to the model or directory.\n            **kwargs: Additional arguments for model loading.\n                - module_path: Required. Python import path to the module.\n                - load_func: Optional. Name of the load function (default: 'load_model')\n\n        Returns:\n            The loaded model object.\n\n        Raises:\n            ImportError: If the module cannot be imported.\n            ValueError: If model_path or module_path is not provided or invalid.\n\n        Example:\n            &gt;&gt;&gt; mr = ModelRunner()\n            &gt;&gt;&gt; # model = mr._load_local_model(\n            ... #     \"demo\", model_path=\"/tmp/m.bin\", module_path=\"bench.examples.mypkg.model\"\n            ... # )  # doctest: +SKIP\n        \"\"\"\n        if not model_path:\n            raise ValueError(\"model_path is required for local models\")\n\n        module_path = kwargs.pop(\"module_path\", None)\n        if not module_path:\n            raise ValueError(\"module_path is required for local models\")\n\n        load_func_name = kwargs.pop(\"load_func\", \"load_model\")\n\n        try:\n            # Import the module\n            module = importlib.import_module(module_path)\n\n            # Get the load function\n            load_func = getattr(module, load_func_name, None)\n            if load_func is None or not callable(load_func):\n                raise ValueError(\n                    f\"Module {module_path} has no callable '{load_func_name}' function\"\n                )\n\n            # Load the model\n            model = load_func(model_path, **kwargs)\n\n            # Store the model and its config\n            self._models[model_name] = model\n            self._model_configs[model_name] = {\n                \"type\": \"local\",\n                \"path\": model_path,\n                \"module\": module_path,\n                \"load_func\": load_func_name,\n                **kwargs,\n            }\n            logger.info(f\"Loaded local model {model_name} from {module_path}\")\n\n            return model\n\n        except ImportError as e:\n            raise ImportError(\n                f\"Failed to import model module {module_path}: {e}\"\n            ) from e\n\n    def _load_api_model(self, model_name: str, **kwargs: Any) -&gt; Dict[str, Any]:\n        \"\"\"Register an API-based model.\n\n        Args:\n            model_name: Name to register the model under.\n            **kwargs: Must include 'api_key' and 'endpoint'. Optional:\n                - timeout (float): request timeout in seconds (default: 30.0)\n                - max_retries (int): number of retries on request failure (default: 0)\n                - backoff_factor (float): base backoff seconds\n                between retries (default: 0.5)\n                - headers (dict): additional headers\n\n        Returns:\n            The model configuration dictionary.\n\n        Raises:\n            ValueError: If required arguments are missing.\n\n        Example:\n            &gt;&gt;&gt; mr = ModelRunner()\n            &gt;&gt;&gt; cfg = mr._load_api_model(\"api-a\", endpoint=\"https://api.example\", api_key=\"k\")\n            &gt;&gt;&gt; set(cfg.keys()) &gt;= {\"type\", \"endpoint\", \"api_key\"}\n            True\n        \"\"\"\n        if \"api_key\" not in kwargs:\n            raise ValueError(\"api_key is required for API models\")\n        if \"endpoint\" not in kwargs:\n            raise ValueError(\"endpoint is required for API models\")\n\n        # Store the API configuration\n        model_config = {\n            \"type\": \"api\",\n            \"endpoint\": kwargs[\"endpoint\"],\n            \"api_key\": kwargs[\"api_key\"],\n            \"headers\": kwargs.get(\"headers\", {}),\n            \"timeout\": float(kwargs.get(\"timeout\", 30.0)),\n            \"max_retries\": int(kwargs.get(\"max_retries\", 0)),\n            \"backoff_factor\": float(kwargs.get(\"backoff_factor\", 0.5)),\n        }\n\n        # Store the model config\n        self._model_configs[model_name] = model_config\n\n        # Create a simple callable that will make the API request\n        def api_call(inputs: List[Dict[str, Any]]) -&gt; List[Dict[str, Any]]:\n            return self._call_api_model(model_config, inputs)\n\n        self._models[model_name] = api_call\n        logger.info(f\"Registered API model: {model_name} ({kwargs['endpoint']})\")\n\n        return model_config\n\n    def _create_api_callable(\n        self, model_name: str, **kwargs: Any\n    ) -&gt; Callable[[List[Dict[str, Any]]], List[Dict[str, Any]]]:\n        \"\"\"Create a callable that handles API requests.\n\n        Args:\n            model_name: Name of the model.\n            **kwargs: Configuration for the API call.\n\n        Returns:\n            A function that takes a list of inputs and returns model predictions.\n        \"\"\"\n\n        import requests  # type: ignore[import-untyped]\n\n        endpoint: str = kwargs[\"endpoint\"]\n        headers: Dict[str, str] = kwargs.get(\"headers\", {})\n        request_format: Callable[[List[Dict[str, Any]]], Any] = kwargs[\"request_format\"]\n        response_parser: Callable[[Any], List[Dict[str, Any]]] = kwargs[\n            \"response_parser\"\n        ]\n\n        def api_call(inputs: List[Dict[str, Any]]) -&gt; List[Dict[str, Any]]:\n            \"\"\"Make an API call with the given inputs.\"\"\"\n            try:\n                # Format the request\n                payload = request_format(inputs)\n\n                # Make the request\n                response = requests.post(endpoint, json=payload, headers=headers)\n                response.raise_for_status()\n\n                # Parse the response\n                response_data = response.json()\n                return response_parser(response_data)\n\n            except requests.exceptions.RequestException as e:\n                logger.error(f\"API request failed for model {model_name}: {e}\")\n                # Return empty list on error to maintain consistent return type\n                return []\n\n        return api_call\n\n    def run_model(\n        self,\n        model_id: str,\n        inputs: List[Dict[str, Any]],\n        batch_size: int = 8,\n        **kwargs: Any,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Run inference on a batch of inputs.\n\n        Args:\n            model_id: ID of the loaded model.\n            inputs: List of input dictionaries.\n            batch_size: Number of inputs to process in a batch.\n            **kwargs: Additional arguments for model inference.\n\n        Returns:\n            List of model outputs with 'label' and 'score' keys.\n\n        Raises:\n            ValueError: If model_id is not found or inputs are invalid.\n\n        Examples:\n            API model:\n                &gt;&gt;&gt; mr = ModelRunner()\n                &gt;&gt;&gt; _ = mr.load_model(\"api-x\", model_type=\"api\", endpoint=\"https://api.example\", api_key=\"k\")\n                &gt;&gt;&gt; out = mr.run_model(\"api-x\", inputs=[{\"text\": \"abc\"}], batch_size=1)\n                &gt;&gt;&gt; isinstance(out, list)\n                True\n\n            HuggingFace (summarization):\n                &gt;&gt;&gt; mr = ModelRunner()\n                &gt;&gt;&gt; # _ = mr.load_model(\"hf-sum\", model_type=\"huggingface\", model_path=\"sshleifer/tiny-t5\", hf_task=\"summarization\")  # doctest: +SKIP\n                &gt;&gt;&gt; # out = mr.run_model(\"hf-sum\", inputs=[{\"document\": \"Short note.\"}], batch_size=1)  # doctest: +SKIP\n        \"\"\"\n        if model_id not in self._models:\n            raise ValueError(f\"Model {model_id} not loaded. Call load_model() first.\")\n\n        if not isinstance(inputs, list) or not all(\n            isinstance(item, dict) for item in inputs\n        ):\n            raise ValueError(\"Inputs must be a list of dictionaries.\")\n\n        # Get model type to handle different model types appropriately\n        model_type = self._model_configs[model_id].get(\"type\", \"unknown\")\n        model = self._models[model_id]\n        results: List[Dict[str, Any]] = []\n\n        # Get label map from config if available\n        label_map = self._model_configs[model_id].get(\"label_map\", {})\n\n        # Process inputs in batches\n        for i in range(0, len(inputs), batch_size):\n            batch = inputs[i : i + batch_size]\n\n            try:\n                batch_start = time.time()\n                if model_type == \"huggingface\":\n                    # For HuggingFace pipeline\n                    hf_task = self._model_configs.get(model_id, {}).get(\n                        \"hf_task\", \"text-classification\"\n                    )\n                    gen_kwargs = self._model_configs.get(model_id, {}).get(\n                        \"generation_kwargs\", {}\n                    )\n                    # Select input text field based on task type\n                    if hf_task == \"summarization\":\n                        # Prefer common clinical note fields\n                        def _extract_text(d: Dict[str, Any]) -&gt; str:\n                            for key in (\"document\", \"text\", \"note\"):\n                                v = d.get(key)\n                                if isinstance(v, str) and v.strip():\n                                    return v\n                            # Fallback to stringified input\n                            return str(d)\n\n                        texts = [_extract_text(item) for item in batch]\n                    else:\n                        texts = [item.get(\"text\", \"\") for item in batch]\n\n                    # Pass generation kwargs for generative tasks\n                    call_kwargs: Dict[str, Any] = {}\n                    if hf_task in {\"summarization\", \"text-generation\"} and isinstance(\n                        gen_kwargs, dict\n                    ):\n                        call_kwargs = gen_kwargs\n                    batch_results = model(texts, **call_kwargs)\n                    if not isinstance(batch_results, list):\n                        batch_results = [batch_results]\n\n                    for item, result in zip(batch, batch_results):\n                        # HF may return list of candidates per input\n                        if isinstance(result, list) and result:\n                            result = result[0]\n\n                        if hf_task == \"text-classification\":\n                            raw_label = str(result.get(\"label\", \"\"))\n                            score = float(result.get(\"score\", 0.0))\n                            predicted_label = label_map.get(raw_label, raw_label)\n                            if (\n                                predicted_label.startswith(\"LABEL_\")\n                                and \"_\" in predicted_label\n                            ):\n                                try:\n                                    idx = int(predicted_label.split(\"_\")[1])\n                                    if (\n                                        not label_map\n                                        and hasattr(model, \"model\")\n                                        and hasattr(model.model, \"config\")\n                                    ):\n                                        if hasattr(model.model.config, \"id2label\"):\n                                            label_map = model.model.config.id2label\n                                            predicted_label = label_map.get(\n                                                idx, predicted_label\n                                            )\n                                except (ValueError, IndexError):\n                                    pass\n                            results.append(\n                                {\n                                    \"input\": item,\n                                    \"label\": predicted_label,\n                                    \"score\": score,\n                                    \"raw_label\": raw_label,\n                                }\n                            )\n                        elif hf_task == \"summarization\":\n                            summary = (\n                                result.get(\"summary_text\", \"\")\n                                if isinstance(result, dict)\n                                else str(result)\n                            )\n                            results.append(\n                                {\n                                    \"input\": item,\n                                    \"summary\": summary,\n                                    \"prediction\": summary,\n                                }\n                            )\n                        elif hf_task == \"text-generation\":\n                            gen = \"\"\n                            if isinstance(result, dict):\n                                gen = result.get(\"generated_text\", \"\")\n                            elif (\n                                isinstance(result, list)\n                                and result\n                                and isinstance(result[0], dict)\n                            ):\n                                gen = result[0].get(\"generated_text\", \"\")\n                            else:\n                                gen = str(result)\n                            results.append(\n                                {\n                                    \"input\": item,\n                                    \"text\": gen,\n                                    \"prediction\": gen,\n                                }\n                            )\n                        else:\n                            # Fallback: store as raw text\n                            results.append(\n                                {\n                                    \"input\": item,\n                                    \"text\": str(result),\n                                    \"prediction\": str(result),\n                                }\n                            )\n\n                elif model_type == \"local\":\n                    # For local models that implement __call__\n                    batch_results = model(batch, **kwargs)\n                    if not isinstance(batch_results, list):\n                        batch_results = [batch_results]\n                    results.extend(batch_results)\n\n                elif model_type == \"api\":\n                    # For API models that implement __call__\n                    batch_results = model(batch)\n                    if not isinstance(batch_results, list):\n                        batch_results = [batch_results]\n                    results.extend(batch_results)\n\n                elif model_type == \"interface\":\n                    # For ModelInterface implementations\n                    # We only rely on the standard predict() batch method\n                    batch_results = model.predict(batch)\n                    if not isinstance(batch_results, list):\n                        batch_results = [batch_results]\n                    results.extend(batch_results)\n\n                else:\n                    raise ValueError(f\"Unsupported model type: {model_type}\")\n\n            except Exception as e:\n                logger.error(\n                    f\"Error running model {model_id} on batch: {e}\", exc_info=True\n                )\n                # Add empty dicts for failed predictions to maintain order and type\n                results.extend([{} for _ in batch])\n            finally:\n                try:\n                    latency = time.time() - batch_start\n                    logger.debug(\n                        \"Model batch completed\",\n                        extra={\n                            \"model_id\": model_id,\n                            \"model_type\": model_type,\n                            \"batch_size\": len(batch),\n                            \"latency_sec\": round(latency, 6),\n                        },\n                    )\n                except Exception:\n                    # Best-effort logging only\n                    pass\n\n        return results\n\n    def _call_api_model(\n        self, model_config: Dict[str, Any], batch: List[Dict[str, Any]], **kwargs: Any\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Helper method to call API-based models.\n\n        Args:\n            model_config: Configuration dictionary for the API model.\n            batch: List of input dictionaries to process.\n            **kwargs: Additional keyword arguments for the API call.\n\n        Returns:\n            List of model predictions as dictionaries with 'label' and 'score' keys.\n        \"\"\"\n        import requests\n\n        # Get the endpoint and headers from config\n        endpoint = model_config[\"endpoint\"]\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {model_config['api_key']}\",\n            **model_config.get(\"headers\", {}),\n        }\n\n        timeout = float(model_config.get(\"timeout\", 30.0))\n        max_retries = int(model_config.get(\"max_retries\", 0))\n        backoff = float(model_config.get(\"backoff_factor\", 0.5))\n\n        attempt = 0\n        while True:\n            attempt += 1\n            start = time.time()\n            try:\n                # Make the API request\n                response = requests.post(\n                    url=endpoint, headers=headers, json=batch, timeout=timeout, **kwargs\n                )\n                # Check for errors\n                response.raise_for_status()\n                latency = time.time() - start\n                logger.debug(\n                    \"API call succeeded\",\n                    extra={\n                        \"endpoint\": endpoint,\n                        \"batch_size\": len(batch),\n                        \"status_code\": response.status_code,\n                        \"latency_sec\": round(latency, 6),\n                        \"attempt\": attempt,\n                    },\n                )\n                # Parse and return the response\n                result = response.json()\n                break\n            except requests.exceptions.RequestException as e:\n                logger.warning(\n                    \"API request failed for model %s: %s. Attempt %s/%s\",\n                    model_config,\n                    e,\n                    attempt,\n                    max_retries + 1,\n                )\n                if attempt &gt; max_retries:\n                    # Return empty list on error to maintain consistent return type\n                    return []\n                # Exponential backoff\n                sleep_s = backoff * (2 ** (attempt - 1))\n                time.sleep(sleep_s)\n\n        # Ensure we return a list of results with the expected structure\n        if not isinstance(result, list):\n            result = [result]\n\n        # Convert each item to the expected dictionary format\n        formatted_results = []\n        for item in result:\n            if isinstance(item, dict):\n                # Ensure the dict has the expected keys\n                if \"label\" not in item or \"score\" not in item:\n                    formatted_results.append(\n                        {\"label\": str(item), \"score\": float(item.get(\"score\", 1.0))}\n                    )\n                else:\n                    formatted_results.append(item)\n            else:\n                # Convert non-dict results to the expected format\n                formatted_results.append({\"label\": str(item), \"score\": 1.0})\n\n        return formatted_results\n\n    async def run_model_async(\n        self,\n        model_id: str,\n        inputs: List[Dict[str, Any]],\n        batch_size: int = 8,\n        **kwargs: Any,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Asynchronous wrapper around run_model using a thread executor.\n\n        This avoids adding async HTTP dependencies while providing an async API.\n\n        Example:\n            &gt;&gt;&gt; import asyncio\n            &gt;&gt;&gt; mr = ModelRunner()\n            &gt;&gt;&gt; _ = mr.load_model(\"api-y\", model_type=\"api\", endpoint=\"https://api.example\", api_key=\"k\")\n            &gt;&gt;&gt; async def _go():\n            ...     return await mr.run_model_async(\"api-y\", inputs=[{\"text\": \"abc\"}], batch_size=1)\n            &gt;&gt;&gt; # asyncio.run(_go())  # doctest: +SKIP\n        \"\"\"\n        import asyncio\n\n        loop = asyncio.get_running_loop()\n        return await loop.run_in_executor(\n            None,\n            lambda: self.run_model(model_id, inputs, batch_size=batch_size, **kwargs),\n        )\n</code></pre>"},{"location":"api/reference/#bench.evaluation.model_runner.ModelRunner.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the model runner with empty model and tokenizer caches.</p> Source code in <code>bench/evaluation/model_runner.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the model runner with empty model and tokenizer caches.\"\"\"\n    self._models: Dict[str, Any] = {}\n    self._model_configs: Dict[str, Dict[str, Any]] = {}\n    self._tokenizers: Dict[str, Any] = {}\n</code></pre>"},{"location":"api/reference/#bench.evaluation.model_runner.ModelRunner.load_model","title":"<code>load_model(model_name, model_type='local', model_path=None, **kwargs)</code>","text":"<p>Load a model for inference.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Unique name for the model.</p> required <code>model_type</code> <code>str</code> <p>Type of model (huggingface, local, api).</p> <code>'local'</code> <code>model_path</code> <code>Optional[Union[str, Path]]</code> <p>Path to the model or module.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for model loading.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The loaded model object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model type is unsupported or required arguments are missing.</p> <p>Examples:</p> <p>HuggingFace (pipeline):     &gt;&gt;&gt; mr = ModelRunner()     &gt;&gt;&gt; # Tiny model to keep downloads small; requires transformers     &gt;&gt;&gt; # pipe = mr.load_model(     ... #     \"hf-sum\",     ... #     model_type=\"huggingface\",     ... #     model_path=\"sshleifer/tiny-t5\",     ... #     hf_task=\"summarization\",     ... # )  # doctest: +SKIP</p> <p>Local module:     &gt;&gt;&gt; # Suppose mypkg.model has a load_model(path) -&gt; callable     &gt;&gt;&gt; mr = ModelRunner()     &gt;&gt;&gt; # model = mr.load_model(     ... #     \"local-demo\",     ... #     model_type=\"local\",     ... #     model_path=\"/tmp/model.bin\",     ... #     module_path=\"bench.examples.mypkg.model\",     ... # )  # doctest: +SKIP</p> <p>API model registration:     &gt;&gt;&gt; mr = ModelRunner()     &gt;&gt;&gt; cfg = mr.load_model(     ...     \"api-demo\", model_type=\"api\", endpoint=\"https://api.example/v1/predict\", api_key=\"sk-xxx\"     ... )     &gt;&gt;&gt; isinstance(cfg, dict)     True</p> Source code in <code>bench/evaluation/model_runner.py</code> <pre><code>def load_model(\n    self,\n    model_name: str,\n    model_type: str = \"local\",\n    model_path: Optional[Union[str, Path]] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Load a model for inference.\n\n    Args:\n        model_name: Unique name for the model.\n        model_type: Type of model (huggingface, local, api).\n        model_path: Path to the model or module.\n        **kwargs: Additional arguments for model loading.\n\n    Returns:\n        The loaded model object.\n\n    Raises:\n        ValueError: If the model type is unsupported or required\n            arguments are missing.\n\n    Examples:\n        HuggingFace (pipeline):\n            &gt;&gt;&gt; mr = ModelRunner()\n            &gt;&gt;&gt; # Tiny model to keep downloads small; requires transformers\n            &gt;&gt;&gt; # pipe = mr.load_model(\n            ... #     \"hf-sum\",\n            ... #     model_type=\"huggingface\",\n            ... #     model_path=\"sshleifer/tiny-t5\",\n            ... #     hf_task=\"summarization\",\n            ... # )  # doctest: +SKIP\n\n        Local module:\n            &gt;&gt;&gt; # Suppose mypkg.model has a load_model(path) -&gt; callable\n            &gt;&gt;&gt; mr = ModelRunner()\n            &gt;&gt;&gt; # model = mr.load_model(\n            ... #     \"local-demo\",\n            ... #     model_type=\"local\",\n            ... #     model_path=\"/tmp/model.bin\",\n            ... #     module_path=\"bench.examples.mypkg.model\",\n            ... # )  # doctest: +SKIP\n\n        API model registration:\n            &gt;&gt;&gt; mr = ModelRunner()\n            &gt;&gt;&gt; cfg = mr.load_model(\n            ...     \"api-demo\", model_type=\"api\", endpoint=\"https://api.example/v1/predict\", api_key=\"sk-xxx\"\n            ... )\n            &gt;&gt;&gt; isinstance(cfg, dict)\n            True\n    \"\"\"\n    if model_name in self._models:\n        logger.warning(f\"Model {model_name} is already loaded. Unload it first.\")\n        return self._models[model_name]\n\n    model_path_str = str(model_path) if model_path is not None else None\n    model = None\n\n    if model_type == \"huggingface\":\n        model = self._load_huggingface_model(model_name, model_path_str, **kwargs)\n    elif model_type == \"local\":\n        model = self._load_local_model(model_name, model_path_str, **kwargs)\n    elif model_type == \"api\":\n        model = self._load_api_model(model_name, **kwargs)\n    elif model_type == \"interface\":\n        # Expect a concrete ModelInterface instance passed as 'model'\n        mi = kwargs.get(\"model\")\n        if mi is None:\n            raise ValueError(\n                \"For model_type='interface', pass a 'model' instance implementing ModelInterface\"\n            )\n        # Delay import for typing only to avoid hard dependency here\n        self.register_interface_model(mi)\n        model = mi\n    else:\n        raise ValueError(f\"Unsupported model type: {model_type}\")\n\n    return model\n</code></pre>"},{"location":"api/reference/#bench.evaluation.model_runner.ModelRunner.register_interface_model","title":"<code>register_interface_model(model)</code>","text":"<p>Register a model implementing the ModelInterface.</p> <p>This allows external callers to construct rich model wrappers and plug them into the runner for use with run_model().</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>'ModelInterface'</code> <p>An instance implementing ModelInterface.</p> required Source code in <code>bench/evaluation/model_runner.py</code> <pre><code>def register_interface_model(self, model: \"ModelInterface\") -&gt; None:\n    \"\"\"Register a model implementing the ModelInterface.\n\n    This allows external callers to construct rich model wrappers and plug\n    them into the runner for use with run_model().\n\n    Args:\n        model: An instance implementing ModelInterface.\n    \"\"\"\n    # Store object and minimal config\n    self._models[model.model_id] = model\n    self._model_configs[model.model_id] = {\n        \"type\": \"interface\",\n    }\n    logger.info(f\"Registered interface model: {model.model_id}\")\n</code></pre>"},{"location":"api/reference/#bench.evaluation.model_runner.ModelRunner.run_model","title":"<code>run_model(model_id, inputs, batch_size=8, **kwargs)</code>","text":"<p>Run inference on a batch of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>ID of the loaded model.</p> required <code>inputs</code> <code>List[Dict[str, Any]]</code> <p>List of input dictionaries.</p> required <code>batch_size</code> <code>int</code> <p>Number of inputs to process in a batch.</p> <code>8</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for model inference.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of model outputs with 'label' and 'score' keys.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model_id is not found or inputs are invalid.</p> <p>Examples:</p> <p>API model:     &gt;&gt;&gt; mr = ModelRunner()     &gt;&gt;&gt; _ = mr.load_model(\"api-x\", model_type=\"api\", endpoint=\"https://api.example\", api_key=\"k\")     &gt;&gt;&gt; out = mr.run_model(\"api-x\", inputs=[{\"text\": \"abc\"}], batch_size=1)     &gt;&gt;&gt; isinstance(out, list)     True</p> <p>HuggingFace (summarization):     &gt;&gt;&gt; mr = ModelRunner()     &gt;&gt;&gt; # _ = mr.load_model(\"hf-sum\", model_type=\"huggingface\", model_path=\"sshleifer/tiny-t5\", hf_task=\"summarization\")  # doctest: +SKIP     &gt;&gt;&gt; # out = mr.run_model(\"hf-sum\", inputs=[{\"document\": \"Short note.\"}], batch_size=1)  # doctest: +SKIP</p> Source code in <code>bench/evaluation/model_runner.py</code> <pre><code>def run_model(\n    self,\n    model_id: str,\n    inputs: List[Dict[str, Any]],\n    batch_size: int = 8,\n    **kwargs: Any,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Run inference on a batch of inputs.\n\n    Args:\n        model_id: ID of the loaded model.\n        inputs: List of input dictionaries.\n        batch_size: Number of inputs to process in a batch.\n        **kwargs: Additional arguments for model inference.\n\n    Returns:\n        List of model outputs with 'label' and 'score' keys.\n\n    Raises:\n        ValueError: If model_id is not found or inputs are invalid.\n\n    Examples:\n        API model:\n            &gt;&gt;&gt; mr = ModelRunner()\n            &gt;&gt;&gt; _ = mr.load_model(\"api-x\", model_type=\"api\", endpoint=\"https://api.example\", api_key=\"k\")\n            &gt;&gt;&gt; out = mr.run_model(\"api-x\", inputs=[{\"text\": \"abc\"}], batch_size=1)\n            &gt;&gt;&gt; isinstance(out, list)\n            True\n\n        HuggingFace (summarization):\n            &gt;&gt;&gt; mr = ModelRunner()\n            &gt;&gt;&gt; # _ = mr.load_model(\"hf-sum\", model_type=\"huggingface\", model_path=\"sshleifer/tiny-t5\", hf_task=\"summarization\")  # doctest: +SKIP\n            &gt;&gt;&gt; # out = mr.run_model(\"hf-sum\", inputs=[{\"document\": \"Short note.\"}], batch_size=1)  # doctest: +SKIP\n    \"\"\"\n    if model_id not in self._models:\n        raise ValueError(f\"Model {model_id} not loaded. Call load_model() first.\")\n\n    if not isinstance(inputs, list) or not all(\n        isinstance(item, dict) for item in inputs\n    ):\n        raise ValueError(\"Inputs must be a list of dictionaries.\")\n\n    # Get model type to handle different model types appropriately\n    model_type = self._model_configs[model_id].get(\"type\", \"unknown\")\n    model = self._models[model_id]\n    results: List[Dict[str, Any]] = []\n\n    # Get label map from config if available\n    label_map = self._model_configs[model_id].get(\"label_map\", {})\n\n    # Process inputs in batches\n    for i in range(0, len(inputs), batch_size):\n        batch = inputs[i : i + batch_size]\n\n        try:\n            batch_start = time.time()\n            if model_type == \"huggingface\":\n                # For HuggingFace pipeline\n                hf_task = self._model_configs.get(model_id, {}).get(\n                    \"hf_task\", \"text-classification\"\n                )\n                gen_kwargs = self._model_configs.get(model_id, {}).get(\n                    \"generation_kwargs\", {}\n                )\n                # Select input text field based on task type\n                if hf_task == \"summarization\":\n                    # Prefer common clinical note fields\n                    def _extract_text(d: Dict[str, Any]) -&gt; str:\n                        for key in (\"document\", \"text\", \"note\"):\n                            v = d.get(key)\n                            if isinstance(v, str) and v.strip():\n                                return v\n                        # Fallback to stringified input\n                        return str(d)\n\n                    texts = [_extract_text(item) for item in batch]\n                else:\n                    texts = [item.get(\"text\", \"\") for item in batch]\n\n                # Pass generation kwargs for generative tasks\n                call_kwargs: Dict[str, Any] = {}\n                if hf_task in {\"summarization\", \"text-generation\"} and isinstance(\n                    gen_kwargs, dict\n                ):\n                    call_kwargs = gen_kwargs\n                batch_results = model(texts, **call_kwargs)\n                if not isinstance(batch_results, list):\n                    batch_results = [batch_results]\n\n                for item, result in zip(batch, batch_results):\n                    # HF may return list of candidates per input\n                    if isinstance(result, list) and result:\n                        result = result[0]\n\n                    if hf_task == \"text-classification\":\n                        raw_label = str(result.get(\"label\", \"\"))\n                        score = float(result.get(\"score\", 0.0))\n                        predicted_label = label_map.get(raw_label, raw_label)\n                        if (\n                            predicted_label.startswith(\"LABEL_\")\n                            and \"_\" in predicted_label\n                        ):\n                            try:\n                                idx = int(predicted_label.split(\"_\")[1])\n                                if (\n                                    not label_map\n                                    and hasattr(model, \"model\")\n                                    and hasattr(model.model, \"config\")\n                                ):\n                                    if hasattr(model.model.config, \"id2label\"):\n                                        label_map = model.model.config.id2label\n                                        predicted_label = label_map.get(\n                                            idx, predicted_label\n                                        )\n                            except (ValueError, IndexError):\n                                pass\n                        results.append(\n                            {\n                                \"input\": item,\n                                \"label\": predicted_label,\n                                \"score\": score,\n                                \"raw_label\": raw_label,\n                            }\n                        )\n                    elif hf_task == \"summarization\":\n                        summary = (\n                            result.get(\"summary_text\", \"\")\n                            if isinstance(result, dict)\n                            else str(result)\n                        )\n                        results.append(\n                            {\n                                \"input\": item,\n                                \"summary\": summary,\n                                \"prediction\": summary,\n                            }\n                        )\n                    elif hf_task == \"text-generation\":\n                        gen = \"\"\n                        if isinstance(result, dict):\n                            gen = result.get(\"generated_text\", \"\")\n                        elif (\n                            isinstance(result, list)\n                            and result\n                            and isinstance(result[0], dict)\n                        ):\n                            gen = result[0].get(\"generated_text\", \"\")\n                        else:\n                            gen = str(result)\n                        results.append(\n                            {\n                                \"input\": item,\n                                \"text\": gen,\n                                \"prediction\": gen,\n                            }\n                        )\n                    else:\n                        # Fallback: store as raw text\n                        results.append(\n                            {\n                                \"input\": item,\n                                \"text\": str(result),\n                                \"prediction\": str(result),\n                            }\n                        )\n\n            elif model_type == \"local\":\n                # For local models that implement __call__\n                batch_results = model(batch, **kwargs)\n                if not isinstance(batch_results, list):\n                    batch_results = [batch_results]\n                results.extend(batch_results)\n\n            elif model_type == \"api\":\n                # For API models that implement __call__\n                batch_results = model(batch)\n                if not isinstance(batch_results, list):\n                    batch_results = [batch_results]\n                results.extend(batch_results)\n\n            elif model_type == \"interface\":\n                # For ModelInterface implementations\n                # We only rely on the standard predict() batch method\n                batch_results = model.predict(batch)\n                if not isinstance(batch_results, list):\n                    batch_results = [batch_results]\n                results.extend(batch_results)\n\n            else:\n                raise ValueError(f\"Unsupported model type: {model_type}\")\n\n        except Exception as e:\n            logger.error(\n                f\"Error running model {model_id} on batch: {e}\", exc_info=True\n            )\n            # Add empty dicts for failed predictions to maintain order and type\n            results.extend([{} for _ in batch])\n        finally:\n            try:\n                latency = time.time() - batch_start\n                logger.debug(\n                    \"Model batch completed\",\n                    extra={\n                        \"model_id\": model_id,\n                        \"model_type\": model_type,\n                        \"batch_size\": len(batch),\n                        \"latency_sec\": round(latency, 6),\n                    },\n                )\n            except Exception:\n                # Best-effort logging only\n                pass\n\n    return results\n</code></pre>"},{"location":"api/reference/#bench.evaluation.model_runner.ModelRunner.run_model_async","title":"<code>run_model_async(model_id, inputs, batch_size=8, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronous wrapper around run_model using a thread executor.</p> <p>This avoids adding async HTTP dependencies while providing an async API.</p> Example <p>import asyncio mr = ModelRunner() _ = mr.load_model(\"api-y\", model_type=\"api\", endpoint=\"https://api.example\", api_key=\"k\") async def _go(): ...     return await mr.run_model_async(\"api-y\", inputs=[{\"text\": \"abc\"}], batch_size=1)</p> Source code in <code>bench/evaluation/model_runner.py</code> <pre><code>async def run_model_async(\n    self,\n    model_id: str,\n    inputs: List[Dict[str, Any]],\n    batch_size: int = 8,\n    **kwargs: Any,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Asynchronous wrapper around run_model using a thread executor.\n\n    This avoids adding async HTTP dependencies while providing an async API.\n\n    Example:\n        &gt;&gt;&gt; import asyncio\n        &gt;&gt;&gt; mr = ModelRunner()\n        &gt;&gt;&gt; _ = mr.load_model(\"api-y\", model_type=\"api\", endpoint=\"https://api.example\", api_key=\"k\")\n        &gt;&gt;&gt; async def _go():\n        ...     return await mr.run_model_async(\"api-y\", inputs=[{\"text\": \"abc\"}], batch_size=1)\n        &gt;&gt;&gt; # asyncio.run(_go())  # doctest: +SKIP\n    \"\"\"\n    import asyncio\n\n    loop = asyncio.get_running_loop()\n    return await loop.run_in_executor(\n        None,\n        lambda: self.run_model(model_id, inputs, batch_size=batch_size, **kwargs),\n    )\n</code></pre>"},{"location":"api/reference/#bench.evaluation.model_runner.ModelRunner.run_model_async--asynciorun_go-doctest-skip","title":"asyncio.run(_go())  # doctest: +SKIP","text":""},{"location":"api/reference/#bench.evaluation.model_runner.ModelRunner.unload_model","title":"<code>unload_model(model_name)</code>","text":"<p>Unload a model and clean up resources.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to unload.</p> required Example <p>mr = ModelRunner() _ = mr.load_model(\"api-x\", model_type=\"api\", endpoint=\"https://api.example\", api_key=\"k\") mr.unload_model(\"api-x\")</p> Source code in <code>bench/evaluation/model_runner.py</code> <pre><code>def unload_model(self, model_name: str) -&gt; None:\n    \"\"\"Unload a model and clean up resources.\n\n    Args:\n        model_name: Name of the model to unload.\n\n    Example:\n        &gt;&gt;&gt; mr = ModelRunner()\n        &gt;&gt;&gt; _ = mr.load_model(\"api-x\", model_type=\"api\", endpoint=\"https://api.example\", api_key=\"k\")\n        &gt;&gt;&gt; mr.unload_model(\"api-x\")\n        &gt;&gt;&gt; # No exception means success\n    \"\"\"\n    if model_name in self._models:\n        # Best-effort cleanup of torch resources\n        try:\n            model = self._models.get(model_name)\n            model_ref = getattr(model, \"model\", None)\n            if model_ref is not None and hasattr(model_ref, \"cpu\"):\n                model_ref.cpu()\n            # Empty CUDA cache if available\n            try:\n                import torch  # type: ignore\n\n                if torch.cuda.is_available():  # pragma: no cover - env-dependent\n                    torch.cuda.empty_cache()\n            except Exception:\n                pass\n        except Exception:\n            pass\n        del self._models[model_name]\n    if model_name in self._model_configs:\n        del self._model_configs[model_name]\n    if model_name in self._tokenizers:\n        del self._tokenizers[model_name]\n    logger.info(f\"Unloaded model: {model_name}\")\n</code></pre>"},{"location":"api/reference/#bench.evaluation.model_runner.ModelRunner.unload_model--no-exception-means-success","title":"No exception means success","text":""},{"location":"api/reference/#metrics-clinical","title":"Metrics (clinical)","text":""},{"location":"api/reference/#bench.evaluation.metrics.clinical","title":"<code>bench.evaluation.metrics.clinical</code>","text":"<p>Concrete metric implementations for MedAISure metrics system.</p> <p>Implements: - ClinicalAccuracyMetric (aka clinical correctness on answers) - ReasoningQualityMetric (token-overlap F1 on rationales) - DiagnosticAccuracyMetric (label accuracy) - ClinicalRelevanceMetric (token Jaccard between summary and note)</p>"},{"location":"api/reference/#bench.evaluation.metrics.clinical.ClinicalAccuracyMetric","title":"<code>ClinicalAccuracyMetric</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Clinical correctness on answers using simple entity overlap.</p> <p>This implementation: - Extracts naive clinical entities from text into categories   (diagnoses, treatments, medications, symptoms, labs) using   lightweight lexicons and normalization. - Computes a weighted Jaccard similarity across categories. - Normalizes the final score to [0, 1]. - Stores a per-pair breakdown retrievable via   <code>get_last_breakdown()</code> without altering the base Metric API.</p> <p>Notes: - If no entities are found in both reference and prediction, we   fall back to a conservative normalized string match (exact/substring)   to avoid returning NaN and to keep intuitive behavior for free-form   short answers. - This is intentionally simple and dependency-light. It can be   upgraded later to use proper clinical NLP.</p> Source code in <code>bench/evaluation/metrics/clinical.py</code> <pre><code>class ClinicalAccuracyMetric(Metric):\n    \"\"\"Clinical correctness on answers using simple entity overlap.\n\n    This implementation:\n    - Extracts naive clinical entities from text into categories\n      (diagnoses, treatments, medications, symptoms, labs) using\n      lightweight lexicons and normalization.\n    - Computes a weighted Jaccard similarity across categories.\n    - Normalizes the final score to [0, 1].\n    - Stores a per-pair breakdown retrievable via\n      ``get_last_breakdown()`` without altering the base Metric API.\n\n    Notes:\n    - If no entities are found in both reference and prediction, we\n      fall back to a conservative normalized string match (exact/substring)\n      to avoid returning NaN and to keep intuitive behavior for free-form\n      short answers.\n    - This is intentionally simple and dependency-light. It can be\n      upgraded later to use proper clinical NLP.\n    \"\"\"\n\n    # Lightweight category weights; adjust as needed\n    _CATEGORY_WEIGHTS = {\n        \"diagnoses\": 3.0,\n        \"treatments\": 2.0,\n        \"medications\": 2.0,\n        \"symptoms\": 1.0,\n        \"labs\": 1.0,\n    }\n\n    # Minimal lexicon per category (lowercased, normalized)\n    _LEXICON = {\n        \"diagnoses\": {\n            \"pneumonia\",\n            \"sepsis\",\n            \"myocardial infarction\",\n            \"mi\",\n            \"stroke\",\n            \"copd\",\n            \"asthma\",\n            \"heart failure\",\n            \"diabetes\",\n        },\n        \"treatments\": {\n            \"antibiotics\",\n            \"oxygen\",\n            \"ventilation\",\n            \"iv fluids\",\n            \"insulin\",\n            \"surgery\",\n        },\n        \"medications\": {\n            \"amoxicillin\",\n            \"ceftriaxone\",\n            \"azithromycin\",\n            \"heparin\",\n            \"aspirin\",\n            \"insulin\",\n        },\n        \"symptoms\": {\n            \"fever\",\n            \"cough\",\n            \"shortness of breath\",\n            \"dyspnea\",\n            \"tachycardia\",\n            \"hypotension\",\n            \"chest pain\",\n        },\n        \"labs\": {\n            \"wbc\",\n            \"white blood cell\",\n            \"lactate\",\n            \"troponin\",\n            \"glucose\",\n        },\n    }\n\n    # Simple synonym/normalization map\n    _NORMALIZE_MAP = {\n        \"mi\": \"myocardial infarction\",\n        \"sob\": \"shortness of breath\",\n        \"white blood cells\": \"white blood cell\",\n        \"leukocytosis\": \"white blood cell\",\n    }\n\n    def __init__(self) -&gt; None:\n        self._last_breakdown: List[Dict[str, Any]] = []\n\n    @property\n    def name(self) -&gt; str:\n        return \"clinical_accuracy\"\n\n    def calculate(\n        self, expected_outputs: List[Dict], model_outputs: List[Dict]\n    ) -&gt; float:\n        Metric.validate_inputs(expected_outputs, model_outputs)\n        self._last_breakdown = []\n        scores: List[float] = []\n\n        for ref, pred in zip(expected_outputs, model_outputs):\n            # Extract free-text answers\n            t = (ref or {}).get(\"answer\")\n            p = (\n                (pred or {}).get(\"answer\")\n                or (pred or {}).get(\"prediction\")\n                or (pred or {}).get(\"text\")\n            )\n            t_norm = _normalize_text(t)\n            p_norm = _normalize_text(p)\n\n            # Entity extraction by category\n            t_entities = self._extract_entities(t_norm)\n            p_entities = self._extract_entities(p_norm)\n\n            # Compute weighted Jaccard across categories\n            cat_scores: Dict[str, float] = {}\n            weighted_inter = 0.0\n            weighted_union = 0.0\n            for cat, weight in self._CATEGORY_WEIGHTS.items():\n                t_set = t_entities.get(cat, set())\n                p_set = p_entities.get(cat, set())\n                inter = len(t_set &amp; p_set)\n                union = len(t_set | p_set)\n                if union:\n                    cat_scores[cat] = inter / union\n                    weighted_inter += weight * inter\n                    weighted_union += weight * union\n                else:\n                    cat_scores[cat] = 1.0 if not t_set and not p_set else 0.0\n\n            if weighted_union &gt; 0:\n                score = weighted_inter / weighted_union\n            else:\n                # Fallback to simple normalized string comparison when\n                # there are no recognized entities\n                if not t_norm and not p_norm:\n                    score = 1.0\n                elif not t_norm or not p_norm:\n                    score = 0.0\n                elif t_norm == p_norm or (t_norm in p_norm) or (p_norm in t_norm):\n                    score = 1.0\n                else:\n                    score = 0.0\n\n            scores.append(score)\n            self._last_breakdown.append(\n                {\n                    \"reference\": t_norm,\n                    \"prediction\": p_norm,\n                    \"entities_ref\": {k: sorted(v) for k, v in t_entities.items()},\n                    \"entities_pred\": {k: sorted(v) for k, v in p_entities.items()},\n                    \"category_scores\": cat_scores,\n                    \"pair_score\": score,\n                }\n            )\n\n        return float(sum(scores) / len(scores)) if scores else float(\"nan\")\n\n    def get_last_breakdown(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Return per-sample breakdown from the most recent calculate() call.\"\"\"\n        return self._last_breakdown\n\n    # ------------------------- helpers -------------------------\n    def _normalize_term(self, term: str) -&gt; str:\n        t = term.strip()\n        if not t:\n            return t\n        t = self._NORMALIZE_MAP.get(t, t)\n        return t\n\n    def _extract_entities(self, text: str) -&gt; Dict[str, set]:\n        entities: Dict[str, set] = {k: set() for k in self._CATEGORY_WEIGHTS.keys()}\n        if not text:\n            return entities\n\n        # Detect multi-word terms first to avoid splitting\n        remaining = text\n        # Build a list of multi-word lexicon terms by category\n        multi_terms: List[tuple[str, str]] = []  # (term, category)\n        for cat, terms in self._LEXICON.items():\n            for t in terms:\n                if \" \" in t:\n                    multi_terms.append((t, cat))\n\n        for term, cat in multi_terms:\n            if term in remaining:\n                entities[cat].add(self._normalize_term(term))\n                # naive removal to reduce double counting\n                remaining = remaining.replace(term, \" \")\n\n        # Single-word detection from remaining tokens\n        for tok in _tokenize(remaining):\n            tok = self._normalize_term(tok)\n            for cat, terms in self._LEXICON.items():\n                if tok in terms:\n                    entities[cat].add(tok)\n\n        # Drop empty categories to keep breakdown concise\n        return {k: v for k, v in entities.items() if v}\n</code></pre>"},{"location":"api/reference/#bench.evaluation.metrics.clinical.ClinicalAccuracyMetric.get_last_breakdown","title":"<code>get_last_breakdown()</code>","text":"<p>Return per-sample breakdown from the most recent calculate() call.</p> Source code in <code>bench/evaluation/metrics/clinical.py</code> <pre><code>def get_last_breakdown(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Return per-sample breakdown from the most recent calculate() call.\"\"\"\n    return self._last_breakdown\n</code></pre>"},{"location":"api/reference/#bench.evaluation.metrics.clinical.DiagnosticAccuracyMetric","title":"<code>DiagnosticAccuracyMetric</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>bench/evaluation/metrics/clinical.py</code> <pre><code>class DiagnosticAccuracyMetric(Metric):\n    def _normalize_label(self, s: Any) -&gt; str:\n        \"\"\"Normalize diagnosis/label text, including simple synonyms.\n\n        Includes simple mappings for common abbreviations after base normalization.\n        \"\"\"\n        t = _normalize_text(s)\n        return LABEL_SYNONYMS.get(t, t)\n\n    @property\n    def name(self) -&gt; str:\n        return \"diagnostic_accuracy\"\n\n    def calculate(\n        self, expected_outputs: List[Dict], model_outputs: List[Dict]\n    ) -&gt; float:\n        Metric.validate_inputs(expected_outputs, model_outputs)\n\n        # Domain knowledge and specialty config\n        self._last_breakdown: List[Dict[str, Any]] = []\n        KNOWN_LABELS: Dict[str, set] = {\n            \"cardiology\": {\"myocardial infarction\", \"mi\", \"heart failure\"},\n            \"infectious_disease\": {\"sepsis\", \"pneumonia\", \"flu\"},\n        }\n        CRITICAL_LABELS: Dict[str, set] = {\n            \"cardiology\": {\"myocardial infarction\", \"mi\"},\n            \"infectious_disease\": {\"sepsis\"},\n        }\n\n        # Weighted average to emphasize critical errors\n        total_weight = 0.0\n        weighted_sum = 0.0\n        for ref, pred in zip(expected_outputs, model_outputs):\n            exp_label = (ref or {}).get(\"label\") or (ref or {}).get(\"diagnosis\")\n            pred_label = (pred or {}).get(\"label\") or (pred or {}).get(\"prediction\")\n            specialty = (ref or {}).get(\"specialty\") or (pred or {}).get(\"specialty\")\n\n            # Normalize text labels for comparison (with synonym mapping)\n            t_norm = self._normalize_label(exp_label)\n            p_norm = self._normalize_label(pred_label)\n\n            # Base correctness\n            base = 1.0 if (t_norm and p_norm and t_norm == p_norm) else 0.0\n\n            # Specialty-based weight and knowledge checks\n            weight = 1.0\n            known = True\n            critical = False\n            if specialty:\n                s = _normalize_text(specialty)\n                if s in KNOWN_LABELS:\n                    known = (p_norm in KNOWN_LABELS[s]) if p_norm else False\n                if s in CRITICAL_LABELS:\n                    critical = t_norm in CRITICAL_LABELS[s]\n                    if critical:\n                        weight = 1.5  # emphasize critical diagnoses\n\n            # Penalize unknown label predictions slightly\n            if base == 0.0 and not known:\n                weight *= 1.0  # keep simple; weight influences average\n\n            total_weight += weight\n            weighted_sum += weight * base\n\n            self._last_breakdown.append(\n                {\n                    \"expected\": t_norm,\n                    \"predicted\": p_norm,\n                    \"specialty\": _normalize_text(specialty) if specialty else None,\n                    \"known_pred_label\": known,\n                    \"critical_expected\": critical,\n                    \"weight\": weight,\n                    \"correct\": base,\n                }\n            )\n\n        return float(weighted_sum / total_weight) if total_weight &gt; 0 else float(\"nan\")\n\n    def get_last_breakdown(self) -&gt; List[Dict[str, Any]]:\n        return getattr(self, \"_last_breakdown\", [])\n</code></pre>"},{"location":"api/reference/#bench.evaluation.metrics.clinical.ReasoningQualityMetric","title":"<code>ReasoningQualityMetric</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Token-overlap F1 plus lightweight reasoning heuristics.</p> <p>Components: - overlap_f1: token overlap between reference and predicted rationale - structure_score: presence of structure markers (because, therefore, steps) - evidence_score: mentions of evidence (labs, imaging, vitals, findings) - factual_consistency: simple rule checks against common clinical facts - fallacy_penalty: heuristic detection of common fallacy phrases</p> <p>Final score is a weighted combination clamped to [0,1]. A per-sample breakdown is available via get_last_breakdown().</p> Source code in <code>bench/evaluation/metrics/clinical.py</code> <pre><code>class ReasoningQualityMetric(Metric):\n    \"\"\"Token-overlap F1 plus lightweight reasoning heuristics.\n\n    Components:\n    - overlap_f1: token overlap between reference and predicted rationale\n    - structure_score: presence of structure markers (because, therefore, steps)\n    - evidence_score: mentions of evidence (labs, imaging, vitals, findings)\n    - factual_consistency: simple rule checks against common clinical facts\n    - fallacy_penalty: heuristic detection of common fallacy phrases\n\n    Final score is a weighted combination clamped to [0,1].\n    A per-sample breakdown is available via get_last_breakdown().\n    \"\"\"\n\n    _W_WEIGHTS = {\n        \"overlap_f1\": 0.4,\n        \"structure\": 0.15,\n        \"evidence\": 0.2,\n        \"factual\": 0.25,\n    }\n\n    _STRUCTURE_MARKERS = {\n        \"because\",\n        \"therefore\",\n        \"thus\",\n        \"hence\",\n        \"so\",\n        \"in conclusion\",\n        \"first\",\n        \"second\",\n        \"third\",\n        \"1.\",\n        \"2.\",\n        \"3.\",\n        \"step\",\n    }\n\n    _EVIDENCE_MARKERS = {\n        \"lab\",\n        \"wbc\",\n        \"lactate\",\n        \"troponin\",\n        \"imaging\",\n        \"xray\",\n        \"x-ray\",\n        \"ct\",\n        \"scan\",\n        \"vitals\",\n        \"fever\",\n        \"tachycardia\",\n        \"hypotension\",\n        \"finding\",\n        \"study\",\n        \"source\",\n        \"evidence\",\n    }\n\n    _FALLACY_PHRASES = {\n        \"correlation implies causation\",\n        \"post hoc\",\n        \"begs the question\",\n        \"circular reasoning\",\n        \"non sequitur\",\n        \"because i say\",\n        \"everyone knows\",\n        \"obviously true\",\n    }\n\n    # minimal factual rules: diagnosis -&gt; expected treatment/evidence keywords\n    _FACTS = {\n        \"pneumonia\": {\"antibiotic\", \"antibiotics\", \"xray\", \"x-ray\", \"infiltrate\"},\n        \"sepsis\": {\"iv fluids\", \"fluids\", \"lactate\", \"hypotension\"},\n        \"myocardial infarction\": {\"troponin\", \"ekg\", \"aspirin\", \"heparin\"},\n        \"mi\": {\"troponin\", \"ekg\", \"aspirin\", \"heparin\"},\n    }\n\n    def __init__(self) -&gt; None:\n        self._last_breakdown: List[Dict[str, Any]] = []\n\n    @property\n    def name(self) -&gt; str:\n        return \"reasoning_quality\"\n\n    def calculate(\n        self, expected_outputs: List[Dict], model_outputs: List[Dict]\n    ) -&gt; float:\n        Metric.validate_inputs(expected_outputs, model_outputs)\n        self._last_breakdown = []\n\n        scores: List[float] = []\n        for ref, pred in zip(expected_outputs, model_outputs):\n            r = (ref or {}).get(\"rationale\") or (ref or {}).get(\"explanation\")\n            p = (\n                (pred or {}).get(\"rationale\")\n                or (pred or {}).get(\"explanation\")\n                or (pred or {}).get(\"prediction\")\n                or (pred or {}).get(\"text\")\n            )\n\n            f1_score = self._f1(r, p)\n            structure = self._structure_score(p)\n            evidence = self._evidence_score(p)\n            factual = self._factual_consistency_score(\n                (ref or {}).get(\"answer\") or (ref or {}).get(\"diagnosis\"), p\n            )\n            fallacy_pen = self._fallacy_penalty(p)\n\n            weighted = (\n                self._W_WEIGHTS[\"overlap_f1\"] * f1_score\n                + self._W_WEIGHTS[\"structure\"] * structure\n                + self._W_WEIGHTS[\"evidence\"] * evidence\n                + self._W_WEIGHTS[\"factual\"] * factual\n            )\n            score = max(0.0, min(1.0, weighted - fallacy_pen))\n            scores.append(score)\n\n            self._last_breakdown.append(\n                {\n                    \"reference_rationale\": _normalize_text(r),\n                    \"predicted_rationale\": _normalize_text(p),\n                    \"overlap_f1\": f1_score,\n                    \"structure_score\": structure,\n                    \"evidence_score\": evidence,\n                    \"factual_consistency\": factual,\n                    \"fallacy_penalty\": fallacy_pen,\n                    \"final_score\": score,\n                }\n            )\n\n        return float(sum(scores) / len(scores)) if scores else float(\"nan\")\n\n    def get_last_breakdown(self) -&gt; List[Dict[str, Any]]:\n        return self._last_breakdown\n\n    # ----------------------- helpers -----------------------\n    def _f1(self, a: Any, b: Any) -&gt; float:\n        a_tokens = _tokenize(a)\n        b_tokens = _tokenize(b)\n        if not a_tokens and not b_tokens:\n            return 1.0\n        if not a_tokens or not b_tokens:\n            return 0.0\n        common = 0\n        b_count: Dict[str, int] = {}\n        for tok in b_tokens:\n            b_count[tok] = b_count.get(tok, 0) + 1\n        for tok in a_tokens:\n            if b_count.get(tok, 0) &gt; 0:\n                common += 1\n                b_count[tok] -= 1\n        precision = common / len(b_tokens)\n        recall = common / len(a_tokens)\n        return (\n            0.0\n            if (precision + recall) == 0\n            else 2 * precision * recall / (precision + recall)\n        )\n\n    def _structure_score(self, text: Any) -&gt; float:\n        t = _normalize_text(text)\n        if not t:\n            return 0.0\n        hits = sum(1 for m in self._STRUCTURE_MARKERS if m in t)\n        # cap at 3 for diminishing returns\n        return min(1.0, hits / 3.0)\n\n    def _evidence_score(self, text: Any) -&gt; float:\n        t = _normalize_text(text)\n        if not t:\n            return 0.0\n        hits = sum(1 for m in self._EVIDENCE_MARKERS if m in t)\n        return min(1.0, hits / 3.0)\n\n    def _fallacy_penalty(self, text: Any) -&gt; float:\n        t = _normalize_text(text)\n        if not t:\n            return 0.0\n        hits = sum(1 for m in self._FALLACY_PHRASES if m in t)\n        # each hit penalizes modestly, capped\n        return min(0.4, 0.15 * hits)\n\n    def _factual_consistency_score(self, diagnosis: Any, text: Any) -&gt; float:\n        diag = _normalize_text(diagnosis)\n        t = _normalize_text(text)\n        if not t:\n            return 0.0\n        if not diag:\n            # if unknown diagnosis, score based on generic evidence mentions\n            return self._evidence_score(t)\n        expected = set()\n        for key, facts in self._FACTS.items():\n            if key in diag:\n                expected |= facts\n        if not expected:\n            return self._evidence_score(t)\n        matches = sum(1 for kw in expected if kw in t)\n        return min(1.0, matches / max(1, len(expected) // 2))\n</code></pre>"},{"location":"api/reference/#task-schema","title":"Task schema","text":""},{"location":"api/reference/#bench.models.medical_task","title":"<code>bench.models.medical_task</code>","text":"<p>Core task data models used by MedAISure.</p> <p>This module defines the <code>TaskType</code> enum and the <code>MedicalTask</code> Pydantic model, which represent the canonical schema for describing benchmark tasks.</p> Quick start <p>from bench.models.medical_task import MedicalTask, TaskType task = MedicalTask( ...     task_id=\"qa-demo\", ...     task_type=TaskType.QA, ...     description=\"Answer short medical questions\", ...     inputs=[{\"question\": \"What is BP?\"}], ...     expected_outputs=[{\"answer\": \"blood pressure\"}], ...     metrics=[\"accuracy\"], ...     input_schema={\"required\": [\"question\"]}, ...     output_schema={\"required\": [\"answer\"]}, ... ) task.to_dict()[\"task_type\"] 'qa' print(task.convert(\"yaml\").splitlines()[0]) schema_version: 1</p>"},{"location":"api/reference/#bench.models.medical_task.MedicalTask","title":"<code>MedicalTask</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a medical evaluation task definition.</p> Fields <p>schema_version: Integer version for forward compatibility (default 1). task_id: Unique identifier for the task (non-empty string). name: Optional human-friendly name (defaults to task_id when omitted). task_type: One of <code>TaskType</code> values. description: Optional free-text description. inputs: List of input example dicts for the task (non-empty). expected_outputs: List of expected output example dicts, 1:1 with inputs when both provided. metrics: Non-empty, unique metric names. input_schema/output_schema: Minimal schemas with optional <code>required: [..]</code> keys. dataset: Optional inline records used for lightweight examples or micro-benchmarks.</p> <p>Examples:</p> <p>Create and serialize a task</p> <pre><code>&gt;&gt;&gt; t = MedicalTask(\n...     task_id=\"sum-1\", task_type=TaskType.SUMMARIZATION,\n...     inputs=[{\"document\": \"Patient note\"}],\n...     expected_outputs=[{\"summary\": \"Short note\"}],\n...     metrics=[\"rouge_l\"],\n...     input_schema={\"required\": [\"document\"]},\n...     output_schema={\"required\": [\"summary\"]},\n... )\n&gt;&gt;&gt; isinstance(json.loads(t.to_json()), dict)\nTrue\n</code></pre> <p>Save and load from YAML</p> <pre><code>&gt;&gt;&gt; path = Path(\"/tmp/medaisure_demo.yaml\")\n&gt;&gt;&gt; t.save(path, format=\"yaml\")\n&gt;&gt;&gt; MedicalTask.from_file(path).task_id\n'sum-1'\n</code></pre> Source code in <code>bench/models/medical_task.py</code> <pre><code>class MedicalTask(BaseModel):\n    \"\"\"Represents a medical evaluation task definition.\n\n    Fields:\n        schema_version: Integer version for forward compatibility (default 1).\n        task_id: Unique identifier for the task (non-empty string).\n        name: Optional human-friendly name (defaults to task_id when omitted).\n        task_type: One of `TaskType` values.\n        description: Optional free-text description.\n        inputs: List of input example dicts for the task (non-empty).\n        expected_outputs: List of expected output example dicts, 1:1 with inputs when both provided.\n        metrics: Non-empty, unique metric names.\n        input_schema/output_schema: Minimal schemas with optional `required: [..]` keys.\n        dataset: Optional inline records used for lightweight examples or micro-benchmarks.\n\n    Examples:\n        Create and serialize a task\n        &gt;&gt;&gt; t = MedicalTask(\n        ...     task_id=\"sum-1\", task_type=TaskType.SUMMARIZATION,\n        ...     inputs=[{\"document\": \"Patient note\"}],\n        ...     expected_outputs=[{\"summary\": \"Short note\"}],\n        ...     metrics=[\"rouge_l\"],\n        ...     input_schema={\"required\": [\"document\"]},\n        ...     output_schema={\"required\": [\"summary\"]},\n        ... )\n        &gt;&gt;&gt; isinstance(json.loads(t.to_json()), dict)\n        True\n\n        Save and load from YAML\n        &gt;&gt;&gt; path = Path(\"/tmp/medaisure_demo.yaml\")\n        &gt;&gt;&gt; t.save(path, format=\"yaml\")\n        &gt;&gt;&gt; MedicalTask.from_file(path).task_id\n        'sum-1'\n    \"\"\"\n\n    # Simple schema versioning for forward compatibility\n    schema_version: int = 1\n\n    task_id: str\n    name: Optional[str] = None\n    task_type: TaskType\n    description: str = \"\"\n\n    inputs: List[Dict[str, Any]] = Field(default_factory=list)\n    expected_outputs: List[Dict[str, Any]] = Field(default_factory=list)\n    metrics: List[str] = Field(default_factory=list)\n\n    input_schema: Dict[str, Any] = Field(default_factory=dict)\n    output_schema: Dict[str, Any] = Field(default_factory=dict)\n\n    # Inline dataset samples for lightweight tasks/examples\n    dataset: List[Dict[str, Any]] = Field(default_factory=list)\n\n    @field_validator(\"task_type\", mode=\"before\")\n    @classmethod\n    def _validate_task_type(cls, v: Any) -&gt; TaskType:\n        if isinstance(v, TaskType):\n            return v\n        if isinstance(v, str):\n            try:\n                return TaskType(v)\n            except Exception:\n                raise ValueError(\"invalid task_type\")\n        raise ValueError(\"invalid task_type\")\n\n    @field_validator(\"task_id\")\n    @classmethod\n    def _non_empty_task_id(cls, v: str) -&gt; str:\n        if not isinstance(v, str) or not v.strip():\n            raise ValueError(\"task_id must be a non-empty string\")\n        return v\n\n    @field_validator(\"inputs\")\n    @classmethod\n    def _validate_inputs(cls, v: List[Dict[str, Any]]) -&gt; List[Dict[str, Any]]:\n        if v is None or len(v) == 0:\n            raise ValueError(\"inputs must not be empty\")\n        return v\n\n    @field_validator(\"metrics\")\n    @classmethod\n    def _validate_metrics(cls, v: List[str]) -&gt; List[str]:\n        if any((m is None) or (isinstance(m, str) and m.strip() == \"\") for m in v):\n            raise ValueError(\"metrics must not contain empty values\")\n        # ensure uniqueness and normalized spacing\n        normalized = [m.strip() for m in v]\n        if len(set(normalized)) != len(normalized):\n            raise ValueError(\"metrics must be unique\")\n        return normalized\n\n    @field_validator(\"expected_outputs\")\n    @classmethod\n    def _validate_expected_outputs(\n        cls, v: List[Dict[str, Any]], info: ValidationInfo\n    ) -&gt; List[Dict[str, Any]]:\n        if v is None:\n            return []\n        inputs = (info.data or {}).get(\"inputs\", [])\n        # If provided, require 1:1 alignment with inputs for simple tasks\n        if inputs and v and len(v) != len(inputs):\n            raise ValueError(\n                \"expected_outputs length must match inputs length when both are provided\"\n            )\n        return v\n\n    @field_validator(\"input_schema\")\n    @classmethod\n    def _validate_input_schema(cls, v: Dict[str, Any]) -&gt; Dict[str, Any]:\n        # Keep schema lightweight: ensure structure is a dict and optional 'required' is a list of strings\n        v = v or {}\n        if not isinstance(v, dict):\n            raise ValueError(\"input_schema must be a dictionary\")\n        req = v.get(\"required\")\n        if req is not None:\n            if not isinstance(req, list) or not all(isinstance(k, str) for k in req):\n                raise ValueError(\"input_schema.required must be a list of strings\")\n        return v\n\n    @field_validator(\"output_schema\")\n    @classmethod\n    def _validate_output_schema(cls, v: Dict[str, Any]) -&gt; Dict[str, Any]:\n        v = v or {}\n        if not isinstance(v, dict):\n            raise ValueError(\"output_schema must be a dictionary\")\n        req = v.get(\"required\")\n        if req is not None:\n            if not isinstance(req, list) or not all(isinstance(k, str) for k in req):\n                raise ValueError(\"output_schema.required must be a list of strings\")\n        return v\n\n    @field_validator(\"dataset\")\n    @classmethod\n    def _validate_dataset(\n        cls, v: List[Dict[str, Any]], info: ValidationInfo\n    ) -&gt; List[Dict[str, Any]]:\n        # Optional dataset; if provided, check minimal key presence per schemas\n        v = v or []\n        inp_req = []\n        out_req = []\n        data = info.data or {}\n        try:\n            inp_req = list(\n                (data.get(\"input_schema\", {}) or {}).get(\"required\", []) or []\n            )\n            out_req = list(\n                (data.get(\"output_schema\", {}) or {}).get(\"required\", []) or []\n            )\n        except Exception:\n            # If schemas malformed, other validators will raise\n            inp_req = []\n            out_req = []\n        for i, row in enumerate(v):\n            if not isinstance(row, dict):\n                raise ValueError(f\"dataset[{i}] must be a dict\")\n            # If dataset rows carry nested 'input'/'output', validate keys\n            if inp_req and isinstance(row.get(\"input\"), dict):\n                missing = [k for k in inp_req if k not in row[\"input\"]]\n                if missing:\n                    raise ValueError(\n                        f\"dataset[{i}].input missing required keys: {missing}\"\n                    )\n            if out_req and isinstance(row.get(\"output\"), dict):\n                missing = [k for k in out_req if k not in row[\"output\"]]\n                if missing:\n                    raise ValueError(\n                        f\"dataset[{i}].output missing required keys: {missing}\"\n                    )\n        return v\n\n    # --- Convenience serialization helpers ---\n    def to_dict(\n        self,\n        *,\n        include: Optional[set | dict] = None,\n        exclude: Optional[set | dict] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Return a plain-Python dict representation of the model.\n\n        Supports partial serialization via include/exclude.\n        \"\"\"\n        return self.model_dump(include=include, exclude=exclude)\n\n    def to_json(\n        self,\n        indent: Optional[int] = None,\n        *,\n        include: Optional[set | dict] = None,\n        exclude: Optional[set | dict] = None,\n    ) -&gt; str:\n        \"\"\"Return a JSON string representation of the model.\n\n        Args:\n            indent: Optional indentation for pretty printing.\n            include/exclude: Optional fields to include/exclude.\n        \"\"\"\n        return self.model_dump_json(indent=indent, include=include, exclude=exclude)\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"MedicalTask\":\n        \"\"\"Create a `MedicalTask` from a plain dict with validation.\"\"\"\n        return cls.model_validate(data)\n\n    @classmethod\n    def from_json(cls, data: str) -&gt; \"MedicalTask\":\n        \"\"\"Create a `MedicalTask` from a JSON string with validation.\"\"\"\n        return cls.model_validate_json(data)\n\n    # --- YAML helpers ---\n    def to_yaml(self) -&gt; str:\n        \"\"\"Return a YAML string representation of the model.\"\"\"\n        return yaml.safe_dump(json.loads(self.model_dump_json()), sort_keys=False)\n\n    @classmethod\n    def from_yaml(cls, data: str) -&gt; \"MedicalTask\":\n        \"\"\"Create a `MedicalTask` from YAML string with validation.\n\n        Ensures a default `schema_version` when absent.\n        \"\"\"\n        payload = yaml.safe_load(data) or {}\n        if \"schema_version\" not in payload:\n            payload[\"schema_version\"] = 1\n        return cls.model_validate(payload)\n\n    # --- CSV helpers ---\n    def dataset_to_csv(self) -&gt; str:\n        \"\"\"Export `dataset` to CSV string (best-effort flattening).\"\"\"\n        rows = self.dataset or []\n        if not rows:\n            return \"\"\n        # Compute headers union\n        headers: List[str] = []\n        for r in rows:\n            for k in r.keys():\n                if k not in headers:\n                    headers.append(k)\n        buf = StringIO()\n        writer = csv.DictWriter(buf, fieldnames=headers)\n        writer.writeheader()\n        for r in rows:\n            writer.writerow({k: r.get(k) for k in headers})\n        return buf.getvalue()\n\n    # --- File I/O ---\n    def save(self, file_path: str | Path, format: Optional[str] = None) -&gt; None:\n        \"\"\"Persist the task to disk in JSON or YAML format.\n\n        Args:\n            file_path: Target file path; extension is used when `format` is not provided.\n            format: One of {\"json\", \"yaml\", \"yml\"}.\n        Raises:\n            ValueError: If format/extension is not supported.\n        \"\"\"\n        path = Path(file_path)\n        fmt = (format or path.suffix.lstrip(\".\")).lower()\n        path.parent.mkdir(parents=True, exist_ok=True)\n        if fmt == \"json\":\n            path.write_text(self.to_json(indent=2))\n        elif fmt in {\"yaml\", \"yml\"}:\n            path.write_text(self.to_yaml())\n        else:\n            raise ValueError(f\"Unsupported format for MedicalTask.save: {fmt}\")\n\n    @classmethod\n    def from_file(cls, file_path: str | Path) -&gt; \"MedicalTask\":\n        \"\"\"Load and validate a task from a JSON or YAML file.\"\"\"\n        path = Path(file_path)\n        text = path.read_text()\n        suf = path.suffix.lower()\n        if suf == \".json\":\n            return cls.from_json(text)\n        if suf in {\".yaml\", \".yml\"}:\n            return cls.from_yaml(text)\n        raise ValueError(f\"Unsupported file type for MedicalTask.from_file: {suf}\")\n\n    # --- Conversion helper ---\n    def convert(self, to: str) -&gt; str:\n        \"\"\"Convert the task to a different textual format (\"json\" or \"yaml\").\"\"\"\n        to = to.lower()\n        if to == \"json\":\n            return self.to_json(indent=2)\n        if to in {\"yaml\", \"yml\"}:\n            return self.to_yaml()\n        raise ValueError(f\"Unsupported conversion target: {to}\")\n</code></pre>"},{"location":"api/reference/#bench.models.medical_task.MedicalTask.convert","title":"<code>convert(to)</code>","text":"<p>Convert the task to a different textual format (\"json\" or \"yaml\").</p> Source code in <code>bench/models/medical_task.py</code> <pre><code>def convert(self, to: str) -&gt; str:\n    \"\"\"Convert the task to a different textual format (\"json\" or \"yaml\").\"\"\"\n    to = to.lower()\n    if to == \"json\":\n        return self.to_json(indent=2)\n    if to in {\"yaml\", \"yml\"}:\n        return self.to_yaml()\n    raise ValueError(f\"Unsupported conversion target: {to}\")\n</code></pre>"},{"location":"api/reference/#bench.models.medical_task.MedicalTask.dataset_to_csv","title":"<code>dataset_to_csv()</code>","text":"<p>Export <code>dataset</code> to CSV string (best-effort flattening).</p> Source code in <code>bench/models/medical_task.py</code> <pre><code>def dataset_to_csv(self) -&gt; str:\n    \"\"\"Export `dataset` to CSV string (best-effort flattening).\"\"\"\n    rows = self.dataset or []\n    if not rows:\n        return \"\"\n    # Compute headers union\n    headers: List[str] = []\n    for r in rows:\n        for k in r.keys():\n            if k not in headers:\n                headers.append(k)\n    buf = StringIO()\n    writer = csv.DictWriter(buf, fieldnames=headers)\n    writer.writeheader()\n    for r in rows:\n        writer.writerow({k: r.get(k) for k in headers})\n    return buf.getvalue()\n</code></pre>"},{"location":"api/reference/#bench.models.medical_task.MedicalTask.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create a <code>MedicalTask</code> from a plain dict with validation.</p> Source code in <code>bench/models/medical_task.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"MedicalTask\":\n    \"\"\"Create a `MedicalTask` from a plain dict with validation.\"\"\"\n    return cls.model_validate(data)\n</code></pre>"},{"location":"api/reference/#bench.models.medical_task.MedicalTask.from_file","title":"<code>from_file(file_path)</code>  <code>classmethod</code>","text":"<p>Load and validate a task from a JSON or YAML file.</p> Source code in <code>bench/models/medical_task.py</code> <pre><code>@classmethod\ndef from_file(cls, file_path: str | Path) -&gt; \"MedicalTask\":\n    \"\"\"Load and validate a task from a JSON or YAML file.\"\"\"\n    path = Path(file_path)\n    text = path.read_text()\n    suf = path.suffix.lower()\n    if suf == \".json\":\n        return cls.from_json(text)\n    if suf in {\".yaml\", \".yml\"}:\n        return cls.from_yaml(text)\n    raise ValueError(f\"Unsupported file type for MedicalTask.from_file: {suf}\")\n</code></pre>"},{"location":"api/reference/#bench.models.medical_task.MedicalTask.from_json","title":"<code>from_json(data)</code>  <code>classmethod</code>","text":"<p>Create a <code>MedicalTask</code> from a JSON string with validation.</p> Source code in <code>bench/models/medical_task.py</code> <pre><code>@classmethod\ndef from_json(cls, data: str) -&gt; \"MedicalTask\":\n    \"\"\"Create a `MedicalTask` from a JSON string with validation.\"\"\"\n    return cls.model_validate_json(data)\n</code></pre>"},{"location":"api/reference/#bench.models.medical_task.MedicalTask.from_yaml","title":"<code>from_yaml(data)</code>  <code>classmethod</code>","text":"<p>Create a <code>MedicalTask</code> from YAML string with validation.</p> <p>Ensures a default <code>schema_version</code> when absent.</p> Source code in <code>bench/models/medical_task.py</code> <pre><code>@classmethod\ndef from_yaml(cls, data: str) -&gt; \"MedicalTask\":\n    \"\"\"Create a `MedicalTask` from YAML string with validation.\n\n    Ensures a default `schema_version` when absent.\n    \"\"\"\n    payload = yaml.safe_load(data) or {}\n    if \"schema_version\" not in payload:\n        payload[\"schema_version\"] = 1\n    return cls.model_validate(payload)\n</code></pre>"},{"location":"api/reference/#bench.models.medical_task.MedicalTask.save","title":"<code>save(file_path, format=None)</code>","text":"<p>Persist the task to disk in JSON or YAML format.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Target file path; extension is used when <code>format</code> is not provided.</p> required <code>format</code> <code>Optional[str]</code> <p>One of {\"json\", \"yaml\", \"yml\"}.</p> <code>None</code> <p>Raises:     ValueError: If format/extension is not supported.</p> Source code in <code>bench/models/medical_task.py</code> <pre><code>def save(self, file_path: str | Path, format: Optional[str] = None) -&gt; None:\n    \"\"\"Persist the task to disk in JSON or YAML format.\n\n    Args:\n        file_path: Target file path; extension is used when `format` is not provided.\n        format: One of {\"json\", \"yaml\", \"yml\"}.\n    Raises:\n        ValueError: If format/extension is not supported.\n    \"\"\"\n    path = Path(file_path)\n    fmt = (format or path.suffix.lstrip(\".\")).lower()\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if fmt == \"json\":\n        path.write_text(self.to_json(indent=2))\n    elif fmt in {\"yaml\", \"yml\"}:\n        path.write_text(self.to_yaml())\n    else:\n        raise ValueError(f\"Unsupported format for MedicalTask.save: {fmt}\")\n</code></pre>"},{"location":"api/reference/#bench.models.medical_task.MedicalTask.to_dict","title":"<code>to_dict(*, include=None, exclude=None)</code>","text":"<p>Return a plain-Python dict representation of the model.</p> <p>Supports partial serialization via include/exclude.</p> Source code in <code>bench/models/medical_task.py</code> <pre><code>def to_dict(\n    self,\n    *,\n    include: Optional[set | dict] = None,\n    exclude: Optional[set | dict] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Return a plain-Python dict representation of the model.\n\n    Supports partial serialization via include/exclude.\n    \"\"\"\n    return self.model_dump(include=include, exclude=exclude)\n</code></pre>"},{"location":"api/reference/#bench.models.medical_task.MedicalTask.to_json","title":"<code>to_json(indent=None, *, include=None, exclude=None)</code>","text":"<p>Return a JSON string representation of the model.</p> <p>Parameters:</p> Name Type Description Default <code>indent</code> <code>Optional[int]</code> <p>Optional indentation for pretty printing.</p> <code>None</code> <code>include/exclude</code> <p>Optional fields to include/exclude.</p> required Source code in <code>bench/models/medical_task.py</code> <pre><code>def to_json(\n    self,\n    indent: Optional[int] = None,\n    *,\n    include: Optional[set | dict] = None,\n    exclude: Optional[set | dict] = None,\n) -&gt; str:\n    \"\"\"Return a JSON string representation of the model.\n\n    Args:\n        indent: Optional indentation for pretty printing.\n        include/exclude: Optional fields to include/exclude.\n    \"\"\"\n    return self.model_dump_json(indent=indent, include=include, exclude=exclude)\n</code></pre>"},{"location":"api/reference/#bench.models.medical_task.MedicalTask.to_yaml","title":"<code>to_yaml()</code>","text":"<p>Return a YAML string representation of the model.</p> Source code in <code>bench/models/medical_task.py</code> <pre><code>def to_yaml(self) -&gt; str:\n    \"\"\"Return a YAML string representation of the model.\"\"\"\n    return yaml.safe_dump(json.loads(self.model_dump_json()), sort_keys=False)\n</code></pre>"},{"location":"api/reference/#bench.models.medical_task.TaskType","title":"<code>TaskType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of supported benchmark task types.</p> <ul> <li>DIAGNOSTIC_REASONING: Case-based reasoning \u2192 diagnosis</li> <li>QA: Question answering</li> <li>SUMMARIZATION: Clinical/medical text summarization</li> <li>COMMUNICATION: Patient/provider communication generation</li> </ul> Source code in <code>bench/models/medical_task.py</code> <pre><code>class TaskType(str, Enum):\n    \"\"\"Enumeration of supported benchmark task types.\n\n    - DIAGNOSTIC_REASONING: Case-based reasoning \u2192 diagnosis\n    - QA: Question answering\n    - SUMMARIZATION: Clinical/medical text summarization\n    - COMMUNICATION: Patient/provider communication generation\n    \"\"\"\n\n    DIAGNOSTIC_REASONING = \"diagnostic_reasoning\"\n    QA = \"qa\"\n    SUMMARIZATION = \"summarization\"\n    COMMUNICATION = \"communication\"\n</code></pre>"},{"location":"metrics/clinical_accuracy/","title":"Clinical Accuracy","text":"<p>Definition: Measures correctness of medical facts, diagnoses, and recommendations.</p> <p>Scoring Dimensions - Correctness: factual alignment with gold standard - Safety: absence of dangerous or contraindicated advice - Specificity: precise, actionable answers vs vague text</p> <p>Methodology - Exact match / label accuracy for discrete tasks - NLI-based agreement or LLM-judge with calibrated rubric - Penalize hallucinations; weight critical errors higher</p> <p>Usage - Applies to: Medical QA, Diagnostic Reasoning (final dx), Summarization (facts) - Combine with <code>Reasoning Quality</code> for holistic view</p>"},{"location":"metrics/clinical_accuracy/#code-reference","title":"Code reference","text":"<p>Implementation: <code>bench/evaluation/metrics/clinical.py</code> \u2192 <code>ClinicalAccuracyMetric</code></p> <p>Excerpt <pre><code>class ClinicalAccuracyMetric(Metric):\n    def calculate(self, expected_outputs: list[dict], model_outputs: list[dict]) -&gt; float:\n        Metric.validate_inputs(expected_outputs, model_outputs)\n        # 1) normalize text\n        # 2) extract naive clinical entities by category\n        # 3) weighted Jaccard across categories\n        # 4) fallback to normalized string comparison if no entities\n        ...\n\n    def get_last_breakdown(self) -&gt; list[dict[str, Any]]:\n        \"\"\"Per-sample breakdown from the last calculate() call.\"\"\"\n        return self._last_breakdown\n</code></pre></p> <p>Notes - Categories and weights are kept lightweight for dependency-free scoring. - Synonym normalization is applied for common medical abbreviations.</p> <p>See also - Python API \u2192 Metrics (clinical): api/reference.md#metrics-clinical</p>"},{"location":"metrics/overview/","title":"Metrics Overview","text":"<p>This section summarizes metric categories and how scores are computed and aggregated.</p>"},{"location":"metrics/overview/#categories","title":"Categories","text":"<ul> <li>Clinical Accuracy: correctness and safety of medical content</li> <li>Reasoning Quality: structure, plausibility, and evidence use in reasoning</li> <li>Domain-specific: task- or specialty-specific checks</li> </ul>"},{"location":"metrics/overview/#methodology-flow","title":"Methodology (flow)","text":"<ol> <li>Normalize inputs/outputs</li> <li>Lowercasing, whitespace, common medical synonym normalization</li> <li>Extract features</li> <li>Clinical entities by category (e.g., dx, meds) or structural markers (steps, evidence)</li> <li>Score per-sample</li> <li>Clinical Accuracy: weighted Jaccard over extracted entities; fallback to normalized string compare</li> <li>Reasoning Quality: composite of overlap F1, structure, evidence, factual consistency minus fallacy penalty</li> <li>Aggregate</li> <li>Per-task: mean of per-sample scores (with optional weighting per component)</li> <li>Overall: average across tasks (reported in <code>BenchmarkReport</code>)</li> </ol>"},{"location":"metrics/overview/#weighting-breakdowns","title":"Weighting &amp; breakdowns","text":"<ul> <li>Component weights are defined in metric implementations (see API links below).</li> <li>Per-sample breakdowns are accessible via metric APIs (e.g., <code>get_last_breakdown()</code>), and are surfaced in evaluation metadata when available.</li> </ul>"},{"location":"metrics/overview/#deep-links-api","title":"Deep links (API)","text":"<ul> <li>Python API \u2192 Metrics (clinical): api/reference.md#metrics-clinical</li> <li><code>ClinicalAccuracyMetric.calculate</code></li> <li><code>ReasoningQualityMetric.calculate</code></li> </ul>"},{"location":"metrics/overview/#references","title":"References","text":"<ul> <li>metrics_guidelines.md</li> <li>metrics_testing.md</li> </ul>"},{"location":"metrics/reasoning_quality/","title":"Reasoning Quality","text":"<p>Definition: Evaluates structure, coherence, and clinical plausibility of reasoning traces.</p> <p>Rubric (0-5) - Structure: clear steps, justified transitions - Plausibility: aligns with clinical guidelines/pathophysiology - Use of Data: incorporates key findings; avoids overfitting - Calibration: expresses uncertainty appropriately</p> <p>Scoring Approaches - Rule-based checks (section headers, presence of justification) - LLM-judge with anchored examples - Pair with Clinical Accuracy for final scorecard</p>"},{"location":"metrics/reasoning_quality/#code-reference","title":"Code reference","text":"<p>Implementation: <code>bench/evaluation/metrics/clinical.py</code> \u2192 <code>ReasoningQualityMetric</code></p> <p>Excerpt ```python class ReasoningQualityMetric(Metric):     def calculate(self, expected_outputs: list[dict], model_outputs: list[dict]) -&gt; float:         # overlap F1 + structure + evidence + factual consistency - fallacy_penalty         ...</p> <pre><code># Components (weights in _W_WEIGHTS)\n# - _f1(a, b): token-overlap F1\n# - _structure_score(text): markers like \"because\", numbered steps\n# - _evidence_score(text): labs/imaging/vitals mentions\n# - _factual_consistency_score(diagnosis, text): simple rules\n# - _fallacy_penalty(text): penalize common fallacy phrases\n</code></pre> <p>See also - Python API \u2192 Metrics (clinical): api/reference.md#metrics-clinical</p>"},{"location":"models/api_models/","title":"API Models","text":"<p>Use hosted HTTP APIs by configuring <code>ModelRunner</code> with an endpoint and optional auth.</p>"},{"location":"models/api_models/#configure-api-model","title":"Configure API model","text":"<pre><code>from bench.evaluation.model_runner import ModelRunner\n\nmr = ModelRunner()\nmr.load_model(\n    \"api-demo\",\n    model_type=\"api\",\n    endpoint=\"https://api.example.com/v1/predict\",\n    api_key=\"sk_example\",          # optional; sent as Authorization: Bearer &lt;key&gt;\n    headers={\"X-Client\": \"MedAISure\"},  # merged with auth header\n    timeout=30.0,\n    max_retries=1,\n)\n</code></pre>"},{"location":"models/api_models/#requestresponse-format","title":"Request/response format","text":"<ul> <li>Input to <code>run_model(model_id, inputs)</code> is a list of dicts (<code>inputs: list[dict]</code>).</li> <li><code>ModelRunner</code> will POST JSON: <code>{ \"inputs\": [ ... ] }</code> to the configured <code>endpoint</code>.</li> <li>The API must return a JSON body with a list of outputs aligned 1:1, e.g. <code>{ \"outputs\": [ {\"answer\": \"...\"}, ... ] }</code>.</li> </ul> <p>Example call <pre><code>batch = [\n    {\"question\": \"What is BP?\"},\n    {\"question\": \"Define tachycardia.\"},\n]\nouts = mr.run_model(\"api-demo\", inputs=batch)\n# outs -&gt; list[dict], e.g., [{\"answer\": \"blood pressure\"}, {\"answer\": \"HR &gt; 100 bpm\"}]\n</code></pre></p>"},{"location":"models/api_models/#error-handling-retries","title":"Error handling &amp; retries","text":"<ul> <li>Non-2xx responses or malformed JSON raise an error in the runner.</li> <li><code>max_retries</code> uses simple retry with backoff for transient failures.</li> <li><code>timeout</code> applies per HTTP request.</li> </ul>"},{"location":"models/api_models/#security-notes","title":"Security notes","text":"<ul> <li>API keys are sent as <code>Authorization: Bearer &lt;api_key&gt;</code> by default; you can override via <code>headers</code>.</li> <li>Avoid logging raw keys; prefer environment variables in your invocation script.</li> </ul>"},{"location":"models/api_models/#harness-example","title":"Harness example","text":"<pre><code>from bench.evaluation.harness import EvaluationHarness\n\nh = EvaluationHarness(tasks=[\"medical_qa\"])  # or provide a MedicalTask\nh.evaluate(\n    model_id=\"api-demo\",\n    runner=mr,\n)\n</code></pre> <p>See also - Core API: <code>docs/api/core_api.md</code> - Python API \u2192 Model Runner: <code>api/reference.md#model-runner</code></p>"},{"location":"models/local_models/","title":"Local Models","text":"<p>Use local models with <code>ModelRunner</code> either via HuggingFace pipelines or a Python callable you provide.</p>"},{"location":"models/local_models/#huggingface-pipelines","title":"HuggingFace pipelines","text":"<p>Load a HF model with advanced options and generation kwargs (stored for later): <pre><code>from bench.evaluation.model_runner import ModelRunner\n\nmr = ModelRunner()\nmr.load_model(\n    \"hf-sum\",\n    model_type=\"huggingface\",\n    model_path=\"sshleifer/tiny-t5\",\n    hf_task=\"summarization\",\n    # Advanced options mapped to AutoModel/AutoTokenizer/pipe\n    device=-1,                 # CPU by default\n    device_map=\"auto\",        # GPU mapping if available\n    torch_dtype=\"auto\",       # or \"float16\", etc.\n    trust_remote_code=False,\n    low_cpu_mem_usage=True,\n    revision=None,\n    generation_kwargs={\"max_new_tokens\": 64, \"do_sample\": False},\n)\n\nouts = mr.run_model(\n    \"hf-sum\",\n    inputs=[{\"document\": \"Patient note ...\"}],\n    batch_size=2,\n)\n</code></pre></p> <p>Notes - <code>generation_kwargs</code> are automatically passed for <code>summarization</code>/<code>text-generation</code> pipelines. - Metadata like dtype/device may be captured for reporting where available.</p>"},{"location":"models/local_models/#python-callable-fully-local","title":"Python callable (fully local)","text":"<p>Implement and expose a loader that returns a callable <code>predict(batch, **kwargs)</code> or a simple function accepting <code>batch</code>: <pre><code># bench/examples/mypkg/mylocal.py\ndef load_model(model_path: str | None = None, **kwargs):\n    resources = {\"path\": model_path}\n\n    def _predict(batch, **_):\n        # batch: list[dict]; return list[dict] aligned 1:1\n        results = []\n        for item in batch:\n            if \"document\" in item:\n                results.append({\"summary\": item[\"document\"][:50]})\n            elif \"question\" in item:\n                results.append({\"answer\": \"placeholder\"})\n            else:\n                results.append({\"text\": \"ok\"})\n        return results\n\n    return _predict\n</code></pre></p> <p>Register and run it via <code>ModelRunner</code>: <pre><code>mr = ModelRunner()\nmr.load_model(\n    \"local-demo\",\n    model_type=\"local\",\n    module_path=\"bench.examples.mypkg.mylocal\",\n    model_path=\"/opt/models/demo\",\n    load_func=\"load_model\",  # optional if named load_model\n)\n\nouts = mr.run_model(\n    \"local-demo\",\n    inputs=[{\"question\": \"What is BP?\"}],\n)\n</code></pre></p>"},{"location":"models/local_models/#harness-example","title":"Harness example","text":"<pre><code>from bench.evaluation.harness import EvaluationHarness\n\nh = EvaluationHarness(tasks=[\"medical_qa\"])  # or provide a MedicalTask\nh.evaluate(\n    model_id=\"hf-sum\",\n    runner=mr,\n)\n</code></pre> <p>See also - Core API: <code>docs/api/core_api.md</code> - Python API \u2192 Model Runner: <code>api/reference.md#model-runner</code></p>"},{"location":"models/model_interface/","title":"Model Interface","text":"<p>All models should implement a consistent interface compatible with the Evaluation Harness.</p> <p>Key aspects: - Loading/configuration - Generate/invoke API - Metadata and resource management</p>"},{"location":"models/model_interface/#loading-models-with-modelrunner","title":"Loading models with ModelRunner","text":"<p><code>bench.evaluation.model_runner.ModelRunner</code> supports HuggingFace, local Python modules, simple HTTP APIs, and interface models.</p> <p>HuggingFace (pipeline) with advanced options <pre><code>from bench.evaluation.model_runner import ModelRunner\n\nmr = ModelRunner()\npipe = mr.load_model(\n    \"hf-sum\",\n    model_type=\"huggingface\",\n    model_path=\"sshleifer/tiny-t5\",\n    hf_task=\"summarization\",\n    # Advanced HF loading options\n    device=-1,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n    trust_remote_code=False,\n    low_cpu_mem_usage=True,\n    revision=None,\n    # Generation kwargs stored and later passed during run\n    generation_kwargs={\"max_new_tokens\": 64, \"do_sample\": False},\n)\n</code></pre></p> <p>Local Python module <pre><code>model = mr.load_model(\n    \"local-demo\",\n    model_type=\"local\",\n    model_path=\"/path/to/weights-or-dir\",\n    module_path=\"bench.examples.mypkg.mylocal\",\n    # optional: load_func=\"load_model\",\n)\n</code></pre></p> <p>Simple HTTP API <pre><code>cfg = mr.load_model(\n    \"api-demo\",\n    model_type=\"api\",\n    endpoint=\"https://api.example/v1/predict\",\n    api_key=\"sk-...\",\n    timeout=30.0,\n    max_retries=1,\n)\n</code></pre></p>"},{"location":"models/model_interface/#running-inference","title":"Running inference","text":"<p><code>run_model()</code> signature <pre><code>def run_model(\n    self,\n    model_id: str,\n    inputs: list[dict[str, Any]],\n    batch_size: int = 8,\n    **kwargs: Any,\n) -&gt; list[dict[str, Any]]:\n    ...\n</code></pre></p> <p>HuggingFace generation kwargs passthrough <pre><code># If generation_kwargs were supplied to load_model(...), they will be\n# passed to the pipeline call for summarization/text-generation tasks.\nout = mr.run_model(\n    \"hf-sum\",\n    inputs=[{\"document\": \"Patient note ...\"}],\n    batch_size=2,\n)\n</code></pre></p> <p>Local/API/interface models - Local: your callable returned by <code>load_model()</code> is invoked as <code>model(batch, **kwargs)</code>. - API: a generated callable posts <code>batch</code> to <code>endpoint</code> and parses response. - Interface: objects implementing <code>ModelInterface</code> can be registered and used via <code>.predict(batch)</code>.</p> <p>See source: <code>bench/evaluation/model_runner.py</code>.</p>"},{"location":"tasks/clinical_summarization/","title":"Clinical Summarization","text":"<p>Purpose: Generate succinct, clinically relevant summaries from long-form notes or multi-modal inputs.</p> <ul> <li>Input: progress notes, discharge summaries, consult notes</li> <li>Output: targeted summary (e.g., Problem List, Assessment &amp; Plan)</li> <li>Constraints: brevity, clinical salience, safety-critical details preserved</li> </ul> <p>Implementation - Task ID: clinical_summarization - Expected fields: {id, source_text, target_summary, metadata} - Style: bullet points preferred; avoid verbatim copy</p> <p>Evaluation - Content coverage and correctness (Clinical Accuracy) - Structure and prioritization (Reasoning Quality)</p> <p>See: <code>docs/metrics/clinical_accuracy.md</code></p>"},{"location":"tasks/diagnostic_reasoning/","title":"Diagnostic Reasoning","text":"<p>Purpose: Assess stepwise clinical reasoning from presentation to differential and final diagnosis.</p> <ul> <li>Input: clinical vignette (structured or free text)</li> <li>Output: reasoning steps + differential + most likely diagnosis</li> <li>Format: chain-of-thought optional; require a final explicit diagnosis line</li> </ul> <p>Implementation - Task ID: diagnostic_reasoning - Expected fields: {id, case_text, final_diagnosis, reasoning, metadata} - Optional: require structured sections (History, Exam, Labs, Imaging)</p> <p>Evaluation - Reasoning Quality (structure, clinical plausibility) - Clinical Accuracy (final diagnosis correctness)</p> <p>Guidelines - Avoid definitive claims when uncertainty remains - Include safety considerations and red flags</p> <p>See also: <code>docs/metrics/reasoning_quality.md</code></p>"},{"location":"tasks/medical_qa/","title":"Medical QA","text":"<p>Purpose: Evaluate short-form medical question answering across clinical knowledge domains.</p> <ul> <li>Input: question text, optional context</li> <li>Output: concise answer (text)</li> <li>Dataset examples: synthetic and curated clinical QA</li> <li>Scoring: see Metrics -&gt; Clinical Accuracy, Reasoning Quality</li> </ul> <p>Implementation - Task ID: medical_qa - Loader: see <code>bench/examples/</code> for templates - Expected fields: {id, prompt, answer, metadata}</p> <p>Evaluation Tips - Encourage direct, unambiguous answers - Prefer evidence-based statements with citations if available - Penalize hallucinations and unsafe advice</p> <p>References - Docs: <code>docs/usage.md</code>, <code>docs/metrics/clinical_accuracy.md</code></p>"},{"location":"tasks/overview/","title":"Tasks Overview","text":"<p>Overview of task types and how to add custom tasks.</p> <ul> <li>Medical QA</li> <li>Diagnostic Reasoning</li> <li>Clinical Summarization</li> </ul> <p>See also: <code>bench/examples/</code> and <code>docs/usage.md</code>.</p>"},{"location":"tasks/overview/#task-schema-medicaltask","title":"Task schema (MedicalTask)","text":"<p>Core schema lives in <code>bench/models/medical_task.py</code> with enum <code>TaskType</code> and model <code>MedicalTask</code>.</p> <p>TaskType values <pre><code>from bench.models.medical_task import TaskType\n\nlist(TaskType)\n# [TaskType.DIAGNOSTIC_REASONING, TaskType.QA, TaskType.SUMMARIZATION, TaskType.COMMUNICATION]\n</code></pre></p> <p>MedicalTask fields (essentials) <pre><code>from bench.models.medical_task import MedicalTask, TaskType\n\nt = MedicalTask(\n    task_id=\"qa-demo\",\n    task_type=TaskType.QA,\n    description=\"Answer short medical questions\",\n    inputs=[{\"question\": \"What is BP?\"}],\n    expected_outputs=[{\"answer\": \"blood pressure\"}],\n    metrics=[\"clinical_accuracy\"],\n    input_schema={\"required\": [\"question\"]},\n    output_schema={\"required\": [\"answer\"]},\n)\n</code></pre></p> <p>Validation rules (enforced by Pydantic validators) - inputs: non-empty list - metrics: non-empty, unique values (normalized spacing) - expected_outputs: if provided, length must match inputs (1:1) - input_schema/output_schema: dict with optional <code>required: [str, ...]</code></p> <p>Serialization helpers <pre><code>data = t.to_dict()\njson_s = t.to_json(indent=2)\nyaml_s = t.to_yaml()\nt.save(\"/tmp/task.yaml\")        # infers format from extension\nu = MedicalTask.from_file(\"/tmp/task.yaml\")\nprint(u.convert(\"json\").splitlines()[0])  # pretty JSON string\n</code></pre></p> <p>Lightweight inline dataset (optional) <pre><code>MedicalTask(\n    task_id=\"sum-1\",\n    task_type=TaskType.SUMMARIZATION,\n    inputs=[{\"document\": \"Patient note\"}],\n    expected_outputs=[{\"summary\": \"Short note\"}],\n    metrics=[\"clinical_relevance\"],\n    input_schema={\"required\": [\"document\"]},\n    output_schema={\"required\": [\"summary\"]},\n    dataset=[{\"input\": {\"document\": \"HPI...\"}, \"output\": {\"summary\": \"...\"}}],\n)\n</code></pre></p> <p>Example run - See <code>bench/examples/run_local_model.py</code> for an end-to-end run using <code>EvaluationHarness</code> and a local model.</p> <p>See also - Python API \u2192 Task schema: api/reference.md#task-schema</p>"}]}

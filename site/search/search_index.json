{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api_reference/","title":"API Reference (concise)","text":"<p>References for primary public classes. Read the code for full details.</p>"},{"location":"api_reference/#evaluationharness-benchevaluationharnesspy","title":"EvaluationHarness (<code>bench/evaluation/harness.py</code>)","text":"<ul> <li><code>__init__(tasks_dir, results_dir, cache_dir=None, log_level=\"INFO\", callbacks...)</code></li> <li><code>evaluate(model_id, task_ids, model_type=\"huggingface\", batch_size=8, use_cache=True, save_results=True, report_formats: Optional[List[str]] = None, report_dir: Optional[str] = None, **model_kwargs) -&gt; BenchmarkReport</code></li> <li><code>list_available_tasks() -&gt; List[dict]</code></li> <li><code>get_task_info(task_id) -&gt; dict</code></li> </ul> <p>Notes: - <code>evaluate()</code> loads the model via <code>ModelRunner.load_model()</code> then iterates tasks. - Metrics are computed via <code>MetricCalculator.calculate_metrics()</code>. - Results aggregated via <code>ResultAggregator.add_evaluation_result()</code> and returned as <code>BenchmarkReport</code>. - When <code>report_formats</code> is provided (e.g. <code>[\"html\", \"md\"]</code>), extra reports are generated and saved to <code>report_dir</code> (defaults to <code>results_dir</code>).</p> <p>Example \u2013 basic run: <pre><code>from bench.evaluation import EvaluationHarness\nh = EvaluationHarness(tasks_dir=\"bench/tasks\", results_dir=\"bench/results\")\ntasks = h.list_available_tasks()\nreport = h.evaluate(\n    model_id=\"textattack/bert-base-uncased-MNLI\",\n    task_ids=[tasks[0][\"task_id\"]],\n    model_type=\"huggingface\",\n    batch_size=4,\n    report_formats=[\"html\", \"md\"],\n    report_dir=\"bench/reports\",\n)\nprint(report.overall_scores)\n</code></pre></p> <p>Example \u2013 get task info: <pre><code>info = h.get_task_info(tasks[0][\"task_id\"])\nprint(info[\"name\"], info.get(\"metrics\"))\n</code></pre></p>"},{"location":"api_reference/#taskloader-benchevaluationtask_loaderpy","title":"TaskLoader (<code>bench/evaluation/task_loader.py</code>)","text":"<ul> <li><code>load_task(task_id: str) -&gt; MedicalTask</code> (task_id can be id, local file path, or HTTP(S) URL)</li> <li><code>load_tasks(task_ids: List[str]) -&gt; Dict[str, MedicalTask&gt;</code></li> <li><code>discover_tasks() -&gt; Dict[str, str]</code></li> <li><code>list_available_tasks() -&gt; List[dict]</code></li> </ul> <p>Examples: <pre><code>from bench.evaluation.task_loader import TaskLoader\ntl = TaskLoader(tasks_dir=\"bench/tasks\")\ntask = tl.load_task(\"clinical_summarization_discharge\")\nall_tasks = tl.list_available_tasks()\n</code></pre></p>"},{"location":"api_reference/#modelrunner-benchevaluationmodel_runnerpy","title":"ModelRunner (<code>bench/evaluation/model_runner.py</code>)","text":"<ul> <li><code>load_model(model_name, model_type=\"local\", model_path=None, **kwargs)</code></li> <li><code>model_type in {\"huggingface\", \"local\", \"api\"}</code></li> <li>HuggingFace options:<ul> <li><code>hf_task</code>: <code>text-classification | summarization | text-generation</code></li> <li><code>model_kwargs</code>, <code>tokenizer_kwargs</code>, <code>pipeline_kwargs</code>, <code>device</code>, <code>num_labels</code></li> <li>Generation: <code>generation_kwargs</code> (used in <code>run_model()</code> for summarization/text-generation)</li> <li>Advanced loading: <code>device_map</code>, <code>torch_dtype</code>, <code>low_cpu_mem_usage</code>, <code>revision</code>, <code>trust_remote_code</code></li> </ul> </li> <li>Local: requires <code>module_path</code>, optional <code>load_func</code> (default <code>load_model</code>)</li> <li>API: requires <code>endpoint</code>, <code>api_key</code>, optional <code>timeout</code>, <code>max_retries</code>, <code>backoff_factor</code>, <code>headers</code></li> <li><code>run_model(model_id, inputs, batch_size=8, **kwargs) -&gt; List[dict]</code></li> <li><code>unload_model(model_name)</code></li> </ul> <p>Examples: <pre><code>from bench.evaluation.model_runner import ModelRunner\nmr = ModelRunner()\n# HF summarization with generation params\nmr.load_model(\n    \"sshleifer/distilbart-cnn-12-6\",\n    model_type=\"huggingface\",\n    hf_task=\"summarization\",\n    generation_kwargs={\"max_new_tokens\": 128, \"temperature\": 0.7, \"do_sample\": True},\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n)\npreds = mr.run_model(\"sshleifer/distilbart-cnn-12-6\", inputs=[{\"text\": \"note\"}], batch_size=1)\n\n# Local\nmr.load_model(\"my_local\", model_type=\"local\", module_path=\"bench.examples.mypkg.mylocal\")\npreds = mr.run_model(\"my_local\", inputs=[{\"text\": \"x\"}], batch_size=2)\n</code></pre></p> <p>Returned prediction dicts typically include <code>{\"label\", \"score\"}</code> for classification or <code>{\"summary\"|\"text\"|\"prediction\"}</code> for generative tasks.</p> <p>See runnable examples: - <code>bench/examples/run_hf_summarization_gen.py</code> - <code>bench/examples/run_hf_text_generation_gen.py</code></p>"},{"location":"api_reference/#localmodel-benchevaluationmodel_interfacepy","title":"LocalModel (<code>bench/evaluation/model_interface.py</code>)","text":"<ul> <li><code>LocalModel(predict_fn: Optional[Callable]=None, model_path: Optional[str]=None, loader: Optional[Callable]=None, model_id: Optional[str]=None)</code></li> <li>Load local models directly from files or wrap a Python callable.</li> <li>Auto-loaders by extension:<ul> <li><code>.pkl</code>/<code>.pickle</code> \u2192 <code>pickle.load</code></li> <li><code>.joblib</code> \u2192 <code>joblib.load</code> (optional dependency)</li> <li><code>.pt</code>/<code>.pth</code> \u2192 <code>torch.load</code> (optional dependency)</li> </ul> </li> <li>Prediction: calls the model object (if callable) or its <code>.predict()</code> method; otherwise uses <code>predict_fn</code>.</li> <li><code>metadata</code> includes <code>file_path</code>, <code>ext</code>, <code>file_size</code>, <code>file_mtime</code>, <code>object_class</code>, <code>object_module</code>, and inferred <code>framework</code>.</li> </ul> <p>Examples (runnable scripts):</p> <pre><code>python bench/examples/run_localmodel_pickle.py\npython bench/examples/run_localmodel_joblib.py   # requires joblib\npython bench/examples/run_localmodel_torch.py    # requires torch\n</code></pre> <p>Inline usage \u2013 Pickle: <pre><code>from bench.evaluation.model_interface import LocalModel\nlm = LocalModel(model_path=\".local_models/echo.pkl\")\npreds = lm.predict([{\"text\": \"hello\"}, {\"text\": \"world\"}])\nprint(preds, lm.metadata)\n</code></pre></p> <p>Inline usage \u2013 Joblib: <pre><code>from bench.evaluation.model_interface import LocalModel\nlm = LocalModel(model_path=\".local_models/echo.joblib\")\npreds = lm.predict([{\"a\": 1}, {\"bbb\": 2}])\nprint(preds)\n</code></pre></p> <p>Inline usage \u2013 Torch: <pre><code>from bench.evaluation.model_interface import LocalModel\nlm = LocalModel(model_path=\".local_models/echo.pt\")\npreds = lm.predict([{\"x\": 0.1}, {\"x\": 0.9}])\nprint(preds)\n</code></pre></p>"},{"location":"api_reference/#metriccalculator-benchevaluationmetric_calculatorpy","title":"MetricCalculator (<code>bench/evaluation/metric_calculator.py</code>)","text":"<ul> <li>Built-ins: <code>accuracy</code>, <code>precision</code>, <code>recall</code>, <code>f1</code>, <code>roc_auc</code>, <code>average_precision</code>, <code>mse</code>, <code>mae</code>, <code>r2</code></li> <li>Medical: <code>diagnostic_accuracy</code>, <code>clinical_correctness</code>, <code>reasoning_quality</code>, <code>rouge_l</code>, <code>clinical_relevance</code>, <code>factual_consistency</code></li> <li><code>register_metric(name, fn, **default_kwargs)</code></li> <li><code>calculate_metrics(task_id, predictions, references, metric_names=None, **metric_kwargs) -&gt; Dict[str, MetricResult]</code></li> <li><code>aggregate_metrics(metric_results, aggregation=\"mean\") -&gt; Dict[str, MetricResult]</code></li> </ul> <p>Examples: <pre><code>from bench.evaluation.metric_calculator import MetricCalculator\nmc = MetricCalculator()\n\npreds = [{\"label\": \"yes\"}, {\"label\": \"no\"}]\nrefs = [{\"label\": \"yes\"}, {\"label\": \"no\"}]\nres = mc.calculate_metrics(\"demo\", preds, refs, metric_names=[\"accuracy\"])\n\ndef exact_match(y_true, y_pred, **kw):\n    return float(sum(int(t == p) for t, p in zip(y_true, y_pred)) / len(y_true))\nmc.register_metric(\"exact_match\", exact_match)\nres2 = mc.calculate_metrics(\"demo\", preds, refs, metric_names=[\"exact_match\"])\n</code></pre></p>"},{"location":"api_reference/#resultaggregator-benchevaluationresult_aggregatorpy","title":"ResultAggregator (<code>bench/evaluation/result_aggregator.py</code>)","text":"<ul> <li><code>add_evaluation_result(EvaluationResult, run_id=None)</code></li> <li><code>get_report(run_id) -&gt; BenchmarkReport</code></li> <li><code>export_report_json|csv|markdown|html(run_id, output)</code></li> <li><code>aggregate_statistics(run_id, metrics=None, percentiles=None, tasks=None)</code></li> <li><code>filter_and_sort_tasks(run_id, tasks=None, metrics=None, sort_by=None, descending=True)</code></li> <li><code>compare_runs(run_a, run_b, metrics=None, relative=False)</code></li> </ul> <p>Examples: <pre><code>from bench.evaluation.result_aggregator import ResultAggregator\nfrom bench.models import EvaluationResult\n\nra = ResultAggregator(output_dir=\"bench/results\")\ner = EvaluationResult(\n    model_id=\"demo\",\n    task_id=\"task1\",\n    inputs=[{\"text\": \"a\"}],\n    model_outputs=[{\"label\": \"y\"}],\n    metrics_results={},\n)\nra.add_evaluation_result(er, run_id=\"run-1\")\nreport = ra.get_report(\"run-1\")\nra.export_report_json(\"run-1\", output_path=\"bench/results/run-1.json\")\n</code></pre></p>"},{"location":"api_reference/#data-models-benchmodels","title":"Data Models (<code>bench/models/</code>)","text":"<ul> <li><code>MedicalTask</code>: fields include <code>task_id</code>, <code>task_type</code>, <code>name</code>, <code>description</code>, <code>inputs</code>, <code>expected_outputs</code>, <code>metrics</code>, <code>input_schema</code>, <code>output_schema</code>, <code>dataset</code></li> <li><code>EvaluationResult</code>: <code>model_id</code>, <code>task_id</code>, <code>inputs</code>, <code>model_outputs</code>, <code>metrics_results</code>, <code>metadata</code>, <code>timestamp</code></li> <li><code>BenchmarkReport</code>: overall/task scores + <code>detailed_results</code>, JSON save/load helpers</li> </ul>"},{"location":"api_reference/#advanced-serialization-models","title":"Advanced Serialization (Models)","text":"<ul> <li> <p><code>schema_version</code>: int, default <code>1</code> on all models for forward compatibility</p> </li> <li> <p>Partial serialization (all models):</p> </li> <li><code>to_dict(include=None, exclude=None)</code></li> <li> <p><code>to_json(indent=None, include=None, exclude=None)</code></p> </li> <li> <p>YAML support (all models):</p> </li> <li><code>to_yaml()</code> \u2192 YAML string</li> <li> <p><code>from_yaml(text: str)</code> \u2192 instance</p> </li> <li> <p>File I/O and conversion (all models):</p> </li> <li><code>save(path, format=None)</code> \u2192 format inferred from extension (<code>.json</code>, <code>.yaml</code>, <code>.yml</code>) or by <code>format</code></li> <li><code>from_file(path)</code> \u2192 loads JSON/YAML by extension</li> <li> <p><code>convert(to)</code> \u2192 returns string in <code>json</code> or <code>yaml</code></p> </li> <li> <p>CSV helpers:</p> </li> <li><code>EvaluationResult</code>: <code>inputs_to_csv()</code>, <code>outputs_to_csv()</code>, <code>from_inputs_csv(model_id, task_id, csv_text)</code>, <code>from_outputs_csv(model_id, task_id, csv_text)</code></li> <li><code>MedicalTask</code>: <code>dataset_to_csv()</code></li> <li><code>BenchmarkReport</code>: <code>overall_scores_to_csv()</code>, <code>task_scores_to_csv()</code></li> </ul> <p>Examples: <pre><code>from bench.models import MedicalTask, EvaluationResult, BenchmarkReport\n\n# YAML round-trip\ntask_yaml = task.to_yaml()\ntask2 = MedicalTask.from_yaml(task_yaml)\n\n# Partial JSON (only overall_scores)\nreport_json = report.to_json(indent=2, include={\"overall_scores\": True})\n\n# Save/Load\nresult.save(\"res.yaml\")\nres2 = EvaluationResult.from_file(\"res.yaml\")\n\n# CSV exports\ncsv_inputs = res2.inputs_to_csv()\ncsv_overall = report.overall_scores_to_csv()\n\n## CLI (Typer) \u2013 Evaluate\n\nFile: `bench/cli_typer.py`\n\n- `--tasks &lt;id&gt;`: select tasks (repeatable)\n- `--output-dir &lt;path&gt;`: base report directory\n- `--format &lt;fmt&gt;`: base report format (`json|yaml|md|csv`)\n- `--extra-report &lt;fmt&gt;`: extra report export (repeatable). Example: `--extra-report html --extra-report md`\n- `--report-dir &lt;path&gt;`: directory for extra reports (defaults to `--output-dir`)\n- `--config-file &lt;path&gt;`: JSON/YAML config (see `docs/configuration.md`)\n - `--html-open-metadata [true|false]`: open Metadata sections by default in generated HTML reports\n - `--html-preview-limit &lt;int&gt;`: max items to show in inputs/outputs previews in HTML before truncation\n\nExample:\n\n```bash\n# Base JSON report + extra HTML/MD exports into custom directory\nMEDAISURE_NO_RICH=1 python -m bench.cli_typer evaluate textattack/bert-base-uncased-MNLI \\\n  --tasks clinical_icd10_classification \\\n  --output-dir results \\\n  --format json \\\n  --extra-report html --extra-report md \\\n  --report-dir reports \\\n  --html-open-metadata true \\\n  --html-preview-limit 10\n</code></pre></p>"},{"location":"api_reference/#supported-formats-extra-reports","title":"Supported formats (extra reports)","text":"<ul> <li>json</li> <li>md (markdown)</li> <li>html</li> </ul> <p>Reference implementation: <code>ReportFactory</code> in <code>bench/reports/factory.py</code>.</p> <p>Environment variables affecting HTML rendering:</p> <ul> <li><code>MEDAISURE_HTML_OPEN_METADATA</code> = <code>\"1\"</code> to open Metadata sections by default</li> <li><code>MEDAISURE_HTML_PREVIEW_LIMIT</code> = integer truncation limit for list previews (default 5)</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>This document provides a high-level overview of MedAISure's architecture.</p> <ul> <li>Core components: Task Loader, Model Runner, Metric Calculator, Result Aggregator, Evaluation Harness</li> <li>Data flow: tasks -&gt; model execution -&gt; metrics -&gt; aggregation -&gt; reports</li> </ul> <p>Refer to API Reference for detailed class and method documentation.</p>"},{"location":"configuration/","title":"Configuration","text":"<ul> <li>MEDAISURE_NO_RICH</li> <li>Purpose: Disable Rich live console rendering during tests/CI, and force plain stdout output for both text and JSON from the Typer CLI in <code>bench/cli_typer.py</code>.</li> <li>Values: <code>\"1\"</code> (enabled) or unset/other (disabled).</li> <li>Behavior when set to <code>1</code>:<ul> <li>Rich status/progress bars are not created (<code>_status()</code> becomes no-op).</li> <li><code>_print()</code> writes plain text to <code>sys.stdout</code> (captured by pytest/CliRunner).</li> <li><code>_print_json()</code> always writes raw JSON to <code>sys.stdout</code> (no ANSI styling), ensuring robust parsing in tests.</li> <li>Python logging is disabled via <code>logging.disable(logging.CRITICAL)</code> to avoid noisy writes to captured streams.</li> </ul> </li> <li>Recommended usage:<ul> <li>Local runs of the CLI can omit it for nicer output.</li> <li>Enable for pytest and CI runs to avoid flaky captures: <code>export MEDAISURE_NO_RICH=1</code>.</li> </ul> </li> <li>CI: The default GitHub Actions workflow sets this for the test job (see <code>.github/workflows/tests.yml</code>).</li> </ul>"},{"location":"configuration/#configuration-reference","title":"Configuration Reference","text":"<p>Key parameters and where they apply.</p>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>MEDAISURE_NO_RICH</code>: Disable Rich live console rendering during tests/CI, and force plain stdout output for both text and JSON from the Typer CLI in <code>bench/cli_typer.py</code>.</li> <li><code>MEDAISURE_HTML_OPEN_METADATA</code>: When set to <code>\"1\"</code>, HTML reports open the Metadata section by default; otherwise collapsed.</li> <li><code>MEDAISURE_HTML_PREVIEW_LIMIT</code>: Integer. Max number of list items to show in HTML previews for <code>inputs</code>/<code>model_outputs</code> before truncation. Default <code>5</code>.</li> </ul>"},{"location":"configuration/#evaluationharness","title":"EvaluationHarness","text":"<ul> <li><code>tasks_dir</code>: where task YAML/JSON files are discovered</li> <li><code>results_dir</code>: where <code>&lt;run_id&gt;.json</code> reports and exports are written</li> <li><code>cache_dir</code>: if set, caches per-task predictions for reuse</li> <li><code>log_level</code>: python logging level string</li> <li>callbacks: <code>on_task_start</code>, <code>on_task_end</code>, <code>on_progress</code>, <code>on_error</code>, <code>on_metrics</code></li> </ul>"},{"location":"configuration/#modelrunner-hugging-face","title":"ModelRunner (Hugging Face)","text":"<ul> <li><code>hf_task</code>: <code>text-classification</code> (default), <code>summarization</code> (alias of <code>text2text-generation</code>), <code>text-generation</code></li> <li><code>model_kwargs</code>: passed to <code>AutoModel.from_pretrained</code></li> <li><code>tokenizer_kwargs</code>: passed to <code>AutoTokenizer.from_pretrained</code></li> <li><code>pipeline_kwargs</code>: passed to <code>transformers.pipeline</code></li> <li><code>device</code>: -1 (CPU) or CUDA index</li> <li><code>num_labels</code>: classification heads</li> </ul>"},{"location":"configuration/#modelrunner-local","title":"ModelRunner (Local)","text":"<ul> <li><code>model_path</code>: arbitrary path to your weights/artifacts</li> <li><code>module_path</code>: import path to your loader module</li> <li><code>load_func</code>: optional function name (default <code>load_model</code>)</li> </ul>"},{"location":"configuration/#modelrunner-api","title":"ModelRunner (API)","text":"<ul> <li><code>endpoint</code>: POST URL</li> <li><code>api_key</code>: bearer token used by <code>_call_api_model</code></li> <li>optional: <code>headers</code>, <code>timeout</code>, <code>max_retries</code>, <code>backoff_factor</code></li> </ul>"},{"location":"configuration/#metriccalculator","title":"MetricCalculator","text":"<ul> <li><code>register_metric(name, fn, **defaults)</code></li> <li><code>calculate_metrics(task_id, predictions, references, metric_names=None, **metric_kwargs)</code></li> <li><code>metric_kwargs</code> can be a mapping keyed by metric name for per-metric overrides</li> </ul>"},{"location":"configuration/#resultaggregator","title":"ResultAggregator","text":"<ul> <li><code>export_report_json|csv|markdown|html(run_id, output_path)</code></li> <li><code>aggregate_statistics(run_id, metrics=None, percentiles=None, tasks=None)</code></li> <li><code>filter_and_sort_tasks(run_id, tasks=None, metrics=None, sort_by=None, descending=True)</code></li> <li><code>compare_runs(run_a, run_b, metrics=None, relative=False)</code></li> </ul>"},{"location":"configuration/#benchmarkconfig-cli-config-file","title":"BenchmarkConfig (CLI config file)","text":"<p>You can provide defaults for CLI runs via a JSON/YAML config file and pass it with <code>--config-file</code> to the Typer CLI <code>evaluate</code> command.</p> <p>Fields (subset):</p> <ul> <li><code>model_id</code>: string</li> <li><code>tasks</code>: list of task IDs</li> <li><code>output_dir</code>: path for results</li> <li><code>output_format</code>: <code>json|yaml|md|csv</code></li> <li><code>model_type</code>: <code>huggingface|local|api</code></li> <li><code>batch_size</code>: int</li> <li><code>use_cache</code>: bool</li> <li><code>save_results</code>: bool</li> <li><code>extra_reports</code>: optional list of extra export formats, e.g. <code>[\"html\", \"md\"]</code></li> <li><code>report_dir</code>: optional path where extra reports are written (defaults to <code>output_dir</code>)</li> <li><code>html_open_metadata</code>: optional bool; if true, open Metadata sections by default in HTML reports</li> <li><code>html_preview_limit</code>: optional int; controls truncation length for inputs/outputs previews in HTML</li> </ul> <p>Example YAML:</p> <pre><code>model_id: textattack/bert-base-uncased-MNLI\ntasks: [clinical_icd10_classification]\noutput_dir: results\noutput_format: json\nmodel_type: huggingface\nbatch_size: 4\nuse_cache: true\nsave_results: true\nextra_reports: [html, md]\nreport_dir: reports\nhtml_open_metadata: true\nhtml_preview_limit: 10\n</code></pre>"},{"location":"configuration/#supported-formats-extra-reports","title":"Supported formats (extra reports)","text":"<ul> <li>json</li> <li>md (markdown)</li> <li>html</li> </ul> <p>Reference: extra reports are generated via <code>ReportFactory</code> in <code>bench/reports/factory.py</code>.</p> <p>CLI usage with config file:</p> <pre><code>MEDAISURE_NO_RICH=1 python -m bench.cli_typer evaluate textattack/bert-base-uncased-MNLI \\\n  --tasks clinical_icd10_classification \\\n  --config-file config.yaml\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>See the repository's CONTRIBUTING guide: https://github.com/junaidi-ai/MedAISure/blob/master/CONTRIBUTING.md</p> <p>Quick checklist: - Open an issue first for major changes - Follow PEP 8, add type hints and docstrings - Write/Update tests; ensure <code>pytest</code> passes - Run linters; ensure CI is green</p>"},{"location":"data_models/","title":"MedAISure Benchmark Data Models","text":"<p>This document provides detailed documentation for the core data models used in the MedAISure Benchmark system.</p>"},{"location":"data_models/#table-of-contents","title":"Table of Contents","text":"<ul> <li>MedicalTask</li> <li>EvaluationResult</li> <li>BenchmarkReport</li> <li>Usage Examples</li> <li>Serialization</li> </ul>"},{"location":"data_models/#medicaltask","title":"MedicalTask","text":"<p>Represents a medical task that a model needs to perform.</p>"},{"location":"data_models/#fields","title":"Fields","text":"<ul> <li><code>task_id</code> (str): Unique identifier for the task</li> <li><code>task_type</code> (TaskType): Type of the task (e.g., \"diagnostic_reasoning\", \"qa\")</li> <li><code>description</code> (str): Human-readable description of the task</li> <li><code>inputs</code> (List[Dict]): List of input examples for the task</li> <li><code>expected_outputs</code> (List[Dict]): List of expected outputs corresponding to inputs</li> <li><code>metrics</code> (List[str]): List of metric names to evaluate the task</li> </ul>"},{"location":"data_models/#example","title":"Example","text":"<pre><code>from bench.models import MedicalTask, TaskType\n\ntask = MedicalTask(\n    task_id=\"task_123\",\n    task_type=TaskType.QA,\n    description=\"Answer medical questions\",\n    inputs=[{\"question\": \"What are the symptoms of COVID-19?\"}],\n    expected_outputs=[{\"answer\": \"Common symptoms include fever, cough, and fatigue.\"}],\n    metrics=[\"accuracy\", \"f1_score\"]\n)\n</code></pre>"},{"location":"data_models/#evaluationresult","title":"EvaluationResult","text":"<p>Represents the result of evaluating a model on a specific task.</p>"},{"location":"data_models/#fields_1","title":"Fields","text":"<ul> <li><code>model_id</code> (str): Identifier for the evaluated model</li> <li><code>task_id</code> (str): Identifier of the task being evaluated</li> <li><code>inputs</code> (List[Dict]): Inputs used for evaluation</li> <li><code>model_outputs</code> (List[Dict]): Model's outputs for the given inputs</li> <li><code>metrics_results</code> (Dict[str, float]): Evaluation metrics and their values</li> <li><code>metadata</code> (Dict[str, Any]): Additional metadata about the evaluation</li> <li><code>timestamp</code> (datetime): When the evaluation was performed (auto-generated)</li> </ul>"},{"location":"data_models/#example_1","title":"Example","text":"<pre><code>from datetime import datetime, timezone\nfrom bench.models import EvaluationResult\n\nresult = EvaluationResult(\n    model_id=\"gpt-4\",\n    task_id=\"task_123\",\n    inputs=[{\"question\": \"What are the symptoms of COVID-19?\"}],\n    model_outputs=[{\"answer\": \"Symptoms include fever, cough, and fatigue.\"}],\n    metrics_results={\"accuracy\": 0.9, \"f1_score\": 0.85},\n    metadata={\"model_version\": \"1.0.0\"},\n    timestamp=datetime.now(timezone.utc)\n)\n</code></pre>"},{"location":"data_models/#benchmarkreport","title":"BenchmarkReport","text":"<p>Aggregates evaluation results across multiple tasks for a model.</p>"},{"location":"data_models/#fields_2","title":"Fields","text":"<ul> <li><code>model_id</code> (str): Identifier for the evaluated model</li> <li><code>timestamp</code> (datetime): When the benchmark was run (auto-generated)</li> <li><code>overall_scores</code> (Dict[str, float]): Aggregated scores across all tasks</li> <li><code>task_scores</code> (Dict[str, Dict[str, float]]): Scores for each task</li> <li><code>detailed_results</code> (List[EvaluationResult]): Individual evaluation results</li> <li><code>metadata</code> (Dict[str, Any]): Additional metadata about the benchmark</li> </ul>"},{"location":"data_models/#methods","title":"Methods","text":"<ul> <li><code>add_evaluation_result(result: EvaluationResult)</code>: Add a new evaluation result</li> <li><code>to_file(file_path: Union[str, Path])</code>: Save the report to a JSON file</li> <li><code>from_file(file_path: Union[str, Path]) -&gt; 'BenchmarkReport'</code>: Load a report from a JSON file</li> </ul>"},{"location":"data_models/#example_2","title":"Example","text":"<pre><code>from pathlib import Path\nfrom bench.models import BenchmarkReport\n\n# Create a new report\nreport = BenchmarkReport(\n    model_id=\"gpt-4\",\n    overall_scores={\"accuracy\": 0.85, \"f1_score\": 0.8},\n    task_scores={\n        \"task_1\": {\"accuracy\": 0.9, \"f1_score\": 0.85},\n        \"task_2\": {\"accuracy\": 0.8, \"f1_score\": 0.75}\n    },\n    metadata={\"run_id\": \"run_123\"}\n)\n\n# Add evaluation results\nreport.add_evaluation_result(result1)\nreport.add_evaluation_result(result2)\n\n# Save to file\nreport.to_file(\"benchmark_results.json\")\n\n# Load from file\nloaded_report = BenchmarkReport.from_file(\"benchmark_results.json\")\n</code></pre>"},{"location":"data_models/#usage-examples","title":"Usage Examples","text":""},{"location":"data_models/#creating-a-medical-task","title":"Creating a Medical Task","text":"<pre><code>from bench.models import MedicalTask, TaskType\n\ntask = MedicalTask(\n    task_id=\"diagnosis_001\",\n    task_type=TaskType.DIAGNOSTIC_REASONING,\n    description=\"Diagnose the most likely condition based on symptoms\",\n    inputs=[\n        {\n            \"symptoms\": [\"fever\", \"cough\", \"shortness of breath\"],\n            \"age\": 45,\n            \"gender\": \"M\"\n        }\n    ],\n    expected_outputs=[\n        {\n            \"diagnosis\": \"pneumonia\",\n            \"confidence\": 0.92,\n            \"differential_diagnosis\": [\"influenza\", \"bronchitis\"]\n        }\n    ],\n    metrics=[\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n)\n</code></pre>"},{"location":"data_models/#running-an-evaluation","title":"Running an Evaluation","text":"<pre><code>from bench.models import EvaluationResult\n\n# After running the model on the task\nmodel_outputs = [\n    {\n        \"diagnosis\": \"pneumonia\",\n        \"confidence\": 0.89,\n        \"differential_diagnosis\": [\"influenza\", \"bronchitis\", \"asthma\"]\n    }\n]\n\n# Calculate metrics (simplified example)\nmetrics = {\n    \"accuracy\": 1.0,  # Correct diagnosis\n    \"precision\": 0.9,\n    \"recall\": 0.85,\n    \"f1_score\": 0.87\n}\n\n# Create evaluation result\neval_result = EvaluationResult(\n    model_id=\"our-model-1.0\",\n    task_id=task.task_id,\n    inputs=task.inputs,\n    model_outputs=model_outputs,\n    metrics_results=metrics,\n    metadata={\n        \"model_version\": \"1.0.0\",\n        \"evaluation_time_seconds\": 2.5\n    }\n)\n</code></pre>"},{"location":"data_models/#serialization","title":"Serialization","text":"<p>All models support JSON serialization and deserialization:</p> <pre><code># To JSON string\njson_str = task.model_dump_json(indent=2)\n\n# From JSON string\nloaded_task = MedicalTask.model_validate_json(json_str)\n\n# To dictionary\ndata = task.model_dump()\n\n# From dictionary\nloaded_task = MedicalTask.model_validate(data)\n</code></pre>"},{"location":"data_models/#validation","title":"Validation","text":"<p>Models include built-in validation:</p> <pre><code>from pydantic import ValidationError\n\ntry:\n    invalid_task = MedicalTask(\n        task_id=\"\",  # Empty string not allowed\n        task_type=\"invalid_type\",  # Not a valid TaskType\n        inputs=[],  # Cannot be empty\n        expected_outputs=[{}],\n        metrics=[\"\"]  # Empty metric name not allowed\n    )\nexcept ValidationError as e:\n    print(f\"Validation error: {e}\")\n</code></pre>"},{"location":"data_models/#best-practices","title":"Best Practices","text":"<ol> <li>Always use the provided model classes instead of raw dictionaries</li> <li>Take advantage of built-in validation</li> <li>Use type hints for better IDE support and code clarity</li> <li>Store and share evaluation results using the serialization methods</li> <li>Include relevant metadata for traceability</li> </ol>"},{"location":"data_models/#task-type-schema-requirements","title":"Task-Type Schema Requirements","text":"<p>The evaluation framework applies minimal default schemas per <code>TaskType</code> when tasks do not provide explicit <code>input_schema</code>/<code>output_schema</code>. These defaults are enforced during task load and evaluation. Required keys are:</p> <ul> <li>QA</li> <li>Inputs require: <code>question</code></li> <li> <p>Outputs require: <code>answer</code></p> </li> <li> <p>Summarization</p> </li> <li>Inputs require: <code>document</code></li> <li> <p>Outputs require: <code>summary</code></p> </li> <li> <p>Diagnostic Reasoning</p> </li> <li>Inputs require: <code>symptoms</code></li> <li> <p>Outputs require: <code>diagnosis</code></p> </li> <li> <p>Communication</p> </li> <li>Inputs require: <code>prompt</code></li> <li>Outputs require: <code>response</code></li> </ul> <p>Notes: - These defaults are defined in <code>bench/evaluation/validators.py</code> under <code>DEFAULT_SCHEMAS</code> and are used by <code>ensure_task_schemas()</code>. - Inline datasets may use either flat rows or nested form <code>{ \"input\": { ... }, \"output\": { ... } }</code>. See <code>validate_task_dataset()</code> for details. - Strict validation mode in the harness will raise on schema violations; non-strict mode attaches validation errors to result metadata.</p>"},{"location":"data_models/#task-type-examples","title":"Task-Type Examples","text":"<p>Below are concise, runnable examples for each built-in task type.</p>"},{"location":"data_models/#qa","title":"QA","text":"<pre><code>from bench.models.task_types import MedicalQATask\n\ntask = MedicalQATask(\"qa-demo\")\ntask.dataset = [\n    {\"input\": {\"question\": \"What is BP?\"}, \"output\": {\"answer\": \"blood pressure\"}}\n]\nmetrics = task.evaluate([{ \"answer\": \"blood pressure\" }])  # {\"accuracy\": 1.0, \"clinical_correctness\": 1.0}\n</code></pre>"},{"location":"data_models/#diagnostic-reasoning","title":"Diagnostic Reasoning","text":"<pre><code>from bench.models.task_types import DiagnosticReasoningTask\n\ntask = DiagnosticReasoningTask(\"dx-demo\")\ntask.dataset = [\n    {\"input\": {\"case\": \"60M chest pain\"}, \"output\": {\"diagnosis\": \"ACS\"}}\n]\nmetrics = task.evaluate([{ \"diagnosis\": \"ACS\", \"explanation\": \"because ECG shows ST elevation\" }])\n</code></pre>"},{"location":"data_models/#summarization","title":"Summarization","text":"<pre><code>from bench.models.task_types import ClinicalSummarizationTask\n\ntask = ClinicalSummarizationTask(\"sum-demo\")\ntask.dataset = [\n    {\"input\": {\"document\": \"Patient with HTN and DM.\"}, \"output\": {\"summary\": \"HTN, DM.\"}}\n]\nmetrics = task.evaluate([{ \"summary\": \"HTN, DM.\" }])\n</code></pre>"},{"location":"data_models/#creating-custom-task-instances","title":"Creating Custom Task Instances","text":"<p>You can define your own tasks using <code>MedicalTask</code> (YAML/JSON) and validate/load them via the loader/validators.</p>"},{"location":"data_models/#minimal-yaml-example","title":"Minimal YAML Example","text":"<pre><code>schema_version: 1\ntask_id: qa-custom\ntask_type: qa\ndescription: Answer short questions\ninputs:\n  - { question: \"What is HR?\" }\nexpected_outputs:\n  - { answer: \"heart rate\" }\nmetrics: [accuracy]\ninput_schema:\n  required: [question]\noutput_schema:\n  required: [answer]\ndataset:\n  - input:  { question: \"What is BP?\" }\n    output: { answer: \"blood pressure\" }\n</code></pre>"},{"location":"data_models/#load-and-validate","title":"Load and Validate","text":"<pre><code>from bench.models.medical_task import MedicalTask\nfrom bench.evaluation.validators import validate_task_dataset\n\ntask = MedicalTask.from_file(\"qa-custom.yaml\")\nvalidate_task_dataset(task)  # raises if required keys missing\n</code></pre> <p>See also: - <code>bench/evaluation/task_loader.py</code> for loading tasks from files/URLs - <code>bench/evaluation/model_runner.py</code> for running models on tasks</p>"},{"location":"data_models/#task-specific-metrics","title":"Task-Specific Metrics","text":"<p>This framework ships with lightweight, task-specific metrics implemented in the task classes under <code>bench/models/task_types.py</code>. These are intentionally simple placeholders and should be replaced or extended for production use.</p> <ul> <li>QA (<code>MedicalQATask</code>)</li> <li><code>accuracy</code>: case-insensitive exact match of <code>answer</code></li> <li> <p><code>clinical_correctness</code>: proxy equal to <code>accuracy</code></p> </li> <li> <p>Diagnostic Reasoning (<code>DiagnosticReasoningTask</code>)</p> </li> <li><code>diagnostic_accuracy</code>: case-insensitive exact match of <code>diagnosis</code></li> <li> <p><code>reasoning_quality</code>: heuristic based on presence of explanation cues (e.g., \"because\", \"due to\") in <code>explanation</code>/<code>rationale</code></p> </li> <li> <p>Summarization (<code>ClinicalSummarizationTask</code>)</p> </li> <li><code>rouge_l</code>: unigram-overlap proxy (not true ROUGE-L)</li> <li><code>clinical_relevance</code>: overlap ratio with a small set of medical keywords</li> <li><code>factual_consistency</code>: penalizes hallucinated numbers not present in the reference</li> </ul> <p>For full implementations, consider integrating standard NLP metrics (e.g., official ROUGE, BERTScore) and clinical factuality checks.</p>"},{"location":"data_models/#end-to-end-usage","title":"End-to-End Usage","text":"<p>To see how tasks are loaded and models are executed over them, refer to:</p> <ul> <li><code>bench/evaluation/task_loader.py</code></li> <li>Key methods: <code>TaskLoader.load_task()</code>, <code>TaskLoader.load_tasks()</code>, <code>TaskLoader.list_available_tasks()</code></li> <li> <p>Supports loading by ID, local path, or HTTP(S) URL with validation</p> </li> <li> <p><code>bench/evaluation/model_runner.py</code></p> </li> <li>Key methods: <code>ModelRunner.load_model()</code>, <code>ModelRunner.run_model()</code> / <code>run_model_async()</code>, <code>ModelRunner.unload_model()</code></li> <li>Supports HuggingFace pipelines, local Python module models, and API-based models</li> </ul> <p>You can combine these to evaluate a model end-to-end:</p> <pre><code>from bench.evaluation.task_loader import TaskLoader\nfrom bench.evaluation.model_runner import ModelRunner\n\nloader = TaskLoader()\ntask = loader.load_task(\"bench/tasks/clinical_summarization_basic.yaml\")\n\nrunner = ModelRunner()\nrunner.load_model({\"type\": \"local\", \"path\": \"tests/fixtures/simple_local_model.py\", \"callable\": \"predict\"})\n\noutputs = runner.run_model(task.inputs)\nmetrics = task.evaluate(outputs)\n</code></pre>"},{"location":"data_models/#task-registry","title":"Task Registry","text":"<p>For dynamic registration, discovery, and filtered listing of tasks, use <code>bench/evaluation/task_registry.py</code>.</p> <ul> <li><code>TaskRegistry.register()</code> / <code>register_from_file()</code> / <code>register_from_url()</code></li> <li><code>TaskRegistry.get()</code> to retrieve by ID</li> <li><code>TaskRegistry.discover()</code> to scan a tasks directory</li> <li><code>TaskRegistry.list_available(task_type=..., min_examples=..., has_metrics=...)</code> for simple filtering</li> </ul> <p>Example:</p> <pre><code>from bench.evaluation.task_registry import TaskRegistry\nfrom bench.models.medical_task import TaskType\n\nreg = TaskRegistry(tasks_dir=\"bench/tasks\")\nreg.discover()\n\n# Filter to QA tasks that declare metrics and have &gt;= 1 example\nrows = reg.list_available(task_type=TaskType.QA, min_examples=1, has_metrics=True)\n\n# Load a specific task (from discovery or previously registered)\ntask = reg.get(rows[0].task_id)\n</code></pre>"},{"location":"docker/","title":"Docker Usage for MedAISure","text":"<p>This guide shows how to build and run MedAISure in Docker for both CPU and GPU environments.</p>"},{"location":"docker/#images","title":"Images","text":"<ul> <li>CPU: <code>Dockerfile</code> (Python 3.10 slim)</li> <li>GPU: <code>Dockerfile.gpu</code> (NVIDIA CUDA 11.8 runtime + cuDNN)</li> </ul> <p>Both images install Python deps from <code>requirements.txt</code> and use the CLI entrypoint <code>medaisure-benchmark</code> defined in <code>setup.py</code>.</p>"},{"location":"docker/#build","title":"Build","text":"<pre><code># CPU image\ndocker build -t medaisure/cpu:latest -f Dockerfile .\n\n# GPU image (requires NVIDIA Docker runtime)\ndocker build -t medaisure/gpu:latest -f Dockerfile.gpu .\n</code></pre>"},{"location":"docker/#cuda-enabled-pytorch-wheels","title":"CUDA-enabled PyTorch wheels","text":"<p>The GPU image pins CUDA 11.8 PyTorch wheels during build:</p> <pre><code>python3 -m pip install --index-url https://download.pytorch.org/whl/cu118 \\\n  torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n</code></pre> <p>If you adjust CUDA or PyTorch versions, ensure you use the matching index URL and compatible versions.</p>"},{"location":"docker/#run-docker","title":"Run (Docker)","text":"<pre><code># List tasks (CPU)\ndocker run --rm -v \"$PWD/data:/app/data\" -v \"$PWD/results:/app/results\" medaisure/cpu:latest list --json\n\n# List tasks (GPU)\ndocker run --rm --gpus all -v \"$PWD/data:/app/data\" -v \"$PWD/results:/app/results\" medaisure/gpu:latest list --json\n\n# Show a task (replace &lt;id&gt;)\ndocker run --rm medaisure/cpu:latest show &lt;id&gt;\n</code></pre>"},{"location":"docker/#gpu-smoke-test","title":"GPU Smoke Test","text":"<p>Run a quick CUDA sanity test inside the GPU image:</p> <pre><code>docker run --rm --gpus all medaisure/gpu:latest python3 scripts/gpu_smoke.py\n</code></pre>"},{"location":"docker/#run-docker-compose","title":"Run (Docker Compose)","text":"<pre><code># Build and run CPU service\ndocker compose up --build medaisure-cpu\n\n# Build and run GPU service (requires NVIDIA runtime)\ndocker compose up --build medaisure-gpu\n</code></pre> <p>Volumes: - <code>./data</code> is mounted to <code>/app/data</code> - <code>./results</code> is mounted to <code>/app/results</code></p> <p>To run only the GPU smoke service using profiles:</p> <pre><code>docker compose --profile smoke up --build medaisure-gpu-smoke\n</code></pre>"},{"location":"docker/#environment-variables","title":"Environment Variables","text":"<p>Create a local <code>.env</code> for API keys or configs and mount it if needed:</p> <pre><code>docker run --rm --env-file .env medaisure/cpu:latest list --json\n</code></pre> <p>Alternatively, pass specific vars:</p> <pre><code>docker run --rm -e HF_TOKEN=... medaisure/cpu:latest list --json\n</code></pre>"},{"location":"docker/#notes","title":"Notes","text":"<ul> <li>The entrypoint uses the console script <code>medaisure-benchmark</code> (from <code>bench/cli.py</code>). If you prefer a Python module entrypoint, you can run:   <pre><code>docker run --rm medaisure/cpu:latest python -m bench.cli list --json\n</code></pre></li> <li>GPU runs require a host with NVIDIA drivers and the NVIDIA Container Toolkit installed. Test with <code>docker run --rm --gpus all nvidia/cuda:11.8.0-base nvidia-smi</code>.</li> </ul>"},{"location":"extensions/","title":"Extension Guide","text":"<p>How to extend MedAISure with custom tasks, metrics, and models.</p>"},{"location":"extensions/#add-a-custom-task","title":"Add a Custom Task","text":"<p>1) Create <code>your_task.yaml</code> in your <code>tasks_dir</code> with fields used by <code>MedicalTask</code>: <pre><code>name: \"My Clinical NLI\"\ndescription: \"NLI over clinical text\"\ntask_type: qa\nmetrics: [accuracy, clinical_correctness]\ndataset:\n  - text: \"Patient denies chest pain.\"\n    hypothesis: \"The patient has chest pain.\"\n    label: contradiction\n</code></pre> 2) Load by id (file stem) or path: <pre><code>from bench.evaluation import EvaluationHarness\nh = EvaluationHarness(tasks_dir=\"bench/tasks\")\nreport = h.evaluate(model_id=\"textattack/bert-base-uncased-MNLI\", task_ids=[\"your_task\"], model_type=\"huggingface\")\n</code></pre></p>"},{"location":"extensions/#register-a-custom-metric","title":"Register a Custom Metric","text":"<p><pre><code>from bench.evaluation.metric_calculator import MetricCalculator\n\ndef my_metric(y_true, y_pred, **kwargs):\n    # return a float, or (float, metadata_dict)\n    return float(sum(int(t == p) for t, p in zip(y_true, y_pred)) / len(y_true))\n\nmc = MetricCalculator()\nmc.register_metric(\"my_metric\", my_metric)\n</code></pre> Use it by including <code>my_metric</code> in your task's <code>metrics:</code> or by passing <code>metric_names=[...]</code> to <code>calculate_metrics()</code> manually.</p>"},{"location":"extensions/#use-a-local-model","title":"Use a Local Model","text":"<p>Your local module should expose a loader (default name: <code>load_model</code>): <pre><code># mypkg/mylocal.py\nclass MyModel:\n    def __call__(self, batch, **kwargs):\n        # return list[dict] with keys like \"label\" or \"prediction\"\n        return [{\"label\": \"entailment\", \"score\": 0.9} for _ in batch]\n\ndef load_model(model_path, **kwargs):\n    return MyModel()\n</code></pre> Load via <code>ModelRunner</code> (automatically used by <code>EvaluationHarness.evaluate()</code>): <pre><code>report = h.evaluate(\n    model_id=\"my_local_model\",\n    task_ids=[\"your_task\"],\n    model_type=\"local\",\n    model_path=\"/path/to/artifacts\",\n    module_path=\"mypkg.mylocal\",\n)\n</code></pre></p>"},{"location":"extensions/#use-an-api-model","title":"Use an API Model","text":"<p>Provide <code>endpoint</code> and <code>api_key</code>. Predictions should be a list of dicts with <code>label</code>/<code>score</code> or <code>prediction/text/summary</code>. <pre><code>report = h.evaluate(\n    model_id=\"my_api\",\n    task_ids=[\"your_task\"],\n    model_type=\"api\",\n    endpoint=\"https://api.example.com/predict\",\n    api_key=os.environ[\"MY_API_KEY\"],\n)\n</code></pre></p> <p>See <code>bench/examples/</code> for runnable samples.</p>"},{"location":"getting_started/","title":"Getting Started with MedAISure","text":"<p>This project ships a Quick Start guide. For a concise intro, see Quick Start.</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<pre><code># From PyPI (recommended when published)\npip install medaisure-benchmark\n\n# Or from source\ngit clone https://github.com/junaidi-ai/MedAISure.git\ncd MedAISure\npip install -e .\n</code></pre>"},{"location":"getting_started/#cli-usage","title":"CLI Usage","text":"<pre><code># List available tasks\nmedaisure list-tasks\n\n# Evaluate a model on a task\nmedaisure evaluate my-model --tasks medical_qa_basic\n\n# Generate a report\nmedaisure generate-report ./results/my-model_results.json --format md\n</code></pre>"},{"location":"getting_started/#python-api","title":"Python API","text":"<pre><code>from bench.evaluation.harness import EvaluationHarness\nfrom bench.models import registry as model_registry\n\nh = EvaluationHarness()\nmodel = model_registry.load(\"gpt2\")\nreport = h.evaluate(model, tasks=[\"medical_qa_basic\"])\nprint(report.overall_scores)\n</code></pre>"},{"location":"metrics_guidelines/","title":"Metrics Guidelines","text":"<ul> <li>Centralized synonyms: Use the shared diagnosis label map <code>LABEL_SYNONYMS</code> for clinical abbreviations and common variants.</li> <li>Location: <code>bench/evaluation/metrics/clinical.py</code> (exported via <code>bench.evaluation.metrics</code>)</li> <li>Import example:     <pre><code>from bench.evaluation.metrics import LABEL_SYNONYMS\n# normalized = LABEL_SYNONYMS.get(_normalize_text(label), _normalize_text(label))\n</code></pre></li> <li>Why: Ensures consistent normalization across metrics (e.g., <code>mi \u2192 myocardial infarction</code>, <code>copd \u2192 chronic obstructive pulmonary disease</code>, <code>htn \u2192 hypertension</code>).</li> <li>Where to use:</li> <li>Anywhere you compare predicted/expected diagnoses or labels.</li> <li>Prefer mapping after lowercasing/basic punctuation cleanup (use <code>_normalize_text()</code> if available).</li> <li>Extending the map:</li> <li>Add new entries in <code>bench/evaluation/metrics/clinical.py</code> under the <code>LABEL_SYNONYMS</code> dict.</li> <li>Keep entries lowercased, punctuation-free keys and values.</li> <li>Group additions by domain (cardiology, pulmonary, etc.) with comments.</li> <li>Testing requirements:</li> <li>Add known-value tests in <code>tests/test_metrics_known_values.py</code> for each new synonym, asserting a 1.0 match for the normalized pair.</li> <li>Example:     <pre><code>def test_diagnostic_accuracy_htn_synonym_match():\n    m = DiagnosticAccuracyMetric()\n    expected = [{\"diagnosis\": \"hypertension\"}]\n    outputs = [{\"prediction\": \"htn\"}]\n    assert m.calculate(expected, outputs) == 1.0\n</code></pre></li> <li>Coordination with other metrics:</li> <li>If a metric relies on diagnosis-specific rules, consider normalizing inputs via <code>LABEL_SYNONYMS</code> before rule checks to reduce duplication.</li> <li>Avoid changing existing lexicon logic unless tests are added to cover behavior changes.</li> <li>Contribution tip:</li> <li>When proposing new synonyms, include short clinical justification or references if ambiguity is possible.</li> </ul>"},{"location":"metrics_testing/","title":"Metrics Testing Coverage and How to Run","text":"<p>This document summarizes the unit/property/performance tests added for metric implementations in <code>bench/evaluation/metrics/</code> and how to run them.</p>"},{"location":"metrics_testing/#implemented-tests","title":"Implemented Tests","text":"<ul> <li>Malformed/Unexpected Inputs</li> <li>File: <code>tests/test_metrics_malformed_inputs.py</code></li> <li>Covers resilience to missing keys, <code>None</code>, non-string values, and extras for:<ul> <li><code>ClinicalAccuracyMetric</code></li> <li><code>ReasoningQualityMetric</code></li> <li><code>DiagnosticAccuracyMetric</code></li> <li><code>ClinicalRelevanceMetric</code></li> </ul> </li> <li> <p>Also asserts list shape/type validation via <code>Metric.validate_inputs()</code>.</p> </li> <li> <p>Property-Based Tests (Hypothesis)</p> </li> <li>File: <code>tests/test_metrics_property_based.py</code></li> <li> <p>Generates randomized inputs of equal lengths; checks:</p> <ul> <li>Scores are floats in <code>[0, 1]</code></li> <li>Determinism for identical inputs (ReasoningQuality)</li> </ul> </li> <li> <p>Known-Values / Deterministic Scenarios</p> </li> <li>File: <code>tests/test_metrics_known_values.py</code></li> <li> <p>Asserts exact scores for simple deterministic cases (e.g., identical texts -&gt; 1.0, no overlap -&gt; 0.0).</p> </li> <li> <p>Performance Benchmarks (pytest-benchmark)</p> </li> <li>File: <code>tests/test_metrics_performance_benchmark.py</code></li> <li>Benchmarks runtime on ~1000-item batches for each metric and asserts valid score ranges.</li> </ul>"},{"location":"metrics_testing/#how-to-run","title":"How to Run","text":"<ul> <li> <p>Run only new tests quickly: <pre><code>pytest -q tests/test_metrics_malformed_inputs.py \\\n          tests/test_metrics_property_based.py \\\n          tests/test_metrics_known_values.py \\\n          tests/test_metrics_performance_benchmark.py\n</code></pre></p> </li> <li> <p>Run benchmarks with more iterations: <pre><code>pytest tests/test_metrics_performance_benchmark.py --benchmark-min-time=0.1\n</code></pre></p> </li> </ul>"},{"location":"metrics_testing/#notes","title":"Notes","text":"<ul> <li><code>DiagnosticAccuracyMetric</code> now normalizes the label <code>\"mi\"</code> to <code>\"myocardial infarction\"</code> to align synonyms before comparison.</li> <li>All tests keep metrics dependency-light and validate score normalization <code>[0, 1]</code>.</li> </ul>"},{"location":"performance/","title":"Performance Tips","text":"<ul> <li>Prefer batching via <code>batch_size</code> in <code>EvaluationHarness.evaluate()</code>; default is 8</li> <li>Use GPU for HF models by setting <code>device</code> (e.g., <code>device=0</code>) in <code>model_kwargs</code>/<code>pipeline_kwargs</code></li> <li>Cache predictions by providing <code>cache_dir</code>; reuse with <code>use_cache=True</code></li> <li>Minimize heavy metrics; some (e.g., ROUGE) are slower and optional</li> <li>Filter tasks to a subset during development</li> <li>For large runs, export CSV/Markdown and analyze externally</li> </ul>"},{"location":"quick_start/","title":"Quick Start","text":"<p>This guide helps you get up and running with MedAISure quickly using either local Python or Docker (CPU/GPU).</p>"},{"location":"quick_start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>Docker (optional but recommended for reproducibility)</li> <li>NVIDIA drivers + NVIDIA Container Toolkit (optional, for GPU)</li> </ul>"},{"location":"quick_start/#1-local-installation","title":"1) Local Installation","text":"<pre><code># Clone\ngit clone https://github.com/junaidi-ai/MedAISure.git\ncd MedAISure\n\n# Create and activate venv\npython -m venv .venv\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\n\n# Install package + dev extras\npip install -e .[dev]\n\n# (Optional) set env vars\ncp .env.example .env\n# edit .env as needed (API keys, etc.)\n</code></pre> <p>List available tasks via CLI:</p> <pre><code>medaisure-benchmark list --json\n</code></pre>"},{"location":"quick_start/#2-docker-cpu","title":"2) Docker (CPU)","text":"<pre><code># Build CPU image\ndocker build -t medaisure/cpu:latest -f Dockerfile .\n\n# List tasks\ndocker run --rm -v \"$PWD/data:/app/data\" -v \"$PWD/results:/app/results\" \\\n  medaisure/cpu:latest list --json\n</code></pre>"},{"location":"quick_start/#3-docker-gpu","title":"3) Docker (GPU)","text":"<pre><code># Build GPU image (requires NVIDIA Container Toolkit)\ndocker build -t medaisure/gpu:latest -f Dockerfile.gpu .\n\n# Quick GPU smoke test (sanity check)\ndocker run --rm --gpus all medaisure/gpu:latest \\\n  python3 scripts/gpu_smoke.py\n\n# List tasks with GPU image\ndocker run --rm --gpus all -v \"$PWD/data:/app/data\" -v \"$PWD/results:/app/results\" \\\n  medaisure/gpu:latest list --json\n</code></pre>"},{"location":"quick_start/#4-docker-compose","title":"4) Docker Compose","text":"<pre><code># CPU service\ndocker compose up --build medaisure-cpu\n\n# GPU service (requires NVIDIA runtime)\ndocker compose up --build medaisure-gpu\n\n# GPU smoke-only (profile)\ndocker compose --profile smoke up --build medaisure-gpu-smoke\n</code></pre>"},{"location":"quick_start/#cpu-vs-gpu-differences","title":"CPU vs GPU Differences","text":"<ul> <li>GPU image installs CUDA 11.8-enabled PyTorch wheels. Use <code>--gpus all</code> when running.</li> <li>CPU image uses generic wheels; it will run on systems without NVIDIA GPUs.</li> <li>Performance: GPU path accelerates tensor ops; see <code>scripts/gpu_smoke.py</code> for a quick timing check.</li> </ul>"},{"location":"quick_start/#volumes-data-best-practices","title":"Volumes &amp; Data Best Practices","text":"<ul> <li>Mount <code>./data</code> to <code>/app/data</code> for inputs and <code>./results</code> to <code>/app/results</code> for outputs.</li> <li>Keep large datasets outside of the image; prefer volume mounts.</li> <li>For reproducibility, keep a copy of configs under version control.</li> </ul>"},{"location":"quick_start/#environment-variables","title":"Environment Variables","text":"<ul> <li>Copy <code>.env.example</code> to <code>.env</code> and set any required keys.</li> <li>With Docker, pass <code>--env-file .env</code> or <code>-e KEY=value</code> for specific variables.</li> </ul>"},{"location":"quick_start/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>See docs/troubleshooting.md</li> <li>Verify Docker GPU support: <code>docker run --rm --gpus all nvidia/cuda:11.8.0-base nvidia-smi</code></li> <li>If GPU is not available, use the CPU image/commands.</li> </ul>"},{"location":"security/","title":"Secure Data Handling","text":"<p>This document explains how to use <code>SecureDataHandler</code> (in <code>bench/data/security.py</code>) to protect sensitive data, including algorithm selection, PHI anonymization, field-level policies, audit logging, and key rotation.</p>"},{"location":"security/#quick-start","title":"Quick Start","text":"<pre><code>from bench.data.security import SecureDataHandler\n\nhandler = SecureDataHandler(\"secret-key\")  # Fernet by default\ncipher = handler.encrypt_data({\"name\": \"Alice\", \"age\": 30})\nplain = handler.decrypt_data(cipher)\n</code></pre> <ul> <li>Only string values are encrypted/decrypted; non-strings pass through.</li> <li>If no key is provided, encrypt/decrypt are no-ops.</li> </ul>"},{"location":"security/#algorithm-selection","title":"Algorithm Selection","text":"<p>Supported algorithms: - <code>fernet</code> (default) \u2014 simple, URL-safe tokens, includes integrity. - <code>aes-gcm</code> \u2014 AEAD with GCM; encodes <code>nonce+ciphertext</code> in URL-safe base64. - <code>chacha20</code> \u2014 AEAD ChaCha20-Poly1305; encodes <code>nonce+ciphertext</code> in URL-safe base64.</p> <pre><code>SecureDataHandler(\"pass\", algorithm=\"aes-gcm\")\nSecureDataHandler(\"pass\", algorithm=\"chacha20\")\n</code></pre>"},{"location":"security/#field-level-policies","title":"Field-Level Policies","text":"<p>Control which fields are processed (encrypted/decrypted):</p> <pre><code># Only encrypt these fields\nhandler = SecureDataHandler(\"pass\", include_fields={\"name\", \"email\"})\n\n# Encrypt all string fields except these\nhandler = SecureDataHandler(\"pass\", exclude_fields={\"public_note\"})\n</code></pre> <p><code>include_fields</code> takes precedence over <code>exclude_fields</code>.</p>"},{"location":"security/#phi-detection-anonymization","title":"PHI Detection &amp; Anonymization","text":"<p>Enable anonymization to automatically redact likely PHI before encryption:</p> <pre><code>handler = SecureDataHandler(\"pass\", anonymize=True)\nredacted_then_encrypted = handler.encrypt_data({\n    \"msg\": \"Contact john.doe@example.com or 555-123-4567 on 2024-01-31\",\n})\n</code></pre> <ul> <li>Emails, phone numbers, SSN-like patterns, MRN-like IDs, and <code>YYYY-MM-DD</code> dates are detected.</li> <li>Redaction is deterministic, producing tokens like <code>[REDACTED:ABCDEFGH]</code>.</li> <li>You can also call <code>anonymize_data()</code> directly.</li> </ul>"},{"location":"security/#compliance-audit-logging","title":"Compliance Audit Logging","text":"<p>Record audits to JSONL for encrypt/decrypt/anonymize/rotate:</p> <pre><code>handler = SecureDataHandler(\"pass\", audit_log_path=\"audit.jsonl\")\nhandler.encrypt_data({\"x\": \"1\"})\nhandler.decrypt_data({\"x\": \"...\"})\nhandler.anonymize_data({\"x\": \"...\"})\nhandler.rotate_key(\"new-pass\")\n</code></pre> <p>Each line contains a JSON object with: - <code>ts</code> (ms timestamp) - <code>action</code> (encrypt|decrypt|anonymize|rotate_key) - <code>alg</code>, <code>kid</code> (current key id), and action <code>details</code></p> <p>If logging fails, operations continue (best-effort logging).</p>"},{"location":"security/#key-management-rotation","title":"Key Management &amp; Rotation","text":"<p>Create with an initial key (derived from passphrase). Rotate to a new key while retaining old keys for decryption:</p> <pre><code>handler = SecureDataHandler(\"pass\", algorithm=\"aes-gcm\")\nold_ct = handler.encrypt_data({\"s\": \"one\"})\nhandler.rotate_key(\"new-pass\")  # new key used for future encryptions\nplain_old = handler.decrypt_data(old_ct)  # still decrypts\n</code></pre> <ul> <li>Keys are referenced by <code>kid</code> (e.g., <code>k0</code>, <code>k1</code>, ...).</li> <li>Current key is preferred for decryption; fallbacks attempt older keys.</li> </ul>"},{"location":"security/#batch-helpers","title":"Batch Helpers","text":"<p>For large datasets, use batch helpers to avoid per-item overhead:</p> <pre><code>enc_list = handler.encrypt_batch(list_of_dicts)\nplain_list = handler.decrypt_batch(enc_list)\n</code></pre>"},{"location":"security/#operational-guidance","title":"Operational Guidance","text":"<ul> <li>Choose <code>fernet</code> for simplicity and compatibility; <code>aes-gcm</code> or <code>chacha20</code> for AEAD performance.</li> <li>Use <code>include_fields</code> to explicitly protect PII/PHI fields and avoid unexpected ciphertext expansion.</li> <li>Enable <code>anonymize=True</code> when ingesting free-text inputs that may contain PHI.</li> <li>Configure <code>audit_log_path</code> in environments requiring auditability. Persist logs to secure storage.</li> <li>Rotate keys periodically with <code>rotate_key()</code>. Keep audit logs of rotations and store passphrases in a secure secret manager.</li> <li>Test representative payloads. Some tokens (e.g., long base64) can expand storage size.</li> <li>Be cautious with double-encrypting fields. The handler attempts best-effort decryption and will pass through unknown/plaintext strings.</li> </ul>"},{"location":"security/#references","title":"References","text":"<ul> <li>Implementation: <code>bench/data/security.py</code></li> <li>Usage in data connectors: <code>bench/data/local.py</code></li> <li>Tests: <code>tests/test_security_handler.py</code></li> </ul>"},{"location":"testing/","title":"Testing Guide","text":"<p>This project includes a comprehensive testing suite for dataset connectors and related utilities.</p>"},{"location":"testing/#running-tests","title":"Running Tests","text":"<ul> <li>Local: <code>pytest -q</code></li> <li>With coverage (default via pytest.ini): <code>pytest -v</code></li> <li>Generate HTML coverage report: opens automatically at <code>htmlcov/index.html</code>.</li> </ul> <p>CI runs tests on each push/PR and uploads the HTML coverage report as an artifact named <code>coverage-html</code>.</p>"},{"location":"testing/#test-structure","title":"Test Structure","text":"<ul> <li><code>tests/test_data_connectors.py</code>: Unit tests for <code>JSONDataset</code> and <code>CSVDataset</code> load paths (plain, gzip, zip), validation, and batching.</li> <li><code>tests/test_connector_encrypted_local_datasets.py</code>: Ensures encrypted JSON/CSV files decrypt end-to-end when an <code>encryption_key</code> is provided.</li> <li><code>tests/test_medical_connectors.py</code>: Unit tests for <code>MIMICConnector</code> (SQLite) and <code>PubMedConnector</code> with mocked HTTP responses.</li> <li><code>tests/test_mimic_performance_and_redaction.py</code>: Integration/performance test for <code>MIMICConnector</code> including PHI redaction and cache acceleration check.</li> <li><code>tests/test_security_handler.py</code>: Security-focused tests for <code>SecureDataHandler</code> including algorithms, include/exclude fields, anonymization, audit logging, key rotation, and batch helpers.</li> <li><code>tests/test_encrypted_negative_cases.py</code>: Negative-path tests for encrypted inputs, including corrupted ciphertexts, wrong keys, mixed partially encrypted rows, and truncated base64 tokens.</li> <li><code>tests/test_large_batched_reads.py</code>: Large-scale batched read tests for JSON/CSV with plain, gzip, and zip variants; parameterized batch sizes and CI-safe performance bounds.</li> </ul> <p>Additional end-to-end and performance tests exist across the suite (models, metrics, harness).</p>"},{"location":"testing/#fixtures-and-factories","title":"Fixtures and Factories","text":"<p>Defined in <code>tests/conftest.py</code>:</p> <ul> <li><code>sample_json_file</code>, <code>sample_csv_file</code>: Create simple local datasets.</li> <li><code>encrypted_json_file</code>, <code>encrypted_csv_file</code>: Create encrypted datasets using <code>SecureDataHandler(\"test-pass\")</code> for end-to-end decryption tests.</li> <li>Existing <code>temp_tasks_dir</code>, <code>example_task_definition</code> support task-related tests.</li> </ul>"},{"location":"testing/#external-services-and-mocking","title":"External Services and Mocking","text":"<ul> <li>PubMed API is mocked in unit tests via <code>monkeypatch</code>.</li> <li>A live smoke test exists in <code>tests/test_pubmed_live.py</code> guarded by <code>RUN_PUBMED_LIVE=1</code> and optional <code>NCBI_API_KEY</code>.</li> <li>Databases are simulated using temporary SQLite files created in tests.</li> </ul>"},{"location":"testing/#security-considerations","title":"Security Considerations","text":"<ul> <li>PHI redaction is validated in both connectors and security handler tests.</li> <li>Encryption/decryption is tested across multiple algorithms; audit logging and key rotation are verified.</li> </ul>"},{"location":"testing/#performance-benchmarks","title":"Performance Benchmarks","text":"<ul> <li>Lightweight performance checks exist for connectors and core components.</li> <li>Run <code>pytest -k perf -q</code> to filter performance-oriented tests.</li> <li>Large batched read tests enforce modest time upper bounds tuned for CI to catch regressions without flakiness.</li> </ul>"},{"location":"testing/#coverage","title":"Coverage","text":"<ul> <li>Pytest is configured to collect coverage for <code>bench/*</code> and produce terminal and HTML reports.</li> <li>CI uploads <code>htmlcov</code> so you can review annotated coverage on PRs.</li> </ul>"},{"location":"testing/#tips","title":"Tips","text":"<ul> <li>Use <code>-k &lt;expr&gt;</code> to focus on a subset, e.g., <code>pytest -k medical -q</code>.</li> <li>Use <code>-m integration</code> to include integration tests; mark heavy tests with <code>@pytest.mark.integration</code>.</li> <li>CI splits unit vs. integration suites: the main job excludes <code>integration</code>, while a separate job runs only <code>integration</code> tests.</li> </ul>"},{"location":"testing_guide/","title":"MedAISure Testing Guide","text":"<p>This document summarizes the test coverage added for the core data models under <code>bench/models/</code> and how to run them.</p>"},{"location":"testing_guide/#scope-covered","title":"Scope Covered","text":"<ul> <li>MedicalTask (<code>bench/models/medical_task.py</code>)</li> <li>EvaluationResult (<code>bench/models/evaluation_result.py</code>)</li> <li>BenchmarkReport (<code>bench/models/benchmark_report.py</code>)</li> </ul>"},{"location":"testing_guide/#test-categories","title":"Test Categories","text":"<ul> <li>Property-based tests: <code>tests/test_models_property_based_test.py</code></li> <li>Randomized generation validates model constraints and JSON round-trips.</li> <li>Serialization round-trip tests: <code>tests/test_models_serialization_roundtrip_test.py</code></li> <li>Validates <code>to_dict</code>/<code>from_dict</code>, <code>to_json</code>/<code>from_json</code>, <code>to_yaml</code>/<code>from_yaml</code>, and file I/O helpers.</li> <li>Verifies CSV helpers where applicable.</li> <li>Edge case tests: <code>tests/test_models_edge_cases_test.py</code></li> <li>Invalid inputs, boundary conditions, timezone normalization, metric validation strictness.</li> <li>Performance smoke tests: <code>tests/test_models_performance_test.py</code></li> <li>Aggregation throughput for <code>BenchmarkReport.add_evaluation_result</code>.</li> <li>JSON/YAML serialization throughput for <code>BenchmarkReport</code>.</li> <li>Integration tests: <code>tests/test_models_integration_test.py</code></li> <li><code>EvaluationResult.validate_against_task()</code> with <code>MedicalTask</code> schemas.</li> <li><code>BenchmarkReport.validate_against_tasks()</code> and aggregate correctness across tasks.</li> </ul>"},{"location":"testing_guide/#running-tests","title":"Running Tests","text":"<ul> <li>Fast model-focused subset:</li> </ul> <pre><code>pytest -q tests/test_models_property_based_test.py \\\n         tests/test_models_serialization_roundtrip_test.py \\\n         tests/test_models_edge_cases_test.py \\\n         tests/test_models_integration_test.py\n</code></pre> <ul> <li>Include performance (benchmarks):</li> </ul> <pre><code>pytest -q tests/test_models_performance_test.py --benchmark-only\n</code></pre>"},{"location":"testing_guide/#dev-dependencies","title":"Dev Dependencies","text":"<ul> <li>Property-based tests require <code>hypothesis</code>. Ensure you have dev deps installed:</li> </ul> <pre><code>pip install -r requirements-dev.txt\n</code></pre>"},{"location":"testing_guide/#coverage","title":"Coverage","text":"<ul> <li>Coverage is enabled via <code>pytest-cov</code> in <code>requirements-dev.txt</code>.</li> </ul> <p>Generate an HTML coverage report for the entire suite:</p> <pre><code>pytest --cov=bench --cov-report=html\nopen htmlcov/index.html  # or use a file viewer\n</code></pre>"},{"location":"testing_guide/#notes","title":"Notes","text":"<ul> <li><code>EvaluationResult.metrics_results</code> now rejects non-numeric, NaN, and Infinity values.</li> <li>Timestamp fields are normalized to timezone-aware UTC; tests compare with <code>exclude={\"timestamp\"}</code> when appropriate.</li> </ul>"},{"location":"testing_integration_coverage/","title":"Integration Tests Coverage for Metrics System","text":"<p>This document summarizes the integration tests that validate the end-to-end metrics system in MedAISure.</p>"},{"location":"testing_integration_coverage/#scope-covered","title":"Scope Covered","text":"<ul> <li>End-to-end pipeline</li> <li><code>EvaluationHarness.evaluate()</code> across tasks, including callbacks, caching, and strict validation.</li> <li> <p>Reference: <code>tests/test_integration_end_to_end_local_model_test.py</code></p> </li> <li> <p>Model Runner integration</p> </li> <li>Model loading (local/HF mocked), batched inference, error propagation, and resource cleanup.</li> <li> <p>Reference: <code>tests/test_integration_end_to_end_local_model_test.py</code></p> </li> <li> <p>Metric aggregation and reports</p> </li> <li>Aggregation, statistics, filtering/sorting, and exporters (JSON/CSV/Markdown/HTML).</li> <li> <p>References:</p> <ul> <li><code>tests/test_result_aggregator_extended_test.py</code></li> <li><code>bench/evaluation/result_aggregator.py</code></li> </ul> </li> <li> <p>Performance smoke / large datasets</p> </li> <li>Smoke performance and large dataset scenarios validated.</li> <li> <p>Reference: <code>tests/test_integration_end_to_end_local_model_test.py</code></p> </li> <li> <p>Human judgment comparison</p> </li> <li>Custom <code>human_judgment</code> metric compared across runs using <code>ResultAggregator.compare_runs()</code>.</li> <li> <p>Reference: <code>tests/test_integration_human_judgment_and_regression_test.py::test_human_judgment_comparison_via_compare_runs</code></p> </li> <li> <p>Regression tests (comparisons)</p> </li> <li>Absolute and relative diffs between baseline and current runs using <code>compare_runs(relative=True)</code>.</li> <li>Reference: <code>tests/test_integration_human_judgment_and_regression_test.py::test_regression_detection_relative_diff</code></li> </ul>"},{"location":"testing_integration_coverage/#how-to-run-tests","title":"How to Run Tests","text":"<pre><code>pytest -q\n</code></pre> <p>All tests should pass; as of the latest run: 174 passed.</p>"},{"location":"testing_integration_coverage/#using-run-comparisons-in-practice","title":"Using Run Comparisons in Practice","text":"<ul> <li> <p>Absolute diff <pre><code>diff = agg.compare_runs(\"baseline\", \"current\", metrics=[\"accuracy\"], relative=False)\nprint(diff[\"overall\"][\"accuracy\"])  # positive =&gt; improvement\n</code></pre></p> </li> <li> <p>Relative diff <pre><code>diff = agg.compare_runs(\"baseline\", \"current\", metrics=[\"accuracy\"], relative=True)\n# (b - a) / (|a| + eps)\n</code></pre></p> </li> </ul>"},{"location":"testing_integration_coverage/#exported-reports","title":"Exported Reports","text":"<ul> <li><code>ResultAggregator</code> supports exporting a run report to multiple formats:</li> <li>JSON: <code>export_report_json(run_id, path)</code></li> <li>CSV: <code>export_report_csv(run_id, path)</code></li> <li>Markdown: <code>export_report_markdown(run_id, path)</code></li> <li>HTML: <code>export_report_html(run_id, path)</code></li> </ul> <p>See <code>tests/test_result_aggregator_extended_test.py</code> for usage examples.</p>"},{"location":"testing_integration_coverage/#notes","title":"Notes","text":"<ul> <li>Human judgment comparisons use a dedicated metric name (<code>human_judgment</code>) so they do not interfere with other metrics.</li> <li>Tests are designed to be lightweight and avoid heavy model dependencies.</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#common-issues","title":"Common Issues","text":"<ul> <li>Missing <code>transformers</code> or <code>torch</code> when using Hugging Face</li> <li> <p>Install: <code>pip install transformers torch</code></p> </li> <li> <p><code>rouge-score not available</code> in logs</p> </li> <li> <p>Install: <code>pip install rouge-score</code></p> </li> <li> <p>Model outputs length != inputs length</p> </li> <li> <p>Ensure your model returns one prediction dict per input example.</p> </li> <li> <p>Metrics are NaN</p> </li> <li>Check that reference fields exist in your task dataset (<code>label</code>, <code>answer</code>, <code>summary</code>, <code>note</code>, <code>rationale</code>)</li> <li> <p>Make sure prediction dicts include a compatible field (<code>label</code>, <code>prediction</code>, <code>text</code>, <code>summary</code>)</p> </li> <li> <p>Local model loader errors</p> </li> <li> <p>Provide <code>module_path</code> and ensure it exposes a callable loader (default <code>load_model(model_path, **kwargs)</code>)</p> </li> <li> <p>API request failures</p> </li> <li>Verify <code>endpoint</code>, <code>api_key</code>, and network access</li> <li>Tune <code>timeout</code>, <code>max_retries</code>, <code>backoff_factor</code></li> </ul>"},{"location":"troubleshooting/#debug-tips","title":"Debug Tips","text":"<ul> <li>Set <code>log_level=\"DEBUG\"</code> in <code>EvaluationHarness</code></li> <li>Use <code>use_cache=False</code> to re-run fresh predictions</li> <li>Inspect cached files in <code>&lt;cache_dir&gt;/&lt;run_id&gt;_&lt;task_id&gt;.json</code></li> <li>Enable <code>on_metrics</code> callback to stream metrics after each task</li> </ul>"},{"location":"usage/","title":"Usage Guide","text":"<p>This guide shows how to run evaluations with the MedAISure framework using the public API in <code>bench/evaluation/</code>.</p> <ul> <li>Core class: <code>EvaluationHarness</code> in <code>bench/evaluation/harness.py</code></li> <li>Supporting components: <code>TaskLoader</code>, <code>ModelRunner</code>, <code>MetricCalculator</code>, <code>ResultAggregator</code></li> </ul>"},{"location":"usage/#quick-start","title":"Quick Start","text":"<pre><code>from bench.evaluation import EvaluationHarness\n\nh = EvaluationHarness(\n    tasks_dir=\"bench/tasks\",\n    results_dir=\"bench/results\",\n    cache_dir=\"bench/results/cache\",\n    log_level=\"INFO\",\n)\n\n# Discover tasks and pick one\ntasks = h.list_available_tasks()\nprint([t[\"task_id\"] for t in tasks])\n\nreport = h.evaluate(\n    model_id=\"textattack/bert-base-uncased-MNLI\",\n    task_ids=[tasks[0][\"task_id\"]],\n    model_type=\"huggingface\",\n    model_kwargs={\"num_labels\": 3},\n    pipeline_kwargs={\"top_k\": 1},\n    batch_size=4,\n    use_cache=False,\n)\nprint(report.overall_scores)\n</code></pre>"},{"location":"usage/#running-with-different-model-types","title":"Running with Different Model Types","text":"<ul> <li>Hugging Face: <code>model_type=\"huggingface\"</code> with <code>model_kwargs</code>, <code>pipeline_kwargs</code> (see <code>ModelRunner._load_huggingface_model()</code>)</li> <li>Local model: <code>model_type=\"local\"</code> with <code>model_path</code>, <code>module_path</code>, optional <code>load_func</code> (see <code>ModelRunner._load_local_model()</code>)</li> <li>API model: <code>model_type=\"api\"</code> with <code>endpoint</code>, <code>api_key</code> and optional retry settings (see <code>ModelRunner._load_api_model()</code>)</li> </ul>"},{"location":"usage/#huggingface-advanced-options-generation-quick-start","title":"HuggingFace advanced options &amp; generation quick start","text":"<p>When loading Hugging Face models via <code>ModelRunner.load_model(..., model_type=\"huggingface\", ...)</code>, you can pass:</p> <ul> <li><code>hf_task</code>: <code>text-classification | summarization | text-generation</code></li> <li><code>generation_kwargs</code>: forwarded during inference for generative tasks (e.g., <code>max_new_tokens</code>, <code>temperature</code>, <code>do_sample</code>, <code>top_p</code>, <code>top_k</code>)</li> <li>Advanced loading: <code>device_map</code>, <code>torch_dtype</code>, <code>low_cpu_mem_usage</code>, <code>revision</code>, <code>trust_remote_code</code></li> </ul> <p>Quick start (generation parameters):</p> <pre><code>from bench.evaluation.model_runner import ModelRunner\n\nmr = ModelRunner()\nmr.load_model(\n    \"facebook/bart-large-cnn\",\n    model_type=\"huggingface\",\n    hf_task=\"summarization\",\n    generation_kwargs={\"max_new_tokens\": 96, \"temperature\": 0.7, \"do_sample\": True, \"top_p\": 0.9},\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n)\n\ninputs = [{\"document\": \"Long clinical note ...\"}]\npreds = mr.run_model(\"facebook/bart-large-cnn\", inputs, batch_size=1)\nprint(preds[0][\"summary\"])\n</code></pre> <p>See also the runnable examples: - <code>bench/examples/run_hf_summarization_gen.py</code> - <code>bench/examples/run_hf_text_generation_gen.py</code></p>"},{"location":"usage/#task-selection","title":"Task Selection","text":"<p><code>EvaluationHarness.list_available_tasks()</code> scans <code>tasks_dir</code> for YAML/JSON files. For advanced loading (file path or URL), use <code>TaskLoader.load_task()</code> directly.</p>"},{"location":"usage/#results-and-reports","title":"Results and Reports","text":"<ul> <li>Per-task results: collected as <code>EvaluationResult</code> in <code>bench/models/evaluation_result.py</code></li> <li>Aggregation: <code>ResultAggregator</code> builds a <code>BenchmarkReport</code> and supports CSV/Markdown/HTML export</li> <li>Save path: <code>&lt;results_dir&gt;/&lt;run_id&gt;.json</code></li> </ul>"},{"location":"usage/#caching","title":"Caching","text":"<p>If <code>cache_dir</code> is set, predictions per task are cached to JSON (<code>&lt;run_id&gt;_&lt;task_id&gt;.json</code>). Disable with <code>use_cache=False</code>.</p>"},{"location":"usage/#callbacks","title":"Callbacks","text":"<p><code>EvaluationHarness</code> accepts optional callbacks: - <code>on_task_start(task_id)</code>, <code>on_task_end(task_id, result)</code> - <code>on_progress(idx, total, current_task)</code> - <code>on_error(task_id, exception)</code> - <code>on_metrics(task_id, metrics_dict)</code></p> <p>See their usage throughout <code>EvaluationHarness.evaluate()</code>.</p>"},{"location":"api/overview/","title":"API Overview","text":"<p>This section documents the Python API using auto-generated references via mkdocstrings.</p> <ul> <li>Core modules</li> <li>Public classes and methods</li> </ul>"},{"location":"api/reference/","title":"Python API Reference","text":""},{"location":"api/reference/#bench","title":"<code>bench</code>","text":"<p>MedAISure Benchmark: Evaluating Large Language Models in Healthcare.</p> <p>This package provides a comprehensive framework for evaluating LLMs on medical tasks, including diagnostics, summarization, and patient communication.</p>"},{"location":"metrics/overview/","title":"Metrics Overview","text":"<p>This section describes supported metric categories and how they are computed within MedAISure.</p> <ul> <li>Clinical accuracy</li> <li>Reasoning quality</li> <li>Domain-specific metrics</li> </ul> <p>See also: metrics_guidelines.md, metrics_testing.md.</p>"},{"location":"models/api_models/","title":"API Models","text":"<p>Using hosted API models with MedAISure.</p> <ul> <li>Authentication and configuration</li> <li>Rate limiting and retries</li> <li>Cost and telemetry</li> </ul>"},{"location":"models/local_models/","title":"Local Models","text":"<p>Guidance for using local models (e.g., HuggingFace Transformers) with MedAISure.</p> <ul> <li>Installation and dependencies</li> <li>Example configuration</li> <li>Performance considerations</li> </ul>"},{"location":"models/model_interface/","title":"Model Interface","text":"<p>All models should implement a consistent interface compatible with the Evaluation Harness.</p> <p>Key aspects: - Loading/configuration - Generate/invoke API - Metadata and resource management</p>"},{"location":"tasks/overview/","title":"Tasks Overview","text":"<p>Overview of task types and how to add custom tasks.</p> <ul> <li>Medical QA</li> <li>Diagnostic Reasoning</li> <li>Clinical Summarization</li> </ul> <p>See also: <code>bench/examples/</code> and <code>docs/usage.md</code>.</p>"}]}
